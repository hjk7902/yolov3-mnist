{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64759373",
   "metadata": {},
   "source": [
    "# YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c5d7b",
   "metadata": {},
   "source": [
    "## YOLOv3의 합성곱과 Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20444768",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5c4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b53f4",
   "metadata": {},
   "source": [
    "### 합성곱함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b347a",
   "metadata": {},
   "source": [
    "#### BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66820a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(layers.BatchNormalization):\n",
    "    # \"동결 상태(Frozen state)\"와 \"추론 모드(Inference mode)\"는 별개의 개념입니다. \n",
    "    # 'layer.trainable=False' 이면 레이어를 동결시킵니다. 이것은 훈련하는 동안 내부 상태 즉, 가중치가 바뀌지 않습니다.\n",
    "    # 그런데 layer.trainable=False이면 추론 모드로 실행됩니다. \n",
    "    # 레이어는 추론모드에서 현재 배치의 평균 및 분산을 사용하는 대신 현재 배치를 정규화하기 위해 이동 평균과 이동 분산을 사용합니다.\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eff2b2",
   "metadata": {},
   "source": [
    "#### convolutional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11eb8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional(input_layer, filters, kernel_size,\n",
    "                  downsample=False, activate=True, bn=True):\n",
    "    if downsample:\n",
    "        input_layer = layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
    "        padding = 'valid'\n",
    "        strides = 2\n",
    "    else:\n",
    "        strides = 1\n",
    "        padding = 'same'\n",
    "\n",
    "    kernel_init = tf.random_normal_initializer(stddev=0.01)\n",
    "    conv = layers.Conv2D(filters=filters, \n",
    "                         kernel_size=kernel_size,\n",
    "                         strides=strides, padding=padding, \n",
    "                         use_bias=not bn,\n",
    "                         kernel_initializer=kernel_init,\n",
    "                         kernel_regularizer=l2(0.0005)\n",
    "                        )(input_layer)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    if activate:\n",
    "        conv = layers.LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a162e72",
   "metadata": {},
   "source": [
    "### 레지듀얼 블록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504eeee",
   "metadata": {},
   "source": [
    "#### residual_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236046cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer, filter_num1, filter_num2):\n",
    "    short_cut = input_layer\n",
    "    conv = convolutional(input_layer, filters=filter_num1, kernel_size=(1,1))\n",
    "    conv = convolutional(conv       , filters=filter_num2, kernel_size=(3,3))\n",
    "    residual_output = short_cut + conv\n",
    "    return residual_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14da4f8",
   "metadata": {},
   "source": [
    "## 다크넷 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e441c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet53(input_data):\n",
    "    input_data = convolutional(input_data, 32, (3,3))\n",
    "    input_data = convolutional(input_data, 64, (3,3), downsample=True)\n",
    "\n",
    "    for i in range(1):\n",
    "        input_data = residual_block(input_data,  32, 64)\n",
    "\n",
    "    input_data = convolutional(input_data, 128, (3,3), downsample=True)\n",
    "\n",
    "    for i in range(2):\n",
    "        input_data = residual_block(input_data, 64, 128)\n",
    "\n",
    "    input_data = convolutional(input_data, 256, (3,3), downsample=True)\n",
    "\n",
    "    for i in range(8):\n",
    "        input_data = residual_block(input_data, 128, 256)\n",
    "\n",
    "    route_1 = input_data\n",
    "    input_data = convolutional(input_data, 512, (3,3), downsample=True)\n",
    "\n",
    "    for i in range(8):\n",
    "        input_data = residual_block(input_data, 256, 512)\n",
    "\n",
    "    route_2 = input_data\n",
    "    input_data = convolutional(input_data, 1024, (3,3), downsample=True)\n",
    "\n",
    "    for i in range(4):\n",
    "        input_data = residual_block(input_data, 512, 1024)\n",
    "\n",
    "    return route_1, route_2, input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59faf35",
   "metadata": {},
   "source": [
    "## upsample() - 업샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c848bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(input_layer):\n",
    "    width, height = input_layer.shape[1], input_layer.shape[2]\n",
    "    output_layer = tf.image.resize(input_layer, (width*2, height*2), \n",
    "                                   method='nearest')\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f165733",
   "metadata": {},
   "source": [
    "## YOLOv3 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b47029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLOv3(input_layer, num_class):\n",
    "    # Darknet-53을 실행하고 그 결과를 받음\n",
    "    route_1, route_2, conv = darknet53(input_layer)\n",
    "    \n",
    "    conv = convolutional(conv, 512, (1,1))\n",
    "    conv = convolutional(conv, 1024, (3,3))\n",
    "    conv = convolutional(conv, 512, (1,1))\n",
    "    conv = convolutional(conv, 1024, (3,3))\n",
    "    conv = convolutional(conv, 512, (1,1))\n",
    "    conv_lobj_branch = convolutional(conv, 1024, (3,3))\n",
    "    \n",
    "    # conv_lbbox는 큰 객체를 예측하기 위해 사용, Shape = [None, 13, 13, 255] \n",
    "    conv_lbbox = convolutional(conv_lobj_branch, \n",
    "                               3*(num_class+5), (1,1),\n",
    "                               activate=False, bn=False)\n",
    "    \n",
    "    conv = convolutional(conv, 256, (1,1))\n",
    "    # 최근방법(nearest)을 이용하여 업샘플링\n",
    "    # 이렇게 하면 업샘플링시 학습이 필요 없으므로 인공신경망 파라미터를 줄인다.\n",
    "    conv = upsample(conv)\n",
    "\n",
    "    conv = tf.concat([conv, route_2], axis=-1)\n",
    "    conv = convolutional(conv, 256, (1,1))\n",
    "    conv = convolutional(conv, 512, (3,3))\n",
    "    conv = convolutional(conv, 256, (1,1))\n",
    "    conv = convolutional(conv, 512, (3,3))\n",
    "    conv = convolutional(conv, 256, (1,1))\n",
    "    conv_mobj_branch = convolutional(conv, 512, (3,3))\n",
    "\n",
    "    # conv_mbbox는 중간 크기 객체를 예측하기 위해 사용, shape = [None, 26, 26, 255]\n",
    "    conv_mbbox = convolutional(conv_mobj_branch, \n",
    "                               3*(num_class+5), (1,1),\n",
    "                               activate=False, bn=False)\n",
    "\n",
    "    conv = convolutional(conv, 128, (1,1))\n",
    "    conv = upsample(conv)\n",
    "\n",
    "    conv = tf.concat([conv, route_1], axis=-1)\n",
    "    conv = convolutional(conv, 128, (1,1))\n",
    "    conv = convolutional(conv, 256, (3,3))\n",
    "    conv = convolutional(conv, 128, (1,1))\n",
    "    conv = convolutional(conv, 256, (3,3))\n",
    "    conv = convolutional(conv, 128, (1,1))\n",
    "    conv_sobj_branch = convolutional(conv, 256, (3,3))\n",
    "    \n",
    "    # conv_sbbox는 작은 객체를 예측하기 위해 사용, shape = [None, 52, 52, 255]\n",
    "    conv_sbbox = convolutional(conv_sobj_branch, \n",
    "                               3*(num_class+5), (1,1),\n",
    "                               activate=False, bn=False)\n",
    "        \n",
    "    return [conv_sbbox, conv_mbbox, conv_lbbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ccef6",
   "metadata": {},
   "source": [
    "## 합성곱 신경망의 출력을 디코딩 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efab343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "YOLO_STRIDES  = [8, 16, 32]\n",
    "YOLO_ANCHORS  = [[[10,  13], [16,   30], [33,   23]],\n",
    "                 [[30,  61], [62,   45], [59,  119]],\n",
    "                 [[116, 90], [156, 198], [373, 326]]]\n",
    "STRIDES       = np.array(YOLO_STRIDES)\n",
    "ANCHORS       = (np.array(YOLO_ANCHORS).T/STRIDES).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8e3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(conv_output, num_class, i=0):\n",
    "    conv_shape       = tf.shape(conv_output)\n",
    "    batch_size       = conv_shape[0]\n",
    "    output_size      = conv_shape[1]\n",
    "\n",
    "    conv_output = tf.reshape(conv_output, \n",
    "                             (batch_size, output_size, output_size, \n",
    "                              3, num_class+5))\n",
    "\n",
    "    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # 상자의 x, y위치\n",
    "    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # 상자의 가로, 세로 크기\n",
    "    conv_raw_conf = conv_output[:, :, :, :, 4:5] # 상자의 신뢰도(confidence)\n",
    "    conv_raw_prob = conv_output[:, :, :, :, 5: ] # 클래스별 확률\n",
    "\n",
    "    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n",
    "    y = tf.range(output_size, dtype=tf.int32)\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    y = tf.tile(y, [1, output_size])\n",
    "    x = tf.range(output_size, dtype=tf.int32)\n",
    "    x = tf.expand_dims(x, 0)\n",
    "    x = tf.tile(x, [output_size, 1])\n",
    "\n",
    "    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
    "    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], \n",
    "                      [batch_size, 1, 1, 3, 1])\n",
    "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
    "\n",
    "    # 상자의 중심점을 계산\n",
    "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n",
    "    # 상자의 너비와 높이를 계산\n",
    "    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n",
    "\n",
    "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
    "    pred_conf = tf.sigmoid(conv_raw_conf) # 상자의 신뢰도 계산\n",
    "    pred_prob = tf.sigmoid(conv_raw_prob) # 클래스별 확률 계산\n",
    "\n",
    "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59f568",
   "metadata": {},
   "source": [
    "## YOLOv3 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cabc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f1cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_YOLOv3(input_shape=(416,416,3), train_mode=False, num_class=NUM_CLASS):\n",
    "    input_layer  = layers.Input(input_shape)\n",
    "    conv_tensors = YOLOv3(input_layer, num_class)\n",
    "    output_tensors = []\n",
    "    for i, conv_tensor in enumerate(conv_tensors):\n",
    "        pred_tensor = decode(conv_tensor, num_class, i)\n",
    "        if train_mode: output_tensors.append(conv_tensor)\n",
    "        output_tensors.append(pred_tensor)\n",
    "\n",
    "    model = tf.keras.Model(input_layer, output_tensors)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42f66d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 416, 416, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 416, 416, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 416, 416, 32  128        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 416, 416, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32  0          ['leaky_re_lu[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 208, 208, 64  18432       ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 208, 208, 64  256        ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 208, 208, 64  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 208, 208, 32  2048        ['leaky_re_lu_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 208, 208, 32  128        ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 208, 208, 32  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 208, 208, 64  18432       ['leaky_re_lu_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 208, 208, 64  256        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 208, 208, 64  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 208, 208, 64  0          ['leaky_re_lu_1[0][0]',          \n",
      " da)                            )                                 'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 209, 209, 64  0          ['tf.__operators__.add[0][0]']   \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 104, 104, 12  73728       ['zero_padding2d_1[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 104, 104, 12  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 104, 104, 12  0           ['batch_normalization_4[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 104, 104, 64  8192        ['leaky_re_lu_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 104, 104, 64  256        ['conv2d_5[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 104, 104, 64  0           ['batch_normalization_5[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 104, 104, 12  73728       ['leaky_re_lu_5[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 104, 104, 12  512        ['conv2d_6[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 104, 104, 12  0           ['batch_normalization_6[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 104, 104, 12  0          ['leaky_re_lu_4[0][0]',          \n",
      " mbda)                          8)                                'leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 104, 104, 64  8192        ['tf.__operators__.add_1[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 104, 104, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 104, 104, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 104, 104, 12  73728       ['leaky_re_lu_7[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 104, 104, 12  512        ['conv2d_8[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 104, 104, 12  0           ['batch_normalization_8[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 104, 104, 12  0          ['tf.__operators__.add_1[0][0]', \n",
      " mbda)                          8)                                'leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, 105, 105, 12  0          ['tf.__operators__.add_2[0][0]'] \n",
      " D)                             8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 52, 52, 256)  294912      ['zero_padding2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 52, 52, 256)  1024       ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 52, 52, 256)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 52, 52, 128)  32768       ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 52, 52, 128)  512        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_10[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 52, 52, 256)  0          ['leaky_re_lu_9[0][0]',          \n",
      " mbda)                                                            'leaky_re_lu_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 52, 52, 128)  512        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_3[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 52, 52, 128)  512        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_14[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_4[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_15[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 52, 52, 128)  512        ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_5[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 52, 52, 128)  512        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_18[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_6[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 52, 52, 128)  512        ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_7[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 52, 52, 128)  512        ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_22 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_22[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_23 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 52, 52, 256)  0          ['tf.__operators__.add_8[0][0]', \n",
      " mbda)                                                            'leaky_re_lu_23[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 52, 52, 128)  32768       ['tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 52, 52, 128)  512        ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 52, 52, 256)  0          ['tf.__operators__.add_9[0][0]', \n",
      " ambda)                                                           'leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, 53, 53, 256)  0          ['tf.__operators__.add_10[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 26, 26, 512)  1179648     ['zero_padding2d_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_26 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 26, 26, 256)  131072      ['leaky_re_lu_26[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_27 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_27[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 26, 26, 512)  0          ['leaky_re_lu_26[0][0]',         \n",
      " ambda)                                                           'leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_30 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_11[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_30[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_31 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_31[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_12[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_34 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_13[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_34[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_35 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_35[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_14[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_38 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_15[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_38[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_39 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_39[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_16[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 26, 26, 256)  131072      ['tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 26, 26, 512)  0          ['tf.__operators__.add_17[0][0]',\n",
      " ambda)                                                           'leaky_re_lu_42[0][0]']         \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadding2  (None, 27, 27, 512)  0          ['tf.__operators__.add_18[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 13, 13, 1024  4718592     ['zero_padding2d_4[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_43[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_43[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 13, 13, 512)  524288      ['leaky_re_lu_43[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_44[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_45[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_45[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 13, 13, 1024  0          ['leaky_re_lu_43[0][0]',         \n",
      " ambda)                         )                                 'leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 13, 13, 512)  524288      ['tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_46[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_47[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_47[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 13, 13, 1024  0          ['tf.__operators__.add_19[0][0]',\n",
      " ambda)                         )                                 'leaky_re_lu_47[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 13, 13, 512)  524288      ['tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_48[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_49[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_49[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 13, 13, 1024  0          ['tf.__operators__.add_20[0][0]',\n",
      " ambda)                         )                                 'leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 13, 13, 512)  524288      ['tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_50 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_50[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_51[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_51 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_51[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 13, 13, 1024  0          ['tf.__operators__.add_21[0][0]',\n",
      " ambda)                         )                                 'leaky_re_lu_51[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 13, 13, 512)  524288      ['tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_52[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_53[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_53[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 13, 13, 512)  524288      ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_54 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_54[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_55[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_55 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_55[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 13, 13, 512)  524288      ['leaky_re_lu_55[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 13, 13, 512)  2048       ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 13, 13, 512)  0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 13, 13, 256)  131072      ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_58 (LeakyReLU)     (None, 13, 13, 256)  0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " tf.image.resize (TFOpLambda)   (None, 26, 26, 256)  0           ['leaky_re_lu_58[0][0]']         \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 26, 26, 768)  0           ['tf.image.resize[0][0]',        \n",
      "                                                                  'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 26, 26, 256)  196608      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_59 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_59[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 26, 26, 256)  131072      ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_62 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 26, 26, 256)  131072      ['leaky_re_lu_62[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 26, 26, 256)  1024       ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_63 (LeakyReLU)     (None, 26, 26, 256)  0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 26, 26, 128)  32768       ['leaky_re_lu_63[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 26, 26, 128)  512        ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 26, 26, 128)  0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " tf.image.resize_1 (TFOpLambda)  (None, 52, 52, 128)  0          ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 52, 52, 384)  0           ['tf.image.resize_1[0][0]',      \n",
      "                                                                  'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 52, 52, 128)  49152       ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 52, 52, 128)  512        ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_66 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_66[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_67 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 52, 52, 128)  32768       ['leaky_re_lu_67[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 52, 52, 128)  512        ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 52, 52, 128)  32768       ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 52, 52, 128)  512        ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_70 (LeakyReLU)     (None, 52, 52, 128)  0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 52, 52, 256)  294912      ['leaky_re_lu_70[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 26, 26, 512)  1179648     ['leaky_re_lu_63[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 13, 13, 1024  4718592     ['leaky_re_lu_56[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 52, 52, 256)  1024       ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 26, 26, 512)  2048       ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_57[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_71 (LeakyReLU)     (None, 52, 52, 256)  0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 26, 26, 512)  0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 13, 13, 1024  0           ['batch_normalization_57[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 52, 52, 45)   11565       ['leaky_re_lu_71[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 26, 26, 45)   23085       ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 13, 13, 45)   46125       ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (4,)                0           ['conv2d_74[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (4,)                0           ['conv2d_66[0][0]']              \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOpLamb  (4,)                0           ['conv2d_58[0][0]']              \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.range_1 (TFOpLambda)        (52,)                0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.range (TFOpLambda)          (52,)                0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.range_3 (TFOpLambda)        (26,)                0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.range_2 (TFOpLambda)        (26,)                0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.range_5 (TFOpLambda)        (13,)                0           ['tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.range_4 (TFOpLambda)        (13,)                0           ['tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (1, 52)              0           ['tf.range_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (52, 1)              0           ['tf.range[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (1, 26)              0           ['tf.range_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (26, 1)              0           ['tf.range_2[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (1, 13)              0           ['tf.range_5[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (13, 1)              0           ['tf.range_4[0][0]']             \n",
      "                                                                                                  \n",
      " tf.tile_1 (TFOpLambda)         (52, 52)             0           ['tf.expand_dims_1[0][0]',       \n",
      "                                                                  'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (52, 52)             0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.tile_4 (TFOpLambda)         (26, 26)             0           ['tf.expand_dims_3[0][0]',       \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.tile_3 (TFOpLambda)         (26, 26)             0           ['tf.expand_dims_2[0][0]',       \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.tile_7 (TFOpLambda)         (13, 13)             0           ['tf.expand_dims_5[0][0]',       \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.tile_6 (TFOpLambda)         (13, 13)             0           ['tf.expand_dims_4[0][0]',       \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (52, 52, 1)         0           ['tf.tile_1[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (52, 52, 1)         0           ['tf.tile[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_15 (S  (26, 26, 1)         0           ['tf.tile_4[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  (26, 26, 1)         0           ['tf.tile_3[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_24 (S  (13, 13, 1)         0           ['tf.tile_7[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_25 (S  (13, 13, 1)         0           ['tf.tile_6[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (52, 52, 2)          0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.concat_5 (TFOpLambda)       (26, 26, 2)          0           ['tf.__operators__.getitem_15[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_16[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.concat_8 (TFOpLambda)       (13, 13, 2)          0           ['tf.__operators__.getitem_24[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_25[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 52, 52, 3, 1  0           ['conv2d_74[0][0]',              \n",
      "                                5)                                'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (1, 52, 52, 1, 2)   0           ['tf.concat_2[0][0]']            \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 26, 26, 3, 1  0           ['conv2d_66[0][0]',              \n",
      "                                5)                                'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_17 (S  (1, 26, 26, 1, 2)   0           ['tf.concat_5[0][0]']            \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.reshape_2 (TFOpLambda)      (None, 13, 13, 3, 1  0           ['conv2d_58[0][0]',              \n",
      "                                5)                                'tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_26 (S  (1, 13, 13, 1, 2)   0           ['tf.concat_8[0][0]']            \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 52, 52, 3, 2  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                 )                                                                 \n",
      "                                                                                                  \n",
      " tf.tile_2 (TFOpLambda)         (None, 52, 52, 3, 2  0           ['tf.__operators__.getitem_8[0][0\n",
      "                                )                                ]',                              \n",
      "                                                                  'tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 52, 52, 3, 2  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                 )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 26, 26, 3, 2  0          ['tf.reshape_1[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.tile_5 (TFOpLambda)         (None, 26, 26, 3, 2  0           ['tf.__operators__.getitem_17[0][\n",
      "                                )                                0]',                             \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 26, 26, 3, 2  0          ['tf.reshape_1[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_20 (S  (None, 13, 13, 3, 2  0          ['tf.reshape_2[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.tile_8 (TFOpLambda)         (None, 13, 13, 3, 2  0           ['tf.__operators__.getitem_26[0][\n",
      "                                )                                0]',                             \n",
      "                                                                  'tf.__operators__.getitem_18[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_21 (S  (None, 13, 13, 3, 2  0          ['tf.reshape_2[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambda)   (None, 52, 52, 3, 2  0           ['tf.__operators__.getitem_2[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 52, 52, 3, 2  0           ['tf.tile_2[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.exp (TFOpLambda)       (None, 52, 52, 3, 2  0           ['tf.__operators__.getitem_3[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_3 (TFOpLambda)  (None, 26, 26, 3, 2  0          ['tf.__operators__.getitem_11[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (None, 26, 26, 3, 2  0           ['tf.tile_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.exp_1 (TFOpLambda)     (None, 26, 26, 3, 2  0           ['tf.__operators__.getitem_12[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_6 (TFOpLambda)  (None, 13, 13, 3, 2  0          ['tf.__operators__.getitem_20[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.cast_2 (TFOpLambda)         (None, 13, 13, 3, 2  0           ['tf.tile_8[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.exp_2 (TFOpLambda)     (None, 13, 13, 3, 2  0           ['tf.__operators__.getitem_21[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 52, 52, 3, 2  0          ['tf.math.sigmoid[0][0]',        \n",
      " ambda)                         )                                 'tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 52, 52, 3, 2  0          ['tf.math.exp[0][0]']            \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 26, 26, 3, 2  0          ['tf.math.sigmoid_3[0][0]',      \n",
      " ambda)                         )                                 'tf.cast_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 26, 26, 3, 2  0          ['tf.math.exp_1[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 13, 13, 3, 2  0          ['tf.math.sigmoid_6[0][0]',      \n",
      " ambda)                         )                                 'tf.cast_2[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 13, 13, 3, 2  0          ['tf.math.exp_2[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 52, 52, 3, 2  0           ['tf.__operators__.add_23[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 52, 52, 3, 2  0          ['tf.math.multiply_1[0][0]']     \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 52, 52, 3, 1  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                 )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 52, 52, 3, 1  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                 0)                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 26, 26, 3, 2  0          ['tf.__operators__.add_24[0][0]']\n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 26, 26, 3, 2  0          ['tf.math.multiply_4[0][0]']     \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None, 26, 26, 3, 1  0          ['tf.reshape_1[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None, 26, 26, 3, 1  0          ['tf.reshape_1[0][0]']           \n",
      " licingOpLambda)                0)                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 13, 13, 3, 2  0          ['tf.__operators__.add_25[0][0]']\n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 13, 13, 3, 2  0          ['tf.math.multiply_7[0][0]']     \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_22 (S  (None, 13, 13, 3, 1  0          ['tf.reshape_2[0][0]']           \n",
      " licingOpLambda)                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_23 (S  (None, 13, 13, 3, 1  0          ['tf.reshape_2[0][0]']           \n",
      " licingOpLambda)                0)                                                                \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 52, 52, 3, 4  0           ['tf.math.multiply[0][0]',       \n",
      "                                )                                 'tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_1 (TFOpLambda)  (None, 52, 52, 3, 1  0          ['tf.__operators__.getitem_4[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_2 (TFOpLambda)  (None, 52, 52, 3, 1  0          ['tf.__operators__.getitem_5[0][0\n",
      "                                0)                               ]']                              \n",
      "                                                                                                  \n",
      " tf.concat_6 (TFOpLambda)       (None, 26, 26, 3, 4  0           ['tf.math.multiply_3[0][0]',     \n",
      "                                )                                 'tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_4 (TFOpLambda)  (None, 26, 26, 3, 1  0          ['tf.__operators__.getitem_13[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_5 (TFOpLambda)  (None, 26, 26, 3, 1  0          ['tf.__operators__.getitem_14[0][\n",
      "                                0)                               0]']                             \n",
      "                                                                                                  \n",
      " tf.concat_9 (TFOpLambda)       (None, 13, 13, 3, 4  0           ['tf.math.multiply_6[0][0]',     \n",
      "                                )                                 'tf.math.multiply_8[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_7 (TFOpLambda)  (None, 13, 13, 3, 1  0          ['tf.__operators__.getitem_22[0][\n",
      "                                )                                0]']                             \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_8 (TFOpLambda)  (None, 13, 13, 3, 1  0          ['tf.__operators__.getitem_23[0][\n",
      "                                0)                               0]']                             \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 52, 52, 3, 1  0           ['tf.concat_3[0][0]',            \n",
      "                                5)                                'tf.math.sigmoid_1[0][0]',      \n",
      "                                                                  'tf.math.sigmoid_2[0][0]']      \n",
      "                                                                                                  \n",
      " tf.concat_7 (TFOpLambda)       (None, 26, 26, 3, 1  0           ['tf.concat_6[0][0]',            \n",
      "                                5)                                'tf.math.sigmoid_4[0][0]',      \n",
      "                                                                  'tf.math.sigmoid_5[0][0]']      \n",
      "                                                                                                  \n",
      " tf.concat_10 (TFOpLambda)      (None, 13, 13, 3, 1  0           ['tf.concat_9[0][0]',            \n",
      "                                5)                                'tf.math.sigmoid_7[0][0]',      \n",
      "                                                                  'tf.math.sigmoid_8[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61,624,807\n",
      "Trainable params: 61,572,199\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yolo = Create_YOLOv3(train_mode=True, num_class=NUM_CLASS)\n",
    "yolo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fcb80a",
   "metadata": {},
   "source": [
    "# 모델 만들고 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63059b7",
   "metadata": {},
   "source": [
    "## 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc31e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10 \n",
    " \n",
    "from yolov3 import Create_YOLOv3\n",
    "yolo = Create_YOLOv3(train_mode=True, num_class=NUM_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab45fb",
   "metadata": {},
   "source": [
    "## 이미지 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e0b0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "def random_horizontal_flip(image, bboxes, p=0.5):\n",
    "    if random.random() < p:\n",
    "        _, w, _ = image.shape\n",
    "        image = image[:, ::-1, :]\n",
    "        bboxes[:, [0, 2]] = w - bboxes[:, [2, 0]]\n",
    "\n",
    "    return image, bboxes\n",
    "\n",
    "# 자르기 \n",
    "def random_crop(image, bboxes, p=0.5):\n",
    "    if random.random() < p:\n",
    "        h, w, _ = image.shape\n",
    "        max_bbox = np.concatenate( \n",
    "            [np.min(bboxes[:, 0:2], axis=0), \n",
    "             np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
    "\n",
    "        max_l_trans = max_bbox[0]\n",
    "        max_u_trans = max_bbox[1]\n",
    "        max_r_trans = w - max_bbox[2]\n",
    "        max_d_trans = h - max_bbox[3]\n",
    "\n",
    "        crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n",
    "        crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n",
    "        crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n",
    "        crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n",
    "\n",
    "        image = image[crop_ymin:crop_ymax, crop_xmin:crop_xmax]\n",
    "\n",
    "        bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n",
    "        bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n",
    "  \n",
    "    return image, bboxes\n",
    "\n",
    "  \n",
    "# 이동 \n",
    "def random_translate(image, bboxes, p=0.5):\n",
    "    if random.random() < p:\n",
    "        h, w, _ = image.shape\n",
    "        max_bbox = np.concatenate( \n",
    "            [np.min(bboxes[:, 0:2], axis=0),\n",
    "             np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
    "\n",
    "        max_l_trans = max_bbox[0]\n",
    "        max_u_trans = max_bbox[1]\n",
    "        max_r_trans = w - max_bbox[2]\n",
    "        max_d_trans = h - max_bbox[3]\n",
    "\n",
    "        tx = random.uniform(-(max_l_trans-1), (max_r_trans-1))\n",
    "        ty = random.uniform(-(max_u_trans-1), (max_d_trans-1))\n",
    "\n",
    "        M = np.array([[1, 0, tx], [0, 1, ty]])\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "        bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n",
    "        bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n",
    "\n",
    "    return image, bboxes\n",
    "\n",
    "\n",
    "# 이미지를 정사각형 크기로 변환, \n",
    "# 채워지는 화소 기본값은 value 속성의 값으로 설정함 \n",
    "def resize_to_square(image, target_size, gt_boxes=None, value=128): \n",
    "    ih, iw = target_size\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    scale = min(iw / w, ih / h)\n",
    "    nw, nh = int(scale * w), int(scale * h)\n",
    "    image_resized = cv2.resize(image, (nw, nh))\n",
    "\n",
    "    image_padded = np.full(shape=[ih, iw, 3],\n",
    "                           fill_value=value)\n",
    "    dw, dh = (iw - nw) // 2, (ih - nh) // 2\n",
    "    image_padded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
    "    image_padded = image_padded / 255.\n",
    "\n",
    "    if gt_boxes is None:        return image_padded\n",
    "    else:\n",
    "        gt_boxes[:, [0,2]] = gt_boxes[:, [0,2]]*scale + dw\n",
    "        gt_boxes[:, [1,3]] = gt_boxes[:, [1,3]]*scale + dh\n",
    "\n",
    "    return image_padded, gt_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d2db6",
   "metadata": {},
   "source": [
    "## IoU 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "651b1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def bbox_iou(boxes1, boxes2):\n",
    "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
    "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5, \n",
    "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5],\n",
    "                       axis=-1)\n",
    "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5, \n",
    "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5],\n",
    "                       axis=-1)\n",
    "\n",
    "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "\n",
    "    return 1.0 * inter_area / union_area\n",
    " \n",
    "# GIoU 계산하는 함수 \n",
    "def bbox_giou(boxes1, boxes2):\n",
    "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
    "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5],\n",
    "                       axis=-1)\n",
    "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
    "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5],\n",
    "                       axis=-1)\n",
    "\n",
    "    boxes1 = tf.concat([tf.minimum(boxes1[...,:2], boxes1[...,2:]),\n",
    "                        tf.maximum(boxes1[...,:2], boxes1[...,2:])], \n",
    "                       axis=-1)\n",
    "    boxes2 = tf.concat([tf.minimum(boxes2[...,:2], boxes2[...,2:]),\n",
    "                        tf.maximum(boxes2[...,:2], boxes2[...,2:])],\n",
    "                       axis=-1)\n",
    "\n",
    "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "  \n",
    "    # 두 경계 상자의 IoU를 계산 \n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    # 왼쪽 위와 오른쪽 아래를 포함하는 가장 작은 사각형 계산 \n",
    "    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n",
    "    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n",
    "  \n",
    "    # 가장 작은 C 상자의 면적 계산 \n",
    "    enclose_area = enclose[..., 0] * enclose[..., 1]\n",
    "  \n",
    "    # GIoU 공식으로 GIoU 계산 \n",
    "    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n",
    "\n",
    "    return giou\n",
    " \n",
    "# CIoU 계산하는 함수 \n",
    "def bbox_ciou(boxes1, boxes2):\n",
    "    boxes1_coor = tf.concat([boxes1[...,:2] - boxes1[...,2:] * 0.5, \n",
    "                             boxes1[...,:2] + boxes1[...,2:] * 0.5], \n",
    "                            axis=-1)\n",
    "    boxes2_coor = tf.concat([boxes2[...,:2] - boxes2[...,2:] * 0.5, \n",
    "                             boxes2[...,:2] + boxes2[...,2:] * 0.5], \n",
    "                            axis=-1)\n",
    "\n",
    "    left = tf.maximum(boxes1_coor[..., 0], boxes2_coor[..., 0])\n",
    "    up = tf.maximum(boxes1_coor[..., 1], boxes2_coor[..., 1])\n",
    "    right = tf.maximum(boxes1_coor[..., 2], boxes2_coor[..., 2])\n",
    "    down = tf.maximum(boxes1_coor[..., 3], boxes2_coor[..., 3])\n",
    "\n",
    "    c = (right - left) * (right - left) + (up - down) * (up - down)\n",
    "    iou = bbox_iou(boxes1, boxes2)\n",
    "\n",
    "    u = (boxes1[..., 0] - boxes2[..., 0]) * (boxes1[..., 0] - boxes2[..., 0]) + (boxes1[..., 1] - boxes2[..., 1]) * (boxes1[..., 1] - boxes2[..., 1])\n",
    "    d = u / c\n",
    "\n",
    "    ar_gt = boxes2[..., 2] / boxes2[..., 3]\n",
    "    ar_pred = boxes1[..., 2] / boxes1[..., 3]\n",
    "\n",
    "    ar_loss = 4 / (np.pi * np.pi) * (tf.atan(ar_gt) - tf.atan(ar_pred)) * (tf.atan(ar_gt) - tf.atan(ar_pred))\n",
    "    alpha = ar_loss / (1 - iou + ar_loss + 0.000001)\n",
    "    ciou_term = d + alpha * ar_loss\n",
    " \n",
    "    return iou - ciou_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e76f6",
   "metadata": {},
   "source": [
    "## 스트라이드와 앵커박스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7556984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "YOLO_STRIDES  = [8, 16, 32]\n",
    "YOLO_ANCHORS  = [[[10,  13], [16,   30], [33,   23]],\n",
    "                 [[30,  61], [62,   45], [59,  119]],\n",
    "                 [[116, 90], [156, 198], [373, 326]]]\n",
    "\n",
    "STRIDES       = np.array(YOLO_STRIDES)\n",
    "ANCHORS       = (np.array(YOLO_ANCHORS).T/STRIDES).T\n",
    "\n",
    "NUM_CLASS     = 10 # COCO 데이터이면 80, MNIST 데이터이면 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e1811",
   "metadata": {},
   "source": [
    "## 데이터 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11797be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from config import *\n",
    "from image_process import *\n",
    "from bbox_iou import *\n",
    "\n",
    "\n",
    "# 파일에서 클래스 라벨을 읽어 딕셔너리로 만들어 반환\n",
    "def read_class_names(class_label_path):\n",
    "    names = {}\n",
    "    with open(class_label_path, 'r') as data:\n",
    "        for ID, name in enumerate(data):\n",
    "            names[ID] = name.strip('\\n')\n",
    "    return names\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 annot_path,\n",
    "                 class_label_path,\n",
    "                 load_images_to_ram=True,\n",
    "                 data_aug=True,\n",
    "                 input_size=416,\n",
    "                 anchor_per_scale=3,\n",
    "                 max_bbox_per_scale=100, \n",
    "                 batch_size=4,\n",
    "                 strides=STRIDES, \n",
    "                 anchors=ANCHORS):\n",
    "        self.input_size = input_size\n",
    "        self.annot_path = annot_path\n",
    "        self.batch_size = batch_size\n",
    "        self.data_aug = False\n",
    "        self.strides = strides\n",
    "        self.classes = read_class_names(class_label_path)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.anchors = anchors\n",
    "        self.anchor_per_scale = anchor_per_scale\n",
    "        self.max_bbox_per_scale = max_bbox_per_scale\n",
    "        self.load_images_to_ram = load_images_to_ram\n",
    "        self.annotations = self.load_annotations(annot_path)\n",
    "        self.num_samples = len(self.annotations)\n",
    "        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size)) \n",
    "        self.batch_count = 0 \n",
    "        self.output_sizes = input_size // strides\n",
    "\n",
    "    # 아노테이션 경로에서 데이터파일을 읽어옴 \n",
    "    def load_annotations(self, annot_path):\n",
    "        # C:\\mnist_test\\000009.jpg \n",
    "        # 156,153,178,175,9 278,294,300,316,0 \n",
    "        annotations = []\n",
    "\n",
    "        with open(self.annot_path, 'r') as f:\n",
    "            # 파일에서 데이터를 불러와 라인별로 자름 \n",
    "            data = f.read().splitlines()\n",
    "\n",
    "        # 공백으로 잘라 맨 앞의 파일경로제외하고  \n",
    "        # 길이가0이 아닌 행들을 리스트로 만들어 놓음 \n",
    "        # 파일명만 있는 행 제거 \n",
    "        # (객체가 없는 이미지의 어노테이션 데이터임) \n",
    "        lines = [line.strip() for line in data \n",
    "                 if len(line.strip().split()[1:]) != 0]\n",
    "\n",
    "        # 랜덤하게 섞음 \n",
    "        np.random.shuffle(lines)\n",
    "\n",
    "        for line in lines:\n",
    "            # 공백으로 나눔 \n",
    "            # 예: line=['C:\\mnist_test\\000009.jpg', \n",
    "            # 156,153,178,175,9', '278,294,300,316,0'] \n",
    "            annotation = line.split()\n",
    "            image_path = annotation[0]\n",
    "\n",
    "            # 어노테이션 이미지파일이 없으면 예외 발생시킴 \n",
    "            if not os.path.exists(image_path):\n",
    "                raise KeyError(f\"{image_path} 파일이 없음\")\n",
    "\n",
    "            # 램 사용하면 이미지를 메모리에 저장 후 사용 \n",
    "            # 램 사용하지 않으면 \n",
    "            #    __next__에서 parse_annotation을 실행, \n",
    "            #    parse_annotation에서 이미지가 로드됨 \n",
    "            if self.load_images_to_ram:\n",
    "                image = cv2.imread(image_path)\n",
    "            else:\n",
    "                image = '' \n",
    "\n",
    "            # [['C:\\mnist_test\\000009.jpg', \n",
    "            # [156,153,178,175,9', '278,294,300,316,0'], ''], ... ] \n",
    "            annotations.append([image_path, annotation[1:],\n",
    "                                image])\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    # 아노테이션 데이터 파싱 \n",
    "    def parse_annotation(self, annotation, mAP='False'):\n",
    "        if self.load_images_to_ram:\n",
    "            image_path = annotation[0]\n",
    "            image = annotation[2]\n",
    "        else:\n",
    "            image_path = annotation[0]\n",
    "            image = cv2.imread(image_path) # 이미지를 불러옴 \n",
    "\n",
    "        #  [[156,153,178,175,9], [278,294,300,316,0]] \n",
    "        bboxes = np.array([list(map(int, box.split(','))) \n",
    "                           for box in annotation[1]])\n",
    "\n",
    "        # 이미지 증강 - 숫자, 문자는 좌/우 반전이 필요 없음 \n",
    "        # 이미지를 변환하면 경계 상자도 같이 바꿔줘야 함 \n",
    "        if self.data_aug:\n",
    "            # 좌/우 반전(생략) \n",
    "#             image, bboxes = random_horizontal_flip( \n",
    "#                 np.copy(image), np.copy(bboxes)) \n",
    "            # 자르기 \n",
    "            image, bboxes = random_crop(np.copy(image),\n",
    "                                        np.copy(bboxes))  \n",
    "            # 이동 \n",
    "            image, bboxes = random_translate(np.copy(image),\n",
    "                                             np.copy(bboxes))\n",
    "\n",
    "        # mAP=False이면 원본 이미지를 입력 이미지 크기로 변환 \n",
    "        if not mAP:\n",
    "            square_shape = [self.input_size, self.input_size]\n",
    "            image, bboxes = self.ip.resize_to_squre( \n",
    "                np.copy(image), square_shape, np.copy(bboxes))\n",
    "\n",
    "        return image, bboxes\n",
    " \n",
    "    # 상자 전처리 \n",
    "    def preprocess_true_boxes(self, bboxes):\n",
    "        # 스트라이드의 수 만큼 출력 레벨이 만들어짐 \n",
    "        OUTPUT_LEVELS = len(self.strides)\n",
    "\n",
    "        # output_size = 416/[8, 16, 32] = [52, 26, 13] -> N\n",
    "        # anchor_per_scale = 3, num_classes = 10(MNIST일 경우)\n",
    "        # 출력 레벨 수 만큼 (N,N,3,15) 모양의 라벨 배열 초기화\n",
    "        label = [np.zeros((self.output_sizes[i],\n",
    "                           self.output_sizes[i],\n",
    "                           self.anchor_per_scale,\n",
    "                           5 + self.num_classes))\n",
    "                 for i in range(OUTPUT_LEVELS)]\n",
    "        # max_bbox_per_scale = 100 \n",
    "        # 출력 레벨 수 만큼 (100,4) 모양 경계상자 배열 초기화 \n",
    "        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4))\n",
    "                       for _ in range(OUTPUT_LEVELS)]\n",
    "        # 출력 레벨 수 만큼 상자 수 배열 초기화 \n",
    "        bbox_count = np.zeros((OUTPUT_LEVELS,))\n",
    "\n",
    "        # 모든 상자 수 만큼 실행 \n",
    "        for bbox in bboxes:\n",
    "            # 상자 좌표 \n",
    "            bbox_coor = bbox[:4]\n",
    "            # 상자 클래스 라벨 \n",
    "            bbox_class_ind = bbox[4]\n",
    "            # 상자의 클래스 라벨 원-핫 인코딩\n",
    "            onehot = np.zeros(self.num_classes, dtype=np.float64) \n",
    "            onehot[bbox_class_ind] = 1.0\n",
    "\n",
    "            # 원-핫 라벨 평활화(Label Smoothing) \n",
    "            # 레이블 정규화라고 부르기도 함 \n",
    "            # 손실함수가 cross entropy이고,\n",
    "            # 활성화 함수를 softmax를 사용할 때 적용 \n",
    "            # 가장 큰 벡터가 나머지 벡터보다 커지는 것을 억제 \n",
    "            # 공식: y_ls = (1-alpha)*y_onehot + alpha/K \n",
    "            K = self.num_classes\n",
    "            alpha = 0.01 \n",
    "            smooth_onehot = (1-alpha)*onehot + alpha/K \n",
    "\n",
    "            # 상자 좌표를 상자 x,y,w,h로 변환 후 표준화 \n",
    "            bbox_xywh = np.concatenate(\n",
    "                [(bbox_coor[2:] + bbox_coor[:2]) * 0.5,\n",
    "                 bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n",
    "            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis] \n",
    "\n",
    "            iou = []\n",
    "            exist_positive = False \n",
    "            for i in range(OUTPUT_LEVELS):  # range(3): \n",
    "                # 앵커박스 \n",
    "                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n",
    "                anchors_xywh[:, 0:2] = np.floor(\n",
    "                    bbox_xywh_scaled[i, 0:2]).astype(np.int32)+0.5\n",
    "                anchors_xywh[:, 2:4] = self.anchors[i]\n",
    "\n",
    "                # 실제 박스와 앵커박스 IoU계산 \n",
    "                iou_scale = bbox_iou(\n",
    "                    bbox_xywh_scaled[i][np.newaxis, :],\n",
    "                    anchors_xywh)\n",
    "                iou.append(iou_scale)\n",
    "\n",
    "                # IoU가 0.3 이상인 박스만 처리함 \n",
    "                iou_mask = iou_scale > 0.3 \n",
    "                if np.any(iou_mask):\n",
    "                    xi, yi = np.floor(\n",
    "                        bbox_xywh_scaled[i, 0:2]).astype(np.int32) \n",
    "\n",
    "                    label[i][yi, xi, iou_mask, :] = 0 \n",
    "                    label[i][yi, xi, iou_mask, 0:4] = bbox_xywh\n",
    "                    label[i][yi, xi, iou_mask, 4:5] = 1.0 \n",
    "                    label[i][yi, xi, iou_mask, 5:] = smooth_onehot\n",
    "\n",
    "                    bbox_ind = int(                        bbox_count[i]%self.max_bbox_per_scale)\n",
    "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
    "                    bbox_count[i] += 1 \n",
    "                    exist_positive = True \n",
    "  \n",
    "            if not exist_positive:\n",
    "                bst_anc_idx = np.argmax(np.array(iou).reshape(-1),\n",
    "                                        axis=-1)\n",
    "                best_detect = int(bst_anc_idx / self.anchor_per_scale)\n",
    "                best_anchor = int(bst_anc_idx % self.anchor_per_scale)\n",
    "                xi, yi = np.floor(\n",
    "                    bbox_xywh_scaled[best_detect,\n",
    "                                     0:2]).astype(np.int32)\n",
    "\n",
    "                label[best_detect][yi, xi, best_anchor, :] = 0 \n",
    "                label[best_detect][yi, xi,\n",
    "                                   best_anchor, 0:4] = bbox_xywh \n",
    "                label[best_detect][yi, xi,\n",
    "                                   best_anchor, 4:5] = 1.0 \n",
    "                label[best_detect][yi, xi,\n",
    "                                   best_anchor, 5:] = smooth_onehot \n",
    "\n",
    "                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n",
    "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh \n",
    "                bbox_count[best_detect] += 1 \n",
    "\n",
    "        label_sbbox, label_mbbox, label_lbbox = label\n",
    "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "        output_boxes = label_sbbox, label_mbbox, label_lbbox,\n",
    "        sbboxes, mbboxes, lbboxes\n",
    "        return output_boxes \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batchs\n",
    "  \n",
    "    def __iter__(self):\n",
    "        return self \n",
    " \n",
    "    # 배치 크기만큼 이미지와 라벨 박스를 반환 \n",
    "    def __next__(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            # 배치 이미지를 갖는 배열 \n",
    "            batch_image = np.zeros( \n",
    "                (self.batch_size,\n",
    "                 self.input_size,\n",
    "                 self.input_size,\n",
    "                 3), dtype=np.float32)\n",
    " \n",
    "            # 배치 라벨(small, middle, large) 경계 상자 \n",
    "            batch_label_sbbox = np.zeros(\n",
    "                (self.batch_size,\n",
    "                 self.output_sizes[0],\n",
    "                 self.output_sizes[0],\n",
    "                 self.anchor_per_scale,\n",
    "                 5 + self.num_classes), dtype=np.float32)\n",
    "            batch_label_mbbox = np.zeros( \n",
    "                (self.batch_size,\n",
    "                 self.output_sizes[1],\n",
    "                 self.output_sizes[1],\n",
    "                 self.anchor_per_scale,\n",
    "                 5 + self.num_classes), dtype=np.float32)\n",
    "            batch_label_lbbox = np.zeros( \n",
    "                (self.batch_size,\n",
    "                 self.output_sizes[2], \n",
    "                 self.output_sizes[2], \n",
    "                 self.anchor_per_scale,\n",
    "                 5 + self.num_classes), dtype=np.float32)\n",
    " \n",
    "            # 배치 크기만큼 경계 상자를 저장할 변수 \n",
    "            batch_sbboxes = np.zeros(\n",
    "                (self.batch_size,\n",
    "                 self.max_bbox_per_scale, 4),\n",
    "                dtype=np.float32)\n",
    "            batch_mbboxes = np.zeros(\n",
    "                (self.batch_size, \n",
    "                 self.max_bbox_per_scale, 4),\n",
    "                dtype=np.float32)\n",
    "            batch_lbboxes = np.zeros( \n",
    "                (self.batch_size,\n",
    "                 self.max_bbox_per_scale, 4),\n",
    "                dtype=np.float32)\n",
    "\n",
    "            exceptions = False \n",
    "            num = 0 \n",
    "            if self.batch_count < self.num_batchs:\n",
    "                # 배치 크기만큼 실행   \n",
    "                while num < self.batch_size:  \n",
    "                    index = self.batch_count * self.batch_size + num \n",
    "                    if index >= self.num_samples: \n",
    "                        index -= self.num_samples\n",
    "                    annotation = self.annotations[index]\n",
    "                    image, bboxes = self.parse_annotation( annotation) \n",
    "                    try:\n",
    "                        label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes) \n",
    "                    except IndexError:\n",
    "                        exceptions = True \n",
    "                        print(\"IndexError,\", annotation[0])\n",
    "\n",
    "                    batch_image[num,:,:,:] = image \n",
    "                    batch_label_mbbox[num,:,:,:,:] = label_mbbox \n",
    "                    batch_label_lbbox[num,:,:,:,:] = label_lbbox \n",
    "                    batch_mbboxes[num,:,:] = mbboxes \n",
    "                    batch_lbboxes[num,:,:] = lbboxes \n",
    "                    batch_label_sbbox[num,:,:,:,:] = label_sbbox \n",
    "                    batch_sbboxes[num,:,:] = sbboxes \n",
    "                    num += 1 \n",
    "\n",
    "                if exceptions:\n",
    "                    print('\\n')\n",
    "                    raise Exception(\"데이터셋에 문제가 있습니다.\")\n",
    "\n",
    "                self.batch_count += 1 \n",
    "                batch_sm_target = batch_label_sbbox, batch_sbboxes \n",
    "                batch_md_target = batch_label_mbbox, batch_mbboxes \n",
    "                batch_lg_target = batch_label_lbbox, batch_lbboxes \n",
    "\n",
    "                target=(batch_sm_target,batch_md_target,batch_lg_target) \n",
    "                return batch_image, target\n",
    "            else:\n",
    "                self.batch_count = 0\n",
    "                np.random.shuffle(self.annotations)\n",
    "                raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bee84",
   "metadata": {},
   "source": [
    "## GPU 사용 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a23a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'GPUs {gpus}')\n",
    "if len(gpus) > 0:\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38437b9",
   "metadata": {},
   "source": [
    "## 학습 로그 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e7fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "LOGDIR = \"logs\" # 학습 로그를 저장할 디렉토리 \n",
    "\n",
    "if os.path.exists(LOGDIR): \n",
    "    shutil.rmtree(LOGDIR) # 로그 디렉토리가 있으면 삭제 \n",
    "\n",
    "writer = tf.summary.create_file_writer(LOGDIR)\n",
    "\n",
    "validate_writer = tf.summary.create_file_writer(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c145aee",
   "metadata": {},
   "source": [
    "## compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02bd2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred, conv, label, bboxes,\n",
    "                 i=0, num_class=80, iou_loss_thresh=0.45):\n",
    "    conv_shape  = tf.shape(conv)\n",
    "    batch_size  = conv_shape[0]\n",
    "    output_size = conv_shape[1]\n",
    "    input_size  = STRIDES[i] * output_size\n",
    "    conv = tf.reshape(conv,\n",
    "                      (batch_size, output_size, output_size,\n",
    "                       3, 5 + num_class))\n",
    "\n",
    "    conv_raw_conf = conv[:, :, :, :, 4:5]\n",
    "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
    "\n",
    "    pred_xywh     = pred[:, :, :, :, 0:4]\n",
    "    pred_conf     = pred[:, :, :, :, 4:5]\n",
    "\n",
    "    label_xywh    = label[:, :, :, :, 0:4]\n",
    "    respond_bbox  = label[:, :, :, :, 4:5]\n",
    "    label_prob    = label[:, :, :, :, 5:]\n",
    "\n",
    "    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), \n",
    "                          axis=-1)\n",
    "    input_size = tf.cast(input_size, tf.float32)    \n",
    "\n",
    "    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n",
    "    giou_loss = respond_bbox * bbox_loss_scale * (1 - giou)\n",
    "\n",
    "    # bbox_iou \n",
    "    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :],\n",
    "                   bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :]) \n",
    "\n",
    "    # 실제 상자에서 가장 큰 예측값을 갖는 상자로 IoU 값 찾기 \n",
    "    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1),\n",
    "                             axis=-1)\n",
    "\n",
    "    # 가장 큰 iou가 임계값보다 작으면 예측 상자에 개체가 포함되지 않은 것으로 간주되고 배경 상자로 설정 \n",
    "    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < iou_loss_thresh, tf.float32 )\n",
    "\n",
    "    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n",
    "\n",
    "    # confidence의 손실 계산  \n",
    "    # 그리드에 객체가 포함된 경우 1, 그렇지 않을경우 0  \n",
    "    conf_loss = conf_focal * (\n",
    "        respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=respond_bbox, logits=conv_raw_conf)\n",
    "        + \n",
    "        respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=respond_bbox, logits=conv_raw_conf) \n",
    "    )\n",
    "\n",
    "    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=label_prob, logits=conv_raw_prob)\n",
    "\n",
    "    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4])) \n",
    "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4])) \n",
    "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4])) \n",
    "\n",
    "    return giou_loss, conf_loss, prob_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427522d",
   "metadata": {},
   "source": [
    "## 학습 단계 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e0b7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(image_data, target, \n",
    "               num_class=80, lr_init=1e-4, lr_end=1e-6):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_result = yolo(image_data, training=True)\n",
    "        giou_loss = conf_loss = prob_loss = 0\n",
    "\n",
    "        # 손실값 계산 \n",
    "        grid = 3\n",
    "        for i in range(grid):\n",
    "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
    "            loss_items = compute_loss(pred, conv, *target[i],\n",
    "                                      i, num_class=NUM_CLASS)\n",
    "            giou_loss += loss_items[0]\n",
    "            conf_loss += loss_items[1]\n",
    "            prob_loss += loss_items[2]\n",
    "\n",
    "        total_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, \n",
    "                                  yolo.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,\n",
    "                                      yolo.trainable_variables))\n",
    "\n",
    "        # 학습률 업데이트 \n",
    "        # 워밍업 참고: https://arxiv.org/abs/1812.01187\n",
    "        global_steps.assign_add(1)\n",
    "        if global_steps < warmup_steps:\n",
    "            lr = global_steps / warmup_steps * lr_init\n",
    "        else:\n",
    "            lr = lr_end + 0.5 * (lr_init - lr_end) * ((1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi)))\n",
    "    optimizer.lr.assign(lr.numpy())\n",
    "\n",
    "    # Loss를 로그에 저장 \n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"lr\", optimizer.lr,\n",
    "                          step=global_steps)\n",
    "        tf.summary.scalar(\"loss/total_loss\", total_loss,\n",
    "                          step=global_steps)\n",
    "        tf.summary.scalar(\"loss/giou_loss\", giou_loss,\n",
    "                          step=global_steps)\n",
    "        tf.summary.scalar(\"loss/conf_loss\", conf_loss,\n",
    "                          step=global_steps)\n",
    "        tf.summary.scalar(\"loss/prob_loss\", prob_loss,\n",
    "                          step=global_steps)\n",
    "        writer.flush()\n",
    "\n",
    "    return global_steps.numpy(), optimizer.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8222f2",
   "metadata": {},
   "source": [
    "## 검증 단계 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097777c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(image_data, target, num_class=80):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_result = yolo(image_data, training=False)\n",
    "        giou_loss = conf_loss = prob_loss = 0 \n",
    "\n",
    "        grid = 3 \n",
    "        for i in range(grid):\n",
    "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
    "            loss_items = compute_loss(pred, conv, *target[i],\n",
    "                                      i, num_class=num_class)\n",
    "            giou_loss += loss_items[0]\n",
    "            conf_loss += loss_items[1]\n",
    "            prob_loss += loss_items[2]\n",
    "\n",
    "        total_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "    return giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a40aa3",
   "metadata": {},
   "source": [
    "## 데이터 생성기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdcd721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_EPOCHS = 2\n",
    "EPOCHS = 100\n",
    "\n",
    "from data import DataGenerator \n",
    "\n",
    "trainset = DataGenerator(data_path=\"/mnist_train\",\n",
    "                         annot_path=\"mnist_train.txt\",\n",
    "                         class_label_path=\"mnist.names\")\n",
    "testset = DataGenerator(data_path=\"/mnist_test\", \n",
    "                        annot_path=\"mnist_test.txt\",\n",
    "                        class_label_path=\"mnist.names\")\n",
    "steps_per_epoch = len(trainset)\n",
    "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64) \n",
    "warmup_steps = WARMUP_EPOCHS * steps_per_epoch\n",
    "total_steps = EPOCHS * steps_per_epoch\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174a711",
   "metadata": {},
   "source": [
    "## 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1ab4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step:    2/250, lr:0.000000, giou_loss:  27.58, conf_loss:1846.81, prob_loss: 151.17, total_loss:2025.56\n",
      "epoch: 0 step:    3/250, lr:0.000001, giou_loss:  25.45, conf_loss:1772.53, prob_loss:  96.68, total_loss:1894.66\n",
      "epoch: 0 step:    4/250, lr:0.000001, giou_loss:  27.94, conf_loss:1766.89, prob_loss: 119.92, total_loss:1914.75\n",
      "epoch: 0 step:    5/250, lr:0.000001, giou_loss:  32.58, conf_loss:1734.60, prob_loss: 118.55, total_loss:1885.72\n",
      "epoch: 0 step:    6/250, lr:0.000001, giou_loss:  30.78, conf_loss:1750.60, prob_loss: 126.14, total_loss:1907.51\n",
      "epoch: 0 step:    7/250, lr:0.000001, giou_loss:  23.23, conf_loss:1762.60, prob_loss:  86.87, total_loss:1872.69\n",
      "epoch: 0 step:    8/250, lr:0.000002, giou_loss:  30.45, conf_loss:1752.72, prob_loss: 113.90, total_loss:1897.07\n",
      "epoch: 0 step:    9/250, lr:0.000002, giou_loss:  23.38, conf_loss:1753.64, prob_loss:  66.52, total_loss:1843.53\n",
      "epoch: 0 step:   10/250, lr:0.000002, giou_loss:  27.18, conf_loss:1725.81, prob_loss: 119.50, total_loss:1872.49\n",
      "epoch: 0 step:   11/250, lr:0.000002, giou_loss:  27.99, conf_loss:1720.05, prob_loss: 110.24, total_loss:1858.28\n",
      "epoch: 0 step:   12/250, lr:0.000002, giou_loss:  32.84, conf_loss:1714.02, prob_loss: 111.41, total_loss:1858.26\n",
      "epoch: 0 step:   13/250, lr:0.000003, giou_loss:  27.63, conf_loss:1694.78, prob_loss:  91.68, total_loss:1814.08\n",
      "epoch: 0 step:   14/250, lr:0.000003, giou_loss:  26.79, conf_loss:1702.46, prob_loss:  91.95, total_loss:1821.20\n",
      "epoch: 0 step:   15/250, lr:0.000003, giou_loss:  30.18, conf_loss:1690.13, prob_loss: 114.52, total_loss:1834.83\n",
      "epoch: 0 step:   16/250, lr:0.000003, giou_loss:  28.79, conf_loss:1674.09, prob_loss: 129.00, total_loss:1831.89\n",
      "epoch: 0 step:   17/250, lr:0.000003, giou_loss:  31.02, conf_loss:1650.95, prob_loss: 118.26, total_loss:1800.23\n",
      "epoch: 0 step:   18/250, lr:0.000004, giou_loss:  30.55, conf_loss:1649.66, prob_loss: 110.20, total_loss:1790.41\n",
      "epoch: 0 step:   19/250, lr:0.000004, giou_loss:  26.14, conf_loss:1630.95, prob_loss:  89.23, total_loss:1746.32\n",
      "epoch: 0 step:   20/250, lr:0.000004, giou_loss:  22.01, conf_loss:1636.30, prob_loss:  81.14, total_loss:1739.45\n",
      "epoch: 0 step:   21/250, lr:0.000004, giou_loss:  30.07, conf_loss:1603.16, prob_loss: 118.51, total_loss:1751.74\n",
      "epoch: 0 step:   22/250, lr:0.000004, giou_loss:  25.27, conf_loss:1611.27, prob_loss:  79.69, total_loss:1716.23\n",
      "epoch: 0 step:   23/250, lr:0.000005, giou_loss:  20.95, conf_loss:1602.85, prob_loss:  69.03, total_loss:1692.83\n",
      "epoch: 0 step:   24/250, lr:0.000005, giou_loss:  23.98, conf_loss:1595.86, prob_loss:  84.44, total_loss:1704.28\n",
      "epoch: 0 step:   25/250, lr:0.000005, giou_loss:  31.82, conf_loss:1563.06, prob_loss: 122.63, total_loss:1717.50\n",
      "epoch: 0 step:   26/250, lr:0.000005, giou_loss:  19.98, conf_loss:1570.22, prob_loss:  76.93, total_loss:1667.13\n",
      "epoch: 0 step:   27/250, lr:0.000005, giou_loss:  16.89, conf_loss:1566.68, prob_loss:  52.40, total_loss:1635.97\n",
      "epoch: 0 step:   28/250, lr:0.000006, giou_loss:  20.93, conf_loss:1534.39, prob_loss:  70.63, total_loss:1625.95\n",
      "epoch: 0 step:   29/250, lr:0.000006, giou_loss:  27.98, conf_loss:1520.99, prob_loss: 123.17, total_loss:1672.15\n",
      "epoch: 0 step:   30/250, lr:0.000006, giou_loss:  13.17, conf_loss:1543.23, prob_loss:  47.81, total_loss:1604.20\n",
      "epoch: 0 step:   31/250, lr:0.000006, giou_loss:  20.12, conf_loss:1495.45, prob_loss:  78.81, total_loss:1594.38\n",
      "epoch: 0 step:   32/250, lr:0.000006, giou_loss:  20.44, conf_loss:1488.25, prob_loss:  82.25, total_loss:1590.94\n",
      "epoch: 0 step:   33/250, lr:0.000007, giou_loss:  21.55, conf_loss:1472.74, prob_loss:  86.13, total_loss:1580.41\n",
      "epoch: 0 step:   34/250, lr:0.000007, giou_loss:  25.36, conf_loss:1458.40, prob_loss: 113.40, total_loss:1597.16\n",
      "epoch: 0 step:   35/250, lr:0.000007, giou_loss:  24.99, conf_loss:1446.34, prob_loss: 107.72, total_loss:1579.06\n",
      "epoch: 0 step:   36/250, lr:0.000007, giou_loss:  23.50, conf_loss:1444.26, prob_loss: 114.34, total_loss:1582.11\n",
      "epoch: 0 step:   37/250, lr:0.000007, giou_loss:  28.27, conf_loss:1434.33, prob_loss: 140.64, total_loss:1603.24\n",
      "epoch: 0 step:   38/250, lr:0.000008, giou_loss:  13.34, conf_loss:1424.90, prob_loss:  55.12, total_loss:1493.36\n",
      "epoch: 0 step:   39/250, lr:0.000008, giou_loss:  15.58, conf_loss:1450.78, prob_loss:  61.44, total_loss:1527.79\n",
      "epoch: 0 step:   40/250, lr:0.000008, giou_loss:  25.24, conf_loss:1400.78, prob_loss: 112.36, total_loss:1538.38\n",
      "epoch: 0 step:   41/250, lr:0.000008, giou_loss:  13.52, conf_loss:1398.22, prob_loss:  57.66, total_loss:1469.41\n",
      "epoch: 0 step:   42/250, lr:0.000008, giou_loss:  20.10, conf_loss:1375.59, prob_loss:  89.31, total_loss:1484.99\n",
      "epoch: 0 step:   43/250, lr:0.000009, giou_loss:  19.00, conf_loss:1377.57, prob_loss:  91.59, total_loss:1488.16\n",
      "epoch: 0 step:   44/250, lr:0.000009, giou_loss:  20.52, conf_loss:1369.84, prob_loss:  90.02, total_loss:1480.38\n",
      "epoch: 0 step:   45/250, lr:0.000009, giou_loss:  20.95, conf_loss:1353.95, prob_loss:  94.51, total_loss:1469.41\n",
      "epoch: 0 step:   46/250, lr:0.000009, giou_loss:  15.09, conf_loss:1352.14, prob_loss:  66.49, total_loss:1433.73\n",
      "epoch: 0 step:   47/250, lr:0.000009, giou_loss:  18.67, conf_loss:1335.89, prob_loss:  94.30, total_loss:1448.86\n",
      "epoch: 0 step:   48/250, lr:0.000010, giou_loss:  12.90, conf_loss:1336.82, prob_loss:  61.89, total_loss:1411.61\n",
      "epoch: 0 step:   49/250, lr:0.000010, giou_loss:  16.59, conf_loss:1322.66, prob_loss:  85.81, total_loss:1425.06\n",
      "epoch: 0 step:   50/250, lr:0.000010, giou_loss:  16.05, conf_loss:1313.64, prob_loss:  80.79, total_loss:1410.48\n",
      "epoch: 0 step:   51/250, lr:0.000010, giou_loss:  12.91, conf_loss:1309.57, prob_loss:  65.78, total_loss:1388.27\n",
      "epoch: 0 step:   52/250, lr:0.000010, giou_loss:  16.05, conf_loss:1299.80, prob_loss:  82.10, total_loss:1397.95\n",
      "epoch: 0 step:   53/250, lr:0.000011, giou_loss:  17.79, conf_loss:1292.91, prob_loss:  89.26, total_loss:1399.95\n",
      "epoch: 0 step:   54/250, lr:0.000011, giou_loss:  15.93, conf_loss:1281.33, prob_loss:  79.41, total_loss:1376.67\n",
      "epoch: 0 step:   55/250, lr:0.000011, giou_loss:  15.23, conf_loss:1273.98, prob_loss:  78.26, total_loss:1367.47\n",
      "epoch: 0 step:   56/250, lr:0.000011, giou_loss:  13.57, conf_loss:1264.39, prob_loss:  68.77, total_loss:1346.73\n",
      "epoch: 0 step:   57/250, lr:0.000011, giou_loss:  14.13, conf_loss:1254.57, prob_loss:  71.52, total_loss:1340.22\n",
      "epoch: 0 step:   58/250, lr:0.000012, giou_loss:  13.72, conf_loss:1250.33, prob_loss:  69.43, total_loss:1333.49\n",
      "epoch: 0 step:   59/250, lr:0.000012, giou_loss:  16.25, conf_loss:1246.77, prob_loss:  86.23, total_loss:1349.25\n",
      "epoch: 0 step:   60/250, lr:0.000012, giou_loss:  21.28, conf_loss:1246.16, prob_loss: 113.61, total_loss:1381.05\n",
      "epoch: 0 step:   61/250, lr:0.000012, giou_loss:  17.07, conf_loss:1229.03, prob_loss:  87.96, total_loss:1334.07\n",
      "epoch: 0 step:   62/250, lr:0.000012, giou_loss:  13.32, conf_loss:1221.55, prob_loss:  69.33, total_loss:1304.20\n",
      "epoch: 0 step:   63/250, lr:0.000013, giou_loss:  19.49, conf_loss:1218.89, prob_loss:  97.30, total_loss:1335.67\n",
      "epoch: 0 step:   64/250, lr:0.000013, giou_loss:  12.43, conf_loss:1205.11, prob_loss:  58.78, total_loss:1276.32\n",
      "epoch: 0 step:   65/250, lr:0.000013, giou_loss:  16.25, conf_loss:1201.82, prob_loss:  88.58, total_loss:1306.64\n",
      "epoch: 0 step:   66/250, lr:0.000013, giou_loss:  13.93, conf_loss:1188.78, prob_loss:  74.47, total_loss:1277.18\n",
      "epoch: 0 step:   67/250, lr:0.000013, giou_loss:  10.57, conf_loss:1185.70, prob_loss:  52.90, total_loss:1249.17\n",
      "epoch: 0 step:   68/250, lr:0.000014, giou_loss:  11.45, conf_loss:1179.48, prob_loss:  60.40, total_loss:1251.34\n",
      "epoch: 0 step:   69/250, lr:0.000014, giou_loss:  15.61, conf_loss:1183.76, prob_loss:  84.14, total_loss:1283.51\n",
      "epoch: 0 step:   70/250, lr:0.000014, giou_loss:  11.58, conf_loss:1162.60, prob_loss:  58.83, total_loss:1233.01\n",
      "epoch: 0 step:   71/250, lr:0.000014, giou_loss:  17.10, conf_loss:1167.01, prob_loss:  89.92, total_loss:1274.02\n",
      "epoch: 0 step:   72/250, lr:0.000014, giou_loss:  17.28, conf_loss:1154.03, prob_loss:  92.29, total_loss:1263.61\n",
      "epoch: 0 step:   73/250, lr:0.000015, giou_loss:  12.96, conf_loss:1147.18, prob_loss:  68.81, total_loss:1228.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step:   74/250, lr:0.000015, giou_loss:  18.15, conf_loss:1138.82, prob_loss:  93.82, total_loss:1250.79\n",
      "epoch: 0 step:   75/250, lr:0.000015, giou_loss:  11.92, conf_loss:1128.93, prob_loss:  66.30, total_loss:1207.15\n",
      "epoch: 0 step:   76/250, lr:0.000015, giou_loss:  11.74, conf_loss:1125.84, prob_loss:  59.57, total_loss:1197.15\n",
      "epoch: 0 step:   77/250, lr:0.000015, giou_loss:  14.41, conf_loss:1121.05, prob_loss:  78.36, total_loss:1213.83\n",
      "epoch: 0 step:   78/250, lr:0.000016, giou_loss:  10.70, conf_loss:1113.08, prob_loss:  55.03, total_loss:1178.81\n",
      "epoch: 0 step:   79/250, lr:0.000016, giou_loss:  11.85, conf_loss:1106.32, prob_loss:  64.76, total_loss:1182.93\n",
      "epoch: 0 step:   80/250, lr:0.000016, giou_loss:  10.50, conf_loss:1096.71, prob_loss:  56.91, total_loss:1164.12\n",
      "epoch: 0 step:   81/250, lr:0.000016, giou_loss:  12.05, conf_loss:1092.16, prob_loss:  67.25, total_loss:1171.46\n",
      "epoch: 0 step:   82/250, lr:0.000016, giou_loss:  13.38, conf_loss:1092.41, prob_loss:  73.03, total_loss:1178.82\n",
      "epoch: 0 step:   83/250, lr:0.000017, giou_loss:  13.37, conf_loss:1083.09, prob_loss:  75.58, total_loss:1172.04\n",
      "epoch: 0 step:   84/250, lr:0.000017, giou_loss:  13.39, conf_loss:1076.01, prob_loss:  76.88, total_loss:1166.28\n",
      "epoch: 0 step:   85/250, lr:0.000017, giou_loss:  15.58, conf_loss:1083.08, prob_loss:  82.27, total_loss:1180.93\n",
      "epoch: 0 step:   86/250, lr:0.000017, giou_loss:  13.51, conf_loss:1059.24, prob_loss:  73.86, total_loss:1146.61\n",
      "epoch: 0 step:   87/250, lr:0.000017, giou_loss:  13.48, conf_loss:1058.55, prob_loss:  67.78, total_loss:1139.80\n",
      "epoch: 0 step:   88/250, lr:0.000018, giou_loss:  13.53, conf_loss:1051.37, prob_loss:  69.17, total_loss:1134.08\n",
      "epoch: 0 step:   89/250, lr:0.000018, giou_loss:  12.54, conf_loss:1047.48, prob_loss:  71.96, total_loss:1131.98\n",
      "epoch: 0 step:   90/250, lr:0.000018, giou_loss:  12.31, conf_loss:1034.30, prob_loss:  68.59, total_loss:1115.20\n",
      "epoch: 0 step:   91/250, lr:0.000018, giou_loss:  16.28, conf_loss:1046.68, prob_loss:  89.25, total_loss:1152.21\n",
      "epoch: 0 step:   92/250, lr:0.000018, giou_loss:  12.92, conf_loss:1029.07, prob_loss:  72.03, total_loss:1114.03\n",
      "epoch: 0 step:   93/250, lr:0.000019, giou_loss:  10.66, conf_loss:1021.80, prob_loss:  59.89, total_loss:1092.36\n",
      "epoch: 0 step:   94/250, lr:0.000019, giou_loss:   9.41, conf_loss:1028.69, prob_loss:  45.37, total_loss:1083.47\n",
      "epoch: 0 step:   95/250, lr:0.000019, giou_loss:  10.58, conf_loss:1015.05, prob_loss:  56.40, total_loss:1082.03\n",
      "epoch: 0 step:   96/250, lr:0.000019, giou_loss:  13.91, conf_loss:1007.11, prob_loss:  72.61, total_loss:1093.62\n",
      "epoch: 0 step:   97/250, lr:0.000019, giou_loss:  10.98, conf_loss: 998.85, prob_loss:  60.08, total_loss:1069.91\n",
      "epoch: 0 step:   98/250, lr:0.000020, giou_loss:  19.03, conf_loss:1010.86, prob_loss:  98.30, total_loss:1128.20\n",
      "epoch: 0 step:   99/250, lr:0.000020, giou_loss:  18.37, conf_loss:1014.88, prob_loss: 100.88, total_loss:1134.13\n",
      "epoch: 0 step:  100/250, lr:0.000020, giou_loss:  10.10, conf_loss: 989.43, prob_loss:  51.44, total_loss:1050.96\n",
      "epoch: 0 step:  101/250, lr:0.000020, giou_loss:  10.79, conf_loss: 981.23, prob_loss:  54.00, total_loss:1046.01\n",
      "epoch: 0 step:  102/250, lr:0.000020, giou_loss:  14.27, conf_loss: 980.29, prob_loss:  74.86, total_loss:1069.42\n",
      "epoch: 0 step:  103/250, lr:0.000021, giou_loss:  11.95, conf_loss: 966.21, prob_loss:  62.42, total_loss:1040.59\n",
      "epoch: 0 step:  104/250, lr:0.000021, giou_loss:  12.02, conf_loss: 968.04, prob_loss:  67.73, total_loss:1047.78\n",
      "epoch: 0 step:  105/250, lr:0.000021, giou_loss:  19.17, conf_loss: 982.33, prob_loss: 110.00, total_loss:1111.50\n",
      "epoch: 0 step:  106/250, lr:0.000021, giou_loss:  12.90, conf_loss: 962.17, prob_loss:  71.37, total_loss:1046.44\n",
      "epoch: 0 step:  107/250, lr:0.000021, giou_loss:  13.04, conf_loss: 950.97, prob_loss:  76.38, total_loss:1040.40\n",
      "epoch: 0 step:  108/250, lr:0.000022, giou_loss:   9.21, conf_loss: 948.92, prob_loss:  49.43, total_loss:1007.56\n",
      "epoch: 0 step:  109/250, lr:0.000022, giou_loss:  11.03, conf_loss: 945.54, prob_loss:  65.77, total_loss:1022.34\n",
      "epoch: 0 step:  110/250, lr:0.000022, giou_loss:  10.64, conf_loss: 936.33, prob_loss:  61.05, total_loss:1008.03\n",
      "epoch: 0 step:  111/250, lr:0.000022, giou_loss:   7.09, conf_loss: 927.79, prob_loss:  40.17, total_loss: 975.05\n",
      "epoch: 0 step:  112/250, lr:0.000022, giou_loss:  12.21, conf_loss: 921.40, prob_loss:  74.42, total_loss:1008.02\n",
      "epoch: 0 step:  113/250, lr:0.000023, giou_loss:  12.70, conf_loss: 923.55, prob_loss:  73.64, total_loss:1009.89\n",
      "epoch: 0 step:  114/250, lr:0.000023, giou_loss:  10.69, conf_loss: 904.71, prob_loss:  59.20, total_loss: 974.60\n",
      "epoch: 0 step:  115/250, lr:0.000023, giou_loss:   8.39, conf_loss: 903.37, prob_loss:  47.97, total_loss: 959.73\n",
      "epoch: 0 step:  116/250, lr:0.000023, giou_loss:  11.15, conf_loss: 904.12, prob_loss:  66.13, total_loss: 981.40\n",
      "epoch: 0 step:  117/250, lr:0.000023, giou_loss:  11.58, conf_loss: 900.23, prob_loss:  61.84, total_loss: 973.65\n",
      "epoch: 0 step:  118/250, lr:0.000024, giou_loss:  12.04, conf_loss: 897.04, prob_loss:  67.28, total_loss: 976.36\n",
      "epoch: 0 step:  119/250, lr:0.000024, giou_loss:   9.04, conf_loss: 885.51, prob_loss:  47.11, total_loss: 941.66\n",
      "epoch: 0 step:  120/250, lr:0.000024, giou_loss:  12.06, conf_loss: 883.62, prob_loss:  70.84, total_loss: 966.51\n",
      "epoch: 0 step:  121/250, lr:0.000024, giou_loss:  11.68, conf_loss: 878.38, prob_loss:  65.19, total_loss: 955.24\n",
      "epoch: 0 step:  122/250, lr:0.000024, giou_loss:  10.71, conf_loss: 874.38, prob_loss:  58.60, total_loss: 943.70\n",
      "epoch: 0 step:  123/250, lr:0.000025, giou_loss:  10.30, conf_loss: 862.89, prob_loss:  58.32, total_loss: 931.51\n",
      "epoch: 0 step:  124/250, lr:0.000025, giou_loss:  11.46, conf_loss: 860.57, prob_loss:  62.90, total_loss: 934.93\n",
      "epoch: 0 step:  125/250, lr:0.000025, giou_loss:   9.33, conf_loss: 856.17, prob_loss:  50.66, total_loss: 916.16\n",
      "epoch: 0 step:  126/250, lr:0.000025, giou_loss:  12.78, conf_loss: 850.28, prob_loss:  73.28, total_loss: 936.34\n",
      "epoch: 0 step:  127/250, lr:0.000025, giou_loss:  11.19, conf_loss: 843.97, prob_loss:  60.86, total_loss: 916.02\n",
      "epoch: 0 step:  128/250, lr:0.000026, giou_loss:  12.05, conf_loss: 853.17, prob_loss:  58.26, total_loss: 923.48\n",
      "epoch: 0 step:  129/250, lr:0.000026, giou_loss:  12.48, conf_loss: 846.74, prob_loss:  70.55, total_loss: 929.77\n",
      "epoch: 0 step:  130/250, lr:0.000026, giou_loss:  12.18, conf_loss: 850.43, prob_loss:  69.31, total_loss: 931.92\n",
      "epoch: 0 step:  131/250, lr:0.000026, giou_loss:  12.78, conf_loss: 837.13, prob_loss:  69.59, total_loss: 919.50\n",
      "epoch: 0 step:  132/250, lr:0.000026, giou_loss:  10.12, conf_loss: 827.70, prob_loss:  59.62, total_loss: 897.44\n",
      "epoch: 0 step:  133/250, lr:0.000027, giou_loss:  10.89, conf_loss: 823.46, prob_loss:  58.58, total_loss: 892.93\n",
      "epoch: 0 step:  134/250, lr:0.000027, giou_loss:  18.85, conf_loss: 823.43, prob_loss:  98.84, total_loss: 941.11\n",
      "epoch: 0 step:  135/250, lr:0.000027, giou_loss:  12.35, conf_loss: 814.02, prob_loss:  63.64, total_loss: 890.02\n",
      "epoch: 0 step:  136/250, lr:0.000027, giou_loss:  16.34, conf_loss: 831.25, prob_loss:  84.59, total_loss: 932.18\n",
      "epoch: 0 step:  137/250, lr:0.000027, giou_loss:  12.54, conf_loss: 804.32, prob_loss:  65.48, total_loss: 882.34\n",
      "epoch: 0 step:  138/250, lr:0.000028, giou_loss:  11.00, conf_loss: 798.97, prob_loss:  64.63, total_loss: 874.60\n",
      "epoch: 0 step:  139/250, lr:0.000028, giou_loss:   9.77, conf_loss: 791.44, prob_loss:  52.66, total_loss: 853.87\n",
      "epoch: 0 step:  140/250, lr:0.000028, giou_loss:  10.62, conf_loss: 794.31, prob_loss:  61.43, total_loss: 866.36\n",
      "epoch: 0 step:  141/250, lr:0.000028, giou_loss:   9.04, conf_loss: 782.71, prob_loss:  45.95, total_loss: 837.70\n",
      "epoch: 0 step:  142/250, lr:0.000028, giou_loss:  17.23, conf_loss: 786.10, prob_loss:  95.24, total_loss: 898.57\n",
      "epoch: 0 step:  143/250, lr:0.000029, giou_loss:  11.02, conf_loss: 780.07, prob_loss:  64.47, total_loss: 855.56\n",
      "epoch: 0 step:  144/250, lr:0.000029, giou_loss:  15.08, conf_loss: 784.27, prob_loss:  89.11, total_loss: 888.46\n",
      "epoch: 0 step:  145/250, lr:0.000029, giou_loss:  18.33, conf_loss: 795.03, prob_loss: 104.33, total_loss: 917.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step:  146/250, lr:0.000029, giou_loss:  11.01, conf_loss: 766.36, prob_loss:  55.16, total_loss: 832.53\n",
      "epoch: 0 step:  147/250, lr:0.000029, giou_loss:  14.06, conf_loss: 764.96, prob_loss:  77.33, total_loss: 856.35\n",
      "epoch: 0 step:  148/250, lr:0.000030, giou_loss:  18.05, conf_loss: 774.52, prob_loss: 103.79, total_loss: 896.36\n",
      "epoch: 0 step:  149/250, lr:0.000030, giou_loss:  10.32, conf_loss: 753.04, prob_loss:  60.62, total_loss: 823.98\n",
      "epoch: 0 step:  150/250, lr:0.000030, giou_loss:  12.96, conf_loss: 745.77, prob_loss:  66.57, total_loss: 825.31\n",
      "epoch: 0 step:  151/250, lr:0.000030, giou_loss:  10.35, conf_loss: 736.08, prob_loss:  55.26, total_loss: 801.69\n",
      "epoch: 0 step:  152/250, lr:0.000030, giou_loss:  10.01, conf_loss: 735.34, prob_loss:  59.11, total_loss: 804.45\n",
      "epoch: 0 step:  153/250, lr:0.000031, giou_loss:   8.70, conf_loss: 736.57, prob_loss:  48.57, total_loss: 793.83\n",
      "epoch: 0 step:  154/250, lr:0.000031, giou_loss:  12.51, conf_loss: 741.73, prob_loss:  76.11, total_loss: 830.35\n",
      "epoch: 0 step:  155/250, lr:0.000031, giou_loss:  13.86, conf_loss: 736.66, prob_loss:  81.62, total_loss: 832.14\n",
      "epoch: 0 step:  156/250, lr:0.000031, giou_loss:  12.49, conf_loss: 721.03, prob_loss:  70.71, total_loss: 804.23\n",
      "epoch: 0 step:  157/250, lr:0.000031, giou_loss:  10.72, conf_loss: 718.64, prob_loss:  62.05, total_loss: 791.41\n",
      "epoch: 0 step:  158/250, lr:0.000032, giou_loss:   9.71, conf_loss: 708.82, prob_loss:  51.36, total_loss: 769.89\n",
      "epoch: 0 step:  159/250, lr:0.000032, giou_loss:  11.34, conf_loss: 715.57, prob_loss:  63.59, total_loss: 790.51\n",
      "epoch: 0 step:  160/250, lr:0.000032, giou_loss:  18.96, conf_loss: 730.53, prob_loss: 105.85, total_loss: 855.34\n",
      "epoch: 0 step:  161/250, lr:0.000032, giou_loss:  11.86, conf_loss: 699.60, prob_loss:  71.79, total_loss: 783.25\n",
      "epoch: 0 step:  162/250, lr:0.000032, giou_loss:  14.22, conf_loss: 699.02, prob_loss:  77.44, total_loss: 790.68\n",
      "epoch: 0 step:  163/250, lr:0.000033, giou_loss:   9.81, conf_loss: 690.14, prob_loss:  51.54, total_loss: 751.49\n",
      "epoch: 0 step:  164/250, lr:0.000033, giou_loss:  13.47, conf_loss: 693.37, prob_loss:  75.30, total_loss: 782.14\n",
      "epoch: 0 step:  165/250, lr:0.000033, giou_loss:  16.34, conf_loss: 692.61, prob_loss:  88.91, total_loss: 797.87\n",
      "epoch: 0 step:  166/250, lr:0.000033, giou_loss:   8.68, conf_loss: 670.00, prob_loss:  51.02, total_loss: 729.70\n",
      "epoch: 0 step:  167/250, lr:0.000033, giou_loss:   8.83, conf_loss: 671.11, prob_loss:  51.44, total_loss: 731.38\n",
      "epoch: 0 step:  168/250, lr:0.000034, giou_loss:  14.55, conf_loss: 683.29, prob_loss:  77.08, total_loss: 774.92\n",
      "epoch: 0 step:  169/250, lr:0.000034, giou_loss:  12.30, conf_loss: 671.17, prob_loss:  71.90, total_loss: 755.37\n",
      "epoch: 0 step:  170/250, lr:0.000034, giou_loss:  12.11, conf_loss: 659.14, prob_loss:  67.71, total_loss: 738.96\n",
      "epoch: 0 step:  171/250, lr:0.000034, giou_loss:  10.78, conf_loss: 655.19, prob_loss:  55.25, total_loss: 721.23\n",
      "epoch: 0 step:  172/250, lr:0.000034, giou_loss:  10.65, conf_loss: 649.46, prob_loss:  64.18, total_loss: 724.29\n",
      "epoch: 0 step:  173/250, lr:0.000035, giou_loss:  12.04, conf_loss: 643.23, prob_loss:  64.34, total_loss: 719.61\n",
      "epoch: 0 step:  174/250, lr:0.000035, giou_loss:  10.98, conf_loss: 642.70, prob_loss:  51.49, total_loss: 705.16\n",
      "epoch: 0 step:  175/250, lr:0.000035, giou_loss:  10.36, conf_loss: 635.54, prob_loss:  52.47, total_loss: 698.37\n",
      "epoch: 0 step:  176/250, lr:0.000035, giou_loss:  10.75, conf_loss: 628.87, prob_loss:  61.87, total_loss: 701.48\n",
      "epoch: 0 step:  177/250, lr:0.000035, giou_loss:   9.56, conf_loss: 631.57, prob_loss:  52.40, total_loss: 693.54\n",
      "epoch: 0 step:  178/250, lr:0.000036, giou_loss:   9.20, conf_loss: 622.90, prob_loss:  49.63, total_loss: 681.74\n",
      "epoch: 0 step:  179/250, lr:0.000036, giou_loss:  10.00, conf_loss: 622.59, prob_loss:  51.34, total_loss: 683.93\n",
      "epoch: 0 step:  180/250, lr:0.000036, giou_loss:  13.41, conf_loss: 623.95, prob_loss:  77.74, total_loss: 715.10\n",
      "epoch: 0 step:  181/250, lr:0.000036, giou_loss:  12.00, conf_loss: 631.00, prob_loss:  68.43, total_loss: 711.43\n",
      "epoch: 0 step:  182/250, lr:0.000036, giou_loss:   9.28, conf_loss: 608.82, prob_loss:  42.73, total_loss: 660.83\n",
      "epoch: 0 step:  183/250, lr:0.000037, giou_loss:  12.40, conf_loss: 615.06, prob_loss:  67.66, total_loss: 695.11\n",
      "epoch: 0 step:  184/250, lr:0.000037, giou_loss:  11.25, conf_loss: 608.69, prob_loss:  63.47, total_loss: 683.40\n",
      "epoch: 0 step:  185/250, lr:0.000037, giou_loss:   9.31, conf_loss: 599.66, prob_loss:  51.72, total_loss: 660.69\n",
      "epoch: 0 step:  186/250, lr:0.000037, giou_loss:   6.67, conf_loss: 589.65, prob_loss:  37.04, total_loss: 633.36\n",
      "epoch: 0 step:  187/250, lr:0.000037, giou_loss:   9.63, conf_loss: 590.76, prob_loss:  55.40, total_loss: 655.80\n",
      "epoch: 0 step:  188/250, lr:0.000038, giou_loss:   6.20, conf_loss: 585.41, prob_loss:  34.99, total_loss: 626.61\n",
      "epoch: 0 step:  189/250, lr:0.000038, giou_loss:   7.61, conf_loss: 579.30, prob_loss:  44.59, total_loss: 631.51\n",
      "epoch: 0 step:  190/250, lr:0.000038, giou_loss:   7.76, conf_loss: 574.84, prob_loss:  37.62, total_loss: 620.22\n",
      "epoch: 0 step:  191/250, lr:0.000038, giou_loss:   9.15, conf_loss: 577.14, prob_loss:  54.27, total_loss: 640.56\n",
      "epoch: 0 step:  192/250, lr:0.000038, giou_loss:   7.37, conf_loss: 567.01, prob_loss:  42.67, total_loss: 617.05\n",
      "epoch: 0 step:  193/250, lr:0.000039, giou_loss:  13.53, conf_loss: 581.03, prob_loss:  71.00, total_loss: 665.57\n",
      "epoch: 0 step:  194/250, lr:0.000039, giou_loss:  10.17, conf_loss: 572.28, prob_loss:  58.26, total_loss: 640.71\n",
      "epoch: 0 step:  195/250, lr:0.000039, giou_loss:   7.32, conf_loss: 553.33, prob_loss:  41.31, total_loss: 601.95\n",
      "epoch: 0 step:  196/250, lr:0.000039, giou_loss:  12.65, conf_loss: 562.12, prob_loss:  69.25, total_loss: 644.01\n",
      "epoch: 0 step:  197/250, lr:0.000039, giou_loss:   8.74, conf_loss: 558.70, prob_loss:  45.83, total_loss: 613.27\n",
      "epoch: 0 step:  198/250, lr:0.000040, giou_loss:  11.77, conf_loss: 549.17, prob_loss:  58.12, total_loss: 619.06\n",
      "epoch: 0 step:  199/250, lr:0.000040, giou_loss:  11.96, conf_loss: 548.22, prob_loss:  62.89, total_loss: 623.07\n",
      "epoch: 0 step:  200/250, lr:0.000040, giou_loss:   8.94, conf_loss: 546.95, prob_loss:  41.93, total_loss: 597.82\n",
      "epoch: 0 step:  201/250, lr:0.000040, giou_loss:   8.74, conf_loss: 536.63, prob_loss:  52.83, total_loss: 598.20\n",
      "epoch: 0 step:  202/250, lr:0.000040, giou_loss:   9.96, conf_loss: 535.41, prob_loss:  48.17, total_loss: 593.54\n",
      "epoch: 0 step:  203/250, lr:0.000041, giou_loss:   8.68, conf_loss: 528.73, prob_loss:  48.20, total_loss: 585.61\n",
      "epoch: 0 step:  204/250, lr:0.000041, giou_loss:  15.46, conf_loss: 533.71, prob_loss:  82.46, total_loss: 631.62\n",
      "epoch: 0 step:  205/250, lr:0.000041, giou_loss:   7.77, conf_loss: 524.66, prob_loss:  45.74, total_loss: 578.18\n",
      "epoch: 0 step:  206/250, lr:0.000041, giou_loss:  14.52, conf_loss: 528.05, prob_loss:  78.06, total_loss: 620.64\n",
      "epoch: 0 step:  207/250, lr:0.000041, giou_loss:  15.33, conf_loss: 531.45, prob_loss:  83.12, total_loss: 629.89\n",
      "epoch: 0 step:  208/250, lr:0.000042, giou_loss:  14.30, conf_loss: 518.32, prob_loss:  70.18, total_loss: 602.80\n",
      "epoch: 0 step:  209/250, lr:0.000042, giou_loss:   6.97, conf_loss: 514.78, prob_loss:  37.45, total_loss: 559.21\n",
      "epoch: 0 step:  210/250, lr:0.000042, giou_loss:   8.91, conf_loss: 501.93, prob_loss:  52.17, total_loss: 563.01\n",
      "epoch: 0 step:  211/250, lr:0.000042, giou_loss:  12.15, conf_loss: 504.12, prob_loss:  64.21, total_loss: 580.47\n",
      "epoch: 0 step:  212/250, lr:0.000042, giou_loss:   7.68, conf_loss: 503.04, prob_loss:  40.85, total_loss: 551.57\n",
      "epoch: 0 step:  213/250, lr:0.000043, giou_loss:  14.05, conf_loss: 504.81, prob_loss:  73.63, total_loss: 592.49\n",
      "epoch: 0 step:  214/250, lr:0.000043, giou_loss:   8.74, conf_loss: 496.51, prob_loss:  49.99, total_loss: 555.24\n",
      "epoch: 0 step:  215/250, lr:0.000043, giou_loss:  11.65, conf_loss: 493.90, prob_loss:  68.74, total_loss: 574.29\n",
      "epoch: 0 step:  216/250, lr:0.000043, giou_loss:   8.24, conf_loss: 486.38, prob_loss:  46.95, total_loss: 541.57\n",
      "epoch: 0 step:  217/250, lr:0.000043, giou_loss:   9.55, conf_loss: 479.55, prob_loss:  59.33, total_loss: 548.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step:  218/250, lr:0.000044, giou_loss:   8.76, conf_loss: 484.33, prob_loss:  54.62, total_loss: 547.71\n",
      "epoch: 0 step:  219/250, lr:0.000044, giou_loss:   9.37, conf_loss: 477.36, prob_loss:  55.93, total_loss: 542.67\n",
      "epoch: 0 step:  220/250, lr:0.000044, giou_loss:   9.38, conf_loss: 473.42, prob_loss:  57.20, total_loss: 540.00\n",
      "epoch: 0 step:  221/250, lr:0.000044, giou_loss:   7.19, conf_loss: 469.01, prob_loss:  46.11, total_loss: 522.31\n",
      "epoch: 0 step:  222/250, lr:0.000044, giou_loss:   8.85, conf_loss: 467.04, prob_loss:  49.71, total_loss: 525.60\n",
      "epoch: 0 step:  223/250, lr:0.000045, giou_loss:  10.79, conf_loss: 464.09, prob_loss:  54.79, total_loss: 529.67\n",
      "epoch: 0 step:  224/250, lr:0.000045, giou_loss:  12.37, conf_loss: 465.44, prob_loss:  59.03, total_loss: 536.84\n",
      "epoch: 0 step:  225/250, lr:0.000045, giou_loss:  12.71, conf_loss: 469.25, prob_loss:  67.02, total_loss: 548.98\n",
      "epoch: 0 step:  226/250, lr:0.000045, giou_loss:  13.03, conf_loss: 459.52, prob_loss:  78.09, total_loss: 550.64\n",
      "epoch: 0 step:  227/250, lr:0.000045, giou_loss:  10.66, conf_loss: 459.45, prob_loss:  63.53, total_loss: 533.63\n",
      "epoch: 0 step:  228/250, lr:0.000046, giou_loss:   7.23, conf_loss: 447.01, prob_loss:  44.11, total_loss: 498.35\n",
      "epoch: 0 step:  229/250, lr:0.000046, giou_loss:  13.61, conf_loss: 452.76, prob_loss:  75.67, total_loss: 542.04\n",
      "epoch: 0 step:  230/250, lr:0.000046, giou_loss:  10.70, conf_loss: 444.50, prob_loss:  60.88, total_loss: 516.07\n",
      "epoch: 0 step:  231/250, lr:0.000046, giou_loss:  12.99, conf_loss: 447.07, prob_loss:  73.10, total_loss: 533.16\n",
      "epoch: 0 step:  232/250, lr:0.000046, giou_loss:   8.34, conf_loss: 435.34, prob_loss:  53.43, total_loss: 497.11\n",
      "epoch: 0 step:  233/250, lr:0.000047, giou_loss:  11.27, conf_loss: 433.78, prob_loss:  60.89, total_loss: 505.94\n",
      "epoch: 0 step:  234/250, lr:0.000047, giou_loss:  11.64, conf_loss: 431.66, prob_loss:  68.04, total_loss: 511.35\n",
      "epoch: 0 step:  235/250, lr:0.000047, giou_loss:  10.69, conf_loss: 425.29, prob_loss:  57.02, total_loss: 493.00\n",
      "epoch: 0 step:  236/250, lr:0.000047, giou_loss:   8.16, conf_loss: 419.86, prob_loss:  43.60, total_loss: 471.62\n",
      "epoch: 0 step:  237/250, lr:0.000047, giou_loss:  11.68, conf_loss: 426.35, prob_loss:  71.78, total_loss: 509.80\n",
      "epoch: 0 step:  238/250, lr:0.000048, giou_loss:   8.27, conf_loss: 421.96, prob_loss:  48.20, total_loss: 478.43\n",
      "epoch: 0 step:  239/250, lr:0.000048, giou_loss:   8.68, conf_loss: 415.37, prob_loss:  49.23, total_loss: 473.29\n",
      "epoch: 0 step:  240/250, lr:0.000048, giou_loss:  11.03, conf_loss: 410.91, prob_loss:  59.31, total_loss: 481.26\n",
      "epoch: 0 step:  241/250, lr:0.000048, giou_loss:  12.75, conf_loss: 421.97, prob_loss:  78.57, total_loss: 513.28\n",
      "epoch: 0 step:  242/250, lr:0.000048, giou_loss:   9.02, conf_loss: 406.33, prob_loss:  50.43, total_loss: 465.78\n",
      "epoch: 0 step:  243/250, lr:0.000049, giou_loss:  11.90, conf_loss: 408.73, prob_loss:  68.98, total_loss: 489.61\n",
      "epoch: 0 step:  244/250, lr:0.000049, giou_loss:  13.76, conf_loss: 407.62, prob_loss:  77.41, total_loss: 498.79\n",
      "epoch: 0 step:  245/250, lr:0.000049, giou_loss:   8.17, conf_loss: 400.96, prob_loss:  38.14, total_loss: 447.27\n",
      "epoch: 0 step:  246/250, lr:0.000049, giou_loss:  13.09, conf_loss: 399.76, prob_loss:  72.13, total_loss: 484.98\n",
      "epoch: 0 step:  247/250, lr:0.000049, giou_loss:   9.91, conf_loss: 395.42, prob_loss:  54.45, total_loss: 459.77\n",
      "epoch: 0 step:  248/250, lr:0.000050, giou_loss:   9.61, conf_loss: 396.14, prob_loss:  53.80, total_loss: 459.56\n",
      "epoch: 0 step:  249/250, lr:0.000050, giou_loss:   7.42, conf_loss: 385.57, prob_loss:  45.40, total_loss: 438.40\n",
      "epoch: 0 step:    0/250, lr:0.000050, giou_loss:  13.74, conf_loss: 392.97, prob_loss:  74.45, total_loss: 481.16\n",
      "epoch: 0 step:    1/250, lr:0.000050, giou_loss:   8.29, conf_loss: 383.13, prob_loss:  53.18, total_loss: 444.60\n",
      "\n",
      "\n",
      "giou_val_loss:  18.44, conf_val_loss: 760.09, prob_val_loss: 125.48, total_val_loss: 904.00\n",
      "\n",
      "\n",
      "epoch: 1 step:    2/250, lr:0.000050, giou_loss:  12.78, conf_loss: 382.42, prob_loss:  73.30, total_loss: 468.50\n",
      "epoch: 1 step:    3/250, lr:0.000051, giou_loss:  10.81, conf_loss: 378.41, prob_loss:  55.05, total_loss: 444.27\n",
      "epoch: 1 step:    4/250, lr:0.000051, giou_loss:  11.23, conf_loss: 379.01, prob_loss:  68.38, total_loss: 458.62\n",
      "epoch: 1 step:    5/250, lr:0.000051, giou_loss:  11.90, conf_loss: 373.26, prob_loss:  75.37, total_loss: 460.54\n",
      "epoch: 1 step:    6/250, lr:0.000051, giou_loss:   7.34, conf_loss: 366.96, prob_loss:  46.47, total_loss: 420.76\n",
      "epoch: 1 step:    7/250, lr:0.000051, giou_loss:   8.79, conf_loss: 370.77, prob_loss:  52.37, total_loss: 431.93\n",
      "epoch: 1 step:    8/250, lr:0.000052, giou_loss:  10.86, conf_loss: 366.79, prob_loss:  63.89, total_loss: 441.54\n",
      "epoch: 1 step:    9/250, lr:0.000052, giou_loss:  11.16, conf_loss: 362.79, prob_loss:  62.09, total_loss: 436.04\n",
      "epoch: 1 step:   10/250, lr:0.000052, giou_loss:   9.65, conf_loss: 357.65, prob_loss:  47.02, total_loss: 414.32\n",
      "epoch: 1 step:   11/250, lr:0.000052, giou_loss:  12.36, conf_loss: 359.78, prob_loss:  69.64, total_loss: 441.77\n",
      "epoch: 1 step:   12/250, lr:0.000052, giou_loss:  13.68, conf_loss: 356.34, prob_loss:  81.00, total_loss: 451.01\n",
      "epoch: 1 step:   13/250, lr:0.000053, giou_loss:  11.14, conf_loss: 349.29, prob_loss:  67.63, total_loss: 428.06\n",
      "epoch: 1 step:   14/250, lr:0.000053, giou_loss:   7.70, conf_loss: 349.43, prob_loss:  45.48, total_loss: 402.61\n",
      "epoch: 1 step:   15/250, lr:0.000053, giou_loss:   8.70, conf_loss: 345.94, prob_loss:  59.82, total_loss: 414.46\n",
      "epoch: 1 step:   16/250, lr:0.000053, giou_loss:   8.95, conf_loss: 344.49, prob_loss:  45.28, total_loss: 398.72\n",
      "epoch: 1 step:   17/250, lr:0.000053, giou_loss:   8.83, conf_loss: 339.26, prob_loss:  52.96, total_loss: 401.05\n",
      "epoch: 1 step:   18/250, lr:0.000054, giou_loss:   9.42, conf_loss: 336.08, prob_loss:  56.34, total_loss: 401.85\n",
      "epoch: 1 step:   19/250, lr:0.000054, giou_loss:   9.79, conf_loss: 341.01, prob_loss:  55.06, total_loss: 405.85\n",
      "epoch: 1 step:   20/250, lr:0.000054, giou_loss:  11.08, conf_loss: 336.05, prob_loss:  71.87, total_loss: 419.00\n",
      "epoch: 1 step:   21/250, lr:0.000054, giou_loss:   9.87, conf_loss: 330.59, prob_loss:  57.26, total_loss: 397.72\n",
      "epoch: 1 step:   22/250, lr:0.000054, giou_loss:  10.76, conf_loss: 330.91, prob_loss:  72.64, total_loss: 414.32\n",
      "epoch: 1 step:   23/250, lr:0.000055, giou_loss:  10.54, conf_loss: 327.32, prob_loss:  67.13, total_loss: 404.99\n",
      "epoch: 1 step:   24/250, lr:0.000055, giou_loss:   6.32, conf_loss: 323.47, prob_loss:  39.79, total_loss: 369.58\n",
      "epoch: 1 step:   25/250, lr:0.000055, giou_loss:   7.58, conf_loss: 320.20, prob_loss:  53.76, total_loss: 381.54\n",
      "epoch: 1 step:   26/250, lr:0.000055, giou_loss:   9.25, conf_loss: 321.05, prob_loss:  55.32, total_loss: 385.63\n",
      "epoch: 1 step:   27/250, lr:0.000055, giou_loss:   8.01, conf_loss: 313.60, prob_loss:  55.02, total_loss: 376.64\n",
      "epoch: 1 step:   28/250, lr:0.000056, giou_loss:   7.02, conf_loss: 312.42, prob_loss:  51.37, total_loss: 370.81\n",
      "epoch: 1 step:   29/250, lr:0.000056, giou_loss:   8.02, conf_loss: 309.28, prob_loss:  47.36, total_loss: 364.66\n",
      "epoch: 1 step:   30/250, lr:0.000056, giou_loss:   9.05, conf_loss: 311.57, prob_loss:  58.23, total_loss: 378.85\n",
      "epoch: 1 step:   31/250, lr:0.000056, giou_loss:   8.19, conf_loss: 306.23, prob_loss:  49.50, total_loss: 363.92\n",
      "epoch: 1 step:   32/250, lr:0.000056, giou_loss:   7.53, conf_loss: 306.72, prob_loss:  38.19, total_loss: 352.44\n",
      "epoch: 1 step:   33/250, lr:0.000057, giou_loss:   8.24, conf_loss: 303.64, prob_loss:  50.99, total_loss: 362.87\n",
      "epoch: 1 step:   34/250, lr:0.000057, giou_loss:  10.90, conf_loss: 307.41, prob_loss:  59.78, total_loss: 378.09\n",
      "epoch: 1 step:   35/250, lr:0.000057, giou_loss:   8.94, conf_loss: 299.23, prob_loss:  50.79, total_loss: 358.96\n",
      "epoch: 1 step:   36/250, lr:0.000057, giou_loss:  11.43, conf_loss: 300.94, prob_loss:  72.12, total_loss: 384.50\n",
      "epoch: 1 step:   37/250, lr:0.000057, giou_loss:  11.42, conf_loss: 301.71, prob_loss:  73.19, total_loss: 386.32\n",
      "epoch: 1 step:   38/250, lr:0.000058, giou_loss:   9.94, conf_loss: 298.78, prob_loss:  69.07, total_loss: 377.79\n",
      "epoch: 1 step:   39/250, lr:0.000058, giou_loss:   8.69, conf_loss: 291.40, prob_loss:  54.08, total_loss: 354.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step:   40/250, lr:0.000058, giou_loss:   8.08, conf_loss: 287.32, prob_loss:  47.75, total_loss: 343.16\n",
      "epoch: 1 step:   41/250, lr:0.000058, giou_loss:   9.63, conf_loss: 290.68, prob_loss:  55.05, total_loss: 355.36\n",
      "epoch: 1 step:   42/250, lr:0.000058, giou_loss:  13.60, conf_loss: 289.65, prob_loss:  80.91, total_loss: 384.15\n",
      "epoch: 1 step:   43/250, lr:0.000059, giou_loss:   8.44, conf_loss: 285.79, prob_loss:  52.87, total_loss: 347.09\n",
      "epoch: 1 step:   44/250, lr:0.000059, giou_loss:   7.37, conf_loss: 282.15, prob_loss:  43.96, total_loss: 333.48\n",
      "epoch: 1 step:   45/250, lr:0.000059, giou_loss:   8.17, conf_loss: 283.31, prob_loss:  55.67, total_loss: 347.15\n",
      "epoch: 1 step:   46/250, lr:0.000059, giou_loss:  10.59, conf_loss: 279.06, prob_loss:  70.45, total_loss: 360.10\n",
      "epoch: 1 step:   47/250, lr:0.000059, giou_loss:   8.23, conf_loss: 276.96, prob_loss:  53.88, total_loss: 339.07\n",
      "epoch: 1 step:   48/250, lr:0.000060, giou_loss:  10.06, conf_loss: 276.61, prob_loss:  58.97, total_loss: 345.64\n",
      "epoch: 1 step:   49/250, lr:0.000060, giou_loss:   8.57, conf_loss: 271.60, prob_loss:  56.75, total_loss: 336.93\n",
      "epoch: 1 step:   50/250, lr:0.000060, giou_loss:   7.87, conf_loss: 271.05, prob_loss:  54.10, total_loss: 333.02\n",
      "epoch: 1 step:   51/250, lr:0.000060, giou_loss:   8.81, conf_loss: 269.27, prob_loss:  67.71, total_loss: 345.79\n",
      "epoch: 1 step:   52/250, lr:0.000060, giou_loss:  13.04, conf_loss: 272.40, prob_loss:  76.56, total_loss: 362.01\n",
      "epoch: 1 step:   53/250, lr:0.000061, giou_loss:   9.05, conf_loss: 266.79, prob_loss:  55.06, total_loss: 330.90\n",
      "epoch: 1 step:   54/250, lr:0.000061, giou_loss:   5.76, conf_loss: 262.54, prob_loss:  33.92, total_loss: 302.21\n",
      "epoch: 1 step:   55/250, lr:0.000061, giou_loss:  10.49, conf_loss: 262.43, prob_loss:  69.18, total_loss: 342.10\n",
      "epoch: 1 step:   56/250, lr:0.000061, giou_loss:   8.80, conf_loss: 260.56, prob_loss:  59.26, total_loss: 328.62\n",
      "epoch: 1 step:   57/250, lr:0.000061, giou_loss:  10.61, conf_loss: 261.92, prob_loss:  63.43, total_loss: 335.96\n",
      "epoch: 1 step:   58/250, lr:0.000062, giou_loss:  10.01, conf_loss: 260.86, prob_loss:  61.88, total_loss: 332.75\n",
      "epoch: 1 step:   59/250, lr:0.000062, giou_loss:   7.27, conf_loss: 251.38, prob_loss:  41.58, total_loss: 300.23\n",
      "epoch: 1 step:   60/250, lr:0.000062, giou_loss:   9.59, conf_loss: 251.76, prob_loss:  58.90, total_loss: 320.25\n",
      "epoch: 1 step:   61/250, lr:0.000062, giou_loss:   7.22, conf_loss: 248.79, prob_loss:  45.05, total_loss: 301.05\n",
      "epoch: 1 step:   62/250, lr:0.000062, giou_loss:  11.10, conf_loss: 255.27, prob_loss:  68.88, total_loss: 335.26\n",
      "epoch: 1 step:   63/250, lr:0.000063, giou_loss:   8.38, conf_loss: 245.37, prob_loss:  52.84, total_loss: 306.59\n",
      "epoch: 1 step:   64/250, lr:0.000063, giou_loss:  12.30, conf_loss: 248.03, prob_loss:  79.72, total_loss: 340.06\n",
      "epoch: 1 step:   65/250, lr:0.000063, giou_loss:   7.49, conf_loss: 240.08, prob_loss:  50.13, total_loss: 297.69\n",
      "epoch: 1 step:   66/250, lr:0.000063, giou_loss:   8.14, conf_loss: 241.55, prob_loss:  50.71, total_loss: 300.40\n",
      "epoch: 1 step:   67/250, lr:0.000063, giou_loss:   7.80, conf_loss: 238.75, prob_loss:  58.38, total_loss: 304.93\n",
      "epoch: 1 step:   68/250, lr:0.000064, giou_loss:  12.49, conf_loss: 240.17, prob_loss:  76.63, total_loss: 329.29\n",
      "epoch: 1 step:   69/250, lr:0.000064, giou_loss:   9.99, conf_loss: 236.66, prob_loss:  69.65, total_loss: 316.30\n",
      "epoch: 1 step:   70/250, lr:0.000064, giou_loss:   9.69, conf_loss: 234.99, prob_loss:  65.36, total_loss: 310.04\n",
      "epoch: 1 step:   71/250, lr:0.000064, giou_loss:  10.77, conf_loss: 231.50, prob_loss:  61.49, total_loss: 303.75\n",
      "epoch: 1 step:   72/250, lr:0.000064, giou_loss:  11.86, conf_loss: 229.99, prob_loss:  63.53, total_loss: 305.37\n",
      "epoch: 1 step:   73/250, lr:0.000065, giou_loss:   8.55, conf_loss: 231.19, prob_loss:  45.19, total_loss: 284.92\n",
      "epoch: 1 step:   74/250, lr:0.000065, giou_loss:   8.44, conf_loss: 230.23, prob_loss:  50.24, total_loss: 288.91\n",
      "epoch: 1 step:   75/250, lr:0.000065, giou_loss:   8.98, conf_loss: 226.50, prob_loss:  61.06, total_loss: 296.54\n",
      "epoch: 1 step:   76/250, lr:0.000065, giou_loss:   7.92, conf_loss: 224.73, prob_loss:  56.80, total_loss: 289.46\n",
      "epoch: 1 step:   77/250, lr:0.000065, giou_loss:  11.54, conf_loss: 224.78, prob_loss:  71.95, total_loss: 308.28\n",
      "epoch: 1 step:   78/250, lr:0.000066, giou_loss:  11.32, conf_loss: 223.68, prob_loss:  75.66, total_loss: 310.66\n",
      "epoch: 1 step:   79/250, lr:0.000066, giou_loss:   9.65, conf_loss: 224.32, prob_loss:  65.42, total_loss: 299.39\n",
      "epoch: 1 step:   80/250, lr:0.000066, giou_loss:   9.98, conf_loss: 218.94, prob_loss:  59.05, total_loss: 287.97\n",
      "epoch: 1 step:   81/250, lr:0.000066, giou_loss:   8.23, conf_loss: 218.16, prob_loss:  54.33, total_loss: 280.72\n",
      "epoch: 1 step:   82/250, lr:0.000066, giou_loss:  10.64, conf_loss: 218.14, prob_loss:  70.20, total_loss: 298.97\n",
      "epoch: 1 step:   83/250, lr:0.000067, giou_loss:   8.14, conf_loss: 216.15, prob_loss:  64.02, total_loss: 288.31\n",
      "epoch: 1 step:   84/250, lr:0.000067, giou_loss:   7.25, conf_loss: 212.21, prob_loss:  46.33, total_loss: 265.79\n",
      "epoch: 1 step:   85/250, lr:0.000067, giou_loss:   7.62, conf_loss: 210.40, prob_loss:  49.88, total_loss: 267.89\n",
      "epoch: 1 step:   86/250, lr:0.000067, giou_loss:   7.93, conf_loss: 208.56, prob_loss:  45.11, total_loss: 261.60\n",
      "epoch: 1 step:   87/250, lr:0.000067, giou_loss:   8.62, conf_loss: 212.47, prob_loss:  48.67, total_loss: 269.76\n",
      "epoch: 1 step:   88/250, lr:0.000068, giou_loss:   7.95, conf_loss: 208.48, prob_loss:  52.75, total_loss: 269.18\n",
      "epoch: 1 step:   89/250, lr:0.000068, giou_loss:  10.74, conf_loss: 204.81, prob_loss:  56.27, total_loss: 271.82\n",
      "epoch: 1 step:   90/250, lr:0.000068, giou_loss:   6.07, conf_loss: 203.65, prob_loss:  36.97, total_loss: 246.68\n",
      "epoch: 1 step:   91/250, lr:0.000068, giou_loss:  13.28, conf_loss: 204.90, prob_loss:  90.10, total_loss: 308.28\n",
      "epoch: 1 step:   92/250, lr:0.000068, giou_loss:  12.06, conf_loss: 204.35, prob_loss:  70.14, total_loss: 286.56\n",
      "epoch: 1 step:   93/250, lr:0.000069, giou_loss:  10.83, conf_loss: 200.78, prob_loss:  64.96, total_loss: 276.57\n",
      "epoch: 1 step:   94/250, lr:0.000069, giou_loss:   6.96, conf_loss: 199.21, prob_loss:  55.70, total_loss: 261.87\n",
      "epoch: 1 step:   95/250, lr:0.000069, giou_loss:   8.20, conf_loss: 196.71, prob_loss:  62.00, total_loss: 266.91\n",
      "epoch: 1 step:   96/250, lr:0.000069, giou_loss:   7.30, conf_loss: 195.81, prob_loss:  34.25, total_loss: 237.36\n",
      "epoch: 1 step:   97/250, lr:0.000069, giou_loss:  12.24, conf_loss: 197.46, prob_loss:  66.25, total_loss: 275.95\n",
      "epoch: 1 step:   98/250, lr:0.000070, giou_loss:  12.24, conf_loss: 195.92, prob_loss:  73.52, total_loss: 281.69\n",
      "epoch: 1 step:   99/250, lr:0.000070, giou_loss:  10.30, conf_loss: 191.33, prob_loss:  67.07, total_loss: 268.70\n",
      "epoch: 1 step:  100/250, lr:0.000070, giou_loss:  10.67, conf_loss: 190.87, prob_loss:  71.34, total_loss: 272.88\n",
      "epoch: 1 step:  101/250, lr:0.000070, giou_loss:   8.79, conf_loss: 188.56, prob_loss:  59.12, total_loss: 256.47\n",
      "epoch: 1 step:  102/250, lr:0.000070, giou_loss:  10.13, conf_loss: 188.33, prob_loss:  65.61, total_loss: 264.07\n",
      "epoch: 1 step:  103/250, lr:0.000071, giou_loss:  10.01, conf_loss: 188.50, prob_loss:  68.04, total_loss: 266.56\n",
      "epoch: 1 step:  104/250, lr:0.000071, giou_loss:   6.03, conf_loss: 183.43, prob_loss:  30.71, total_loss: 220.16\n",
      "epoch: 1 step:  105/250, lr:0.000071, giou_loss:   9.42, conf_loss: 183.19, prob_loss:  65.05, total_loss: 257.65\n",
      "epoch: 1 step:  106/250, lr:0.000071, giou_loss:   5.07, conf_loss: 183.50, prob_loss:  37.21, total_loss: 225.78\n",
      "epoch: 1 step:  107/250, lr:0.000071, giou_loss:   8.96, conf_loss: 180.58, prob_loss:  51.48, total_loss: 241.02\n",
      "epoch: 1 step:  108/250, lr:0.000072, giou_loss:  12.12, conf_loss: 181.66, prob_loss:  56.79, total_loss: 250.58\n",
      "epoch: 1 step:  109/250, lr:0.000072, giou_loss:   9.06, conf_loss: 179.14, prob_loss:  55.01, total_loss: 243.20\n",
      "epoch: 1 step:  110/250, lr:0.000072, giou_loss:  14.00, conf_loss: 180.37, prob_loss:  88.51, total_loss: 282.88\n",
      "epoch: 1 step:  111/250, lr:0.000072, giou_loss:   8.78, conf_loss: 175.94, prob_loss:  62.92, total_loss: 247.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step:  112/250, lr:0.000072, giou_loss:   8.48, conf_loss: 172.96, prob_loss:  45.59, total_loss: 227.03\n",
      "epoch: 1 step:  113/250, lr:0.000073, giou_loss:   7.00, conf_loss: 170.34, prob_loss:  44.73, total_loss: 222.07\n",
      "epoch: 1 step:  114/250, lr:0.000073, giou_loss:  13.65, conf_loss: 176.30, prob_loss:  87.25, total_loss: 277.19\n",
      "epoch: 1 step:  115/250, lr:0.000073, giou_loss:   8.23, conf_loss: 170.70, prob_loss:  54.65, total_loss: 233.58\n",
      "epoch: 1 step:  116/250, lr:0.000073, giou_loss:   7.80, conf_loss: 169.12, prob_loss:  42.34, total_loss: 219.26\n",
      "epoch: 1 step:  117/250, lr:0.000073, giou_loss:   8.81, conf_loss: 168.66, prob_loss:  58.75, total_loss: 236.21\n",
      "epoch: 1 step:  118/250, lr:0.000074, giou_loss:   6.58, conf_loss: 166.30, prob_loss:  38.80, total_loss: 211.68\n",
      "epoch: 1 step:  119/250, lr:0.000074, giou_loss:   7.15, conf_loss: 166.16, prob_loss:  45.63, total_loss: 218.94\n",
      "epoch: 1 step:  120/250, lr:0.000074, giou_loss:  10.54, conf_loss: 168.44, prob_loss:  67.17, total_loss: 246.16\n",
      "epoch: 1 step:  121/250, lr:0.000074, giou_loss:  12.00, conf_loss: 166.70, prob_loss:  61.91, total_loss: 240.61\n",
      "epoch: 1 step:  122/250, lr:0.000074, giou_loss:   8.81, conf_loss: 164.62, prob_loss:  60.39, total_loss: 233.82\n",
      "epoch: 1 step:  123/250, lr:0.000075, giou_loss:   8.34, conf_loss: 162.92, prob_loss:  51.53, total_loss: 222.79\n",
      "epoch: 1 step:  124/250, lr:0.000075, giou_loss:   7.02, conf_loss: 159.56, prob_loss:  46.95, total_loss: 213.53\n",
      "epoch: 1 step:  125/250, lr:0.000075, giou_loss:  11.60, conf_loss: 160.45, prob_loss:  72.42, total_loss: 244.47\n",
      "epoch: 1 step:  126/250, lr:0.000075, giou_loss:  10.95, conf_loss: 159.62, prob_loss:  76.45, total_loss: 247.02\n",
      "epoch: 1 step:  127/250, lr:0.000075, giou_loss:   8.60, conf_loss: 157.54, prob_loss:  62.40, total_loss: 228.54\n",
      "epoch: 1 step:  128/250, lr:0.000076, giou_loss:   5.33, conf_loss: 156.21, prob_loss:  29.56, total_loss: 191.10\n",
      "epoch: 1 step:  129/250, lr:0.000076, giou_loss:   8.00, conf_loss: 154.62, prob_loss:  56.28, total_loss: 218.90\n",
      "epoch: 1 step:  130/250, lr:0.000076, giou_loss:  10.13, conf_loss: 156.86, prob_loss:  64.88, total_loss: 231.87\n",
      "epoch: 1 step:  131/250, lr:0.000076, giou_loss:   7.64, conf_loss: 151.97, prob_loss:  58.20, total_loss: 217.81\n",
      "epoch: 1 step:  132/250, lr:0.000076, giou_loss:  10.49, conf_loss: 152.66, prob_loss:  66.54, total_loss: 229.69\n",
      "epoch: 1 step:  133/250, lr:0.000077, giou_loss:   7.28, conf_loss: 149.65, prob_loss:  52.28, total_loss: 209.21\n",
      "epoch: 1 step:  134/250, lr:0.000077, giou_loss:   9.74, conf_loss: 149.64, prob_loss:  65.98, total_loss: 225.36\n",
      "epoch: 1 step:  135/250, lr:0.000077, giou_loss:   7.60, conf_loss: 149.50, prob_loss:  43.06, total_loss: 200.16\n",
      "epoch: 1 step:  136/250, lr:0.000077, giou_loss:   8.89, conf_loss: 150.23, prob_loss:  59.69, total_loss: 218.81\n",
      "epoch: 1 step:  137/250, lr:0.000077, giou_loss:   9.08, conf_loss: 148.05, prob_loss:  47.70, total_loss: 204.83\n",
      "epoch: 1 step:  138/250, lr:0.000078, giou_loss:  11.24, conf_loss: 146.29, prob_loss:  68.41, total_loss: 225.94\n",
      "epoch: 1 step:  139/250, lr:0.000078, giou_loss:  10.62, conf_loss: 146.11, prob_loss:  61.10, total_loss: 217.83\n",
      "epoch: 1 step:  140/250, lr:0.000078, giou_loss:   8.41, conf_loss: 144.47, prob_loss:  56.53, total_loss: 209.41\n",
      "epoch: 1 step:  141/250, lr:0.000078, giou_loss:  12.25, conf_loss: 147.29, prob_loss:  71.53, total_loss: 231.07\n",
      "epoch: 1 step:  142/250, lr:0.000078, giou_loss:   7.06, conf_loss: 141.14, prob_loss:  49.82, total_loss: 198.02\n",
      "epoch: 1 step:  143/250, lr:0.000079, giou_loss:   6.06, conf_loss: 139.89, prob_loss:  45.11, total_loss: 191.06\n",
      "epoch: 1 step:  144/250, lr:0.000079, giou_loss:   5.81, conf_loss: 138.64, prob_loss:  34.47, total_loss: 178.93\n",
      "epoch: 1 step:  145/250, lr:0.000079, giou_loss:   7.71, conf_loss: 140.19, prob_loss:  53.15, total_loss: 201.05\n",
      "epoch: 1 step:  146/250, lr:0.000079, giou_loss:   7.60, conf_loss: 137.02, prob_loss:  47.53, total_loss: 192.15\n",
      "epoch: 1 step:  147/250, lr:0.000079, giou_loss:  10.02, conf_loss: 139.99, prob_loss:  61.90, total_loss: 211.91\n",
      "epoch: 1 step:  148/250, lr:0.000080, giou_loss:   5.28, conf_loss: 136.00, prob_loss:  40.66, total_loss: 181.94\n",
      "epoch: 1 step:  149/250, lr:0.000080, giou_loss:   6.64, conf_loss: 135.39, prob_loss:  48.24, total_loss: 190.27\n",
      "epoch: 1 step:  150/250, lr:0.000080, giou_loss:   5.65, conf_loss: 134.88, prob_loss:  36.44, total_loss: 176.98\n",
      "epoch: 1 step:  151/250, lr:0.000080, giou_loss:   7.99, conf_loss: 132.55, prob_loss:  51.71, total_loss: 192.25\n",
      "epoch: 1 step:  152/250, lr:0.000080, giou_loss:   6.51, conf_loss: 132.84, prob_loss:  48.79, total_loss: 188.14\n",
      "epoch: 1 step:  153/250, lr:0.000081, giou_loss:   6.77, conf_loss: 131.30, prob_loss:  41.30, total_loss: 179.37\n",
      "epoch: 1 step:  154/250, lr:0.000081, giou_loss:   9.08, conf_loss: 130.90, prob_loss:  50.05, total_loss: 190.03\n",
      "epoch: 1 step:  155/250, lr:0.000081, giou_loss:   8.21, conf_loss: 130.10, prob_loss:  54.90, total_loss: 193.22\n",
      "epoch: 1 step:  156/250, lr:0.000081, giou_loss:   7.36, conf_loss: 129.01, prob_loss:  42.13, total_loss: 178.50\n",
      "epoch: 1 step:  157/250, lr:0.000081, giou_loss:   7.15, conf_loss: 128.10, prob_loss:  40.11, total_loss: 175.35\n",
      "epoch: 1 step:  158/250, lr:0.000082, giou_loss:   6.95, conf_loss: 129.41, prob_loss:  43.33, total_loss: 179.69\n",
      "epoch: 1 step:  159/250, lr:0.000082, giou_loss:  10.03, conf_loss: 128.50, prob_loss:  47.26, total_loss: 185.79\n",
      "epoch: 1 step:  160/250, lr:0.000082, giou_loss:   8.89, conf_loss: 126.33, prob_loss:  54.35, total_loss: 189.57\n",
      "epoch: 1 step:  161/250, lr:0.000082, giou_loss:   6.02, conf_loss: 126.10, prob_loss:  30.66, total_loss: 162.78\n",
      "epoch: 1 step:  162/250, lr:0.000082, giou_loss:   6.08, conf_loss: 126.10, prob_loss:  39.23, total_loss: 171.40\n",
      "epoch: 1 step:  163/250, lr:0.000083, giou_loss:  10.30, conf_loss: 124.52, prob_loss:  54.74, total_loss: 189.56\n",
      "epoch: 1 step:  164/250, lr:0.000083, giou_loss:  10.12, conf_loss: 124.43, prob_loss:  64.71, total_loss: 199.26\n",
      "epoch: 1 step:  165/250, lr:0.000083, giou_loss:   8.01, conf_loss: 125.71, prob_loss:  52.62, total_loss: 186.35\n",
      "epoch: 1 step:  166/250, lr:0.000083, giou_loss:   6.53, conf_loss: 119.14, prob_loss:  42.04, total_loss: 167.70\n",
      "epoch: 1 step:  167/250, lr:0.000083, giou_loss:   7.36, conf_loss: 120.06, prob_loss:  42.83, total_loss: 170.26\n",
      "epoch: 1 step:  168/250, lr:0.000084, giou_loss:  10.17, conf_loss: 121.26, prob_loss:  54.43, total_loss: 185.86\n",
      "epoch: 1 step:  169/250, lr:0.000084, giou_loss:  10.66, conf_loss: 122.55, prob_loss:  68.33, total_loss: 201.54\n",
      "epoch: 1 step:  170/250, lr:0.000084, giou_loss:   5.05, conf_loss: 117.13, prob_loss:  32.45, total_loss: 154.62\n",
      "epoch: 1 step:  171/250, lr:0.000084, giou_loss:   7.73, conf_loss: 118.19, prob_loss:  48.96, total_loss: 174.88\n",
      "epoch: 1 step:  172/250, lr:0.000084, giou_loss:  11.04, conf_loss: 119.84, prob_loss:  57.22, total_loss: 188.09\n",
      "epoch: 1 step:  173/250, lr:0.000085, giou_loss:   9.10, conf_loss: 117.25, prob_loss:  52.91, total_loss: 179.26\n",
      "epoch: 1 step:  174/250, lr:0.000085, giou_loss:   8.96, conf_loss: 115.76, prob_loss:  53.24, total_loss: 177.96\n",
      "epoch: 1 step:  175/250, lr:0.000085, giou_loss:  11.12, conf_loss: 116.35, prob_loss:  66.84, total_loss: 194.32\n",
      "epoch: 1 step:  176/250, lr:0.000085, giou_loss:   8.23, conf_loss: 114.15, prob_loss:  59.00, total_loss: 181.38\n",
      "epoch: 1 step:  177/250, lr:0.000085, giou_loss:   7.85, conf_loss: 114.02, prob_loss:  53.38, total_loss: 175.25\n",
      "epoch: 1 step:  178/250, lr:0.000086, giou_loss:   9.68, conf_loss: 115.19, prob_loss:  64.04, total_loss: 188.90\n",
      "epoch: 1 step:  179/250, lr:0.000086, giou_loss:   4.84, conf_loss: 111.81, prob_loss:  34.11, total_loss: 150.75\n",
      "epoch: 1 step:  180/250, lr:0.000086, giou_loss:   8.04, conf_loss: 111.11, prob_loss:  52.12, total_loss: 171.26\n",
      "epoch: 1 step:  181/250, lr:0.000086, giou_loss:   9.59, conf_loss: 112.07, prob_loss:  65.96, total_loss: 187.62\n",
      "epoch: 1 step:  182/250, lr:0.000086, giou_loss:   8.50, conf_loss: 109.70, prob_loss:  60.68, total_loss: 178.88\n",
      "epoch: 1 step:  183/250, lr:0.000087, giou_loss:  13.41, conf_loss: 112.25, prob_loss:  85.38, total_loss: 211.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step:  184/250, lr:0.000087, giou_loss:   5.61, conf_loss: 107.65, prob_loss:  36.30, total_loss: 149.56\n",
      "epoch: 1 step:  185/250, lr:0.000087, giou_loss:   7.13, conf_loss: 106.37, prob_loss:  48.48, total_loss: 161.97\n",
      "epoch: 1 step:  186/250, lr:0.000087, giou_loss:   6.59, conf_loss: 106.84, prob_loss:  40.65, total_loss: 154.09\n",
      "epoch: 1 step:  187/250, lr:0.000087, giou_loss:   7.62, conf_loss: 105.40, prob_loss:  50.01, total_loss: 163.03\n",
      "epoch: 1 step:  188/250, lr:0.000088, giou_loss:  10.10, conf_loss: 108.28, prob_loss:  66.70, total_loss: 185.07\n",
      "epoch: 1 step:  189/250, lr:0.000088, giou_loss:   7.49, conf_loss: 104.51, prob_loss:  48.94, total_loss: 160.94\n",
      "epoch: 1 step:  190/250, lr:0.000088, giou_loss:   8.63, conf_loss: 104.54, prob_loss:  55.63, total_loss: 168.80\n",
      "epoch: 1 step:  191/250, lr:0.000088, giou_loss:   9.27, conf_loss: 103.97, prob_loss:  58.90, total_loss: 172.14\n",
      "epoch: 1 step:  192/250, lr:0.000088, giou_loss:   6.33, conf_loss: 105.09, prob_loss:  40.80, total_loss: 152.22\n",
      "epoch: 1 step:  193/250, lr:0.000089, giou_loss:   7.52, conf_loss: 103.21, prob_loss:  59.13, total_loss: 169.86\n",
      "epoch: 1 step:  194/250, lr:0.000089, giou_loss:   7.20, conf_loss: 101.08, prob_loss:  50.04, total_loss: 158.32\n",
      "epoch: 1 step:  195/250, lr:0.000089, giou_loss:   8.22, conf_loss: 101.40, prob_loss:  54.39, total_loss: 164.01\n",
      "epoch: 1 step:  196/250, lr:0.000089, giou_loss:   7.74, conf_loss: 100.43, prob_loss:  53.44, total_loss: 161.61\n",
      "epoch: 1 step:  197/250, lr:0.000089, giou_loss:   6.87, conf_loss: 103.60, prob_loss:  49.86, total_loss: 160.33\n",
      "epoch: 1 step:  198/250, lr:0.000090, giou_loss:   5.34, conf_loss:  98.56, prob_loss:  39.63, total_loss: 143.54\n",
      "epoch: 1 step:  199/250, lr:0.000090, giou_loss:   7.01, conf_loss:  97.37, prob_loss:  44.35, total_loss: 148.74\n",
      "epoch: 1 step:  200/250, lr:0.000090, giou_loss:   8.79, conf_loss: 100.14, prob_loss:  50.74, total_loss: 159.68\n",
      "epoch: 1 step:  201/250, lr:0.000090, giou_loss:   5.99, conf_loss:  96.78, prob_loss:  41.85, total_loss: 144.62\n",
      "epoch: 1 step:  202/250, lr:0.000090, giou_loss:   6.88, conf_loss:  96.93, prob_loss:  44.66, total_loss: 148.48\n",
      "epoch: 1 step:  203/250, lr:0.000091, giou_loss:  11.47, conf_loss:  99.58, prob_loss:  60.53, total_loss: 171.58\n",
      "epoch: 1 step:  204/250, lr:0.000091, giou_loss:  10.19, conf_loss:  99.12, prob_loss:  66.66, total_loss: 175.96\n",
      "epoch: 1 step:  205/250, lr:0.000091, giou_loss:  10.16, conf_loss:  97.79, prob_loss:  68.55, total_loss: 176.50\n",
      "epoch: 1 step:  206/250, lr:0.000091, giou_loss:   8.73, conf_loss:  95.15, prob_loss:  53.97, total_loss: 157.84\n",
      "epoch: 1 step:  207/250, lr:0.000091, giou_loss:  10.08, conf_loss:  96.18, prob_loss:  61.02, total_loss: 167.28\n",
      "epoch: 1 step:  208/250, lr:0.000092, giou_loss:   7.30, conf_loss:  93.31, prob_loss:  47.74, total_loss: 148.35\n",
      "epoch: 1 step:  209/250, lr:0.000092, giou_loss:   8.33, conf_loss:  94.56, prob_loss:  47.60, total_loss: 150.48\n",
      "epoch: 1 step:  210/250, lr:0.000092, giou_loss:  11.51, conf_loss:  94.98, prob_loss:  69.12, total_loss: 175.60\n",
      "epoch: 1 step:  211/250, lr:0.000092, giou_loss:   7.88, conf_loss:  92.56, prob_loss:  37.30, total_loss: 137.74\n",
      "epoch: 1 step:  212/250, lr:0.000092, giou_loss:   7.79, conf_loss:  92.17, prob_loss:  54.70, total_loss: 154.66\n",
      "epoch: 1 step:  213/250, lr:0.000093, giou_loss:   5.83, conf_loss:  90.18, prob_loss:  38.01, total_loss: 134.02\n",
      "epoch: 1 step:  214/250, lr:0.000093, giou_loss:   7.95, conf_loss:  90.14, prob_loss:  57.82, total_loss: 155.91\n",
      "epoch: 1 step:  215/250, lr:0.000093, giou_loss:  11.73, conf_loss:  93.07, prob_loss:  75.55, total_loss: 180.34\n",
      "epoch: 1 step:  216/250, lr:0.000093, giou_loss:   6.83, conf_loss:  88.49, prob_loss:  42.09, total_loss: 137.41\n",
      "epoch: 1 step:  217/250, lr:0.000093, giou_loss:   7.21, conf_loss:  88.41, prob_loss:  42.67, total_loss: 138.28\n",
      "epoch: 1 step:  218/250, lr:0.000094, giou_loss:   9.52, conf_loss:  89.47, prob_loss:  60.23, total_loss: 159.22\n",
      "epoch: 1 step:  219/250, lr:0.000094, giou_loss:   7.03, conf_loss:  88.51, prob_loss:  46.94, total_loss: 142.48\n",
      "epoch: 1 step:  220/250, lr:0.000094, giou_loss:   9.42, conf_loss:  88.40, prob_loss:  56.02, total_loss: 153.84\n",
      "epoch: 1 step:  221/250, lr:0.000094, giou_loss:  10.05, conf_loss:  87.50, prob_loss:  67.78, total_loss: 165.33\n",
      "epoch: 1 step:  222/250, lr:0.000094, giou_loss:   6.16, conf_loss:  84.05, prob_loss:  37.51, total_loss: 127.72\n",
      "epoch: 1 step:  223/250, lr:0.000095, giou_loss:  11.11, conf_loss:  87.33, prob_loss:  69.96, total_loss: 168.40\n",
      "epoch: 1 step:  224/250, lr:0.000095, giou_loss:   8.65, conf_loss:  84.96, prob_loss:  59.86, total_loss: 153.48\n",
      "epoch: 1 step:  225/250, lr:0.000095, giou_loss:   8.69, conf_loss:  84.32, prob_loss:  59.42, total_loss: 152.42\n",
      "epoch: 1 step:  226/250, lr:0.000095, giou_loss:   7.25, conf_loss:  83.95, prob_loss:  52.36, total_loss: 143.56\n",
      "epoch: 1 step:  227/250, lr:0.000095, giou_loss:   6.80, conf_loss:  83.61, prob_loss:  46.84, total_loss: 137.25\n",
      "epoch: 1 step:  228/250, lr:0.000096, giou_loss:   7.52, conf_loss:  84.99, prob_loss:  53.30, total_loss: 145.81\n",
      "epoch: 1 step:  229/250, lr:0.000096, giou_loss:   8.72, conf_loss:  83.67, prob_loss:  54.73, total_loss: 147.12\n",
      "epoch: 1 step:  230/250, lr:0.000096, giou_loss:   5.75, conf_loss:  80.54, prob_loss:  39.20, total_loss: 125.49\n",
      "epoch: 1 step:  231/250, lr:0.000096, giou_loss:   5.75, conf_loss:  80.24, prob_loss:  38.27, total_loss: 124.26\n",
      "epoch: 1 step:  232/250, lr:0.000096, giou_loss:  10.80, conf_loss:  83.33, prob_loss:  66.00, total_loss: 160.13\n",
      "epoch: 1 step:  233/250, lr:0.000097, giou_loss:   6.85, conf_loss:  79.97, prob_loss:  47.18, total_loss: 134.00\n",
      "epoch: 1 step:  234/250, lr:0.000097, giou_loss:   9.38, conf_loss:  81.54, prob_loss:  55.57, total_loss: 146.50\n",
      "epoch: 1 step:  235/250, lr:0.000097, giou_loss:   5.66, conf_loss:  78.04, prob_loss:  41.57, total_loss: 125.28\n",
      "epoch: 1 step:  236/250, lr:0.000097, giou_loss:   9.76, conf_loss:  81.75, prob_loss:  58.83, total_loss: 150.33\n",
      "epoch: 1 step:  237/250, lr:0.000097, giou_loss:   9.21, conf_loss:  80.41, prob_loss:  52.12, total_loss: 141.75\n",
      "epoch: 1 step:  238/250, lr:0.000098, giou_loss:  11.66, conf_loss:  80.85, prob_loss:  69.40, total_loss: 161.90\n",
      "epoch: 1 step:  239/250, lr:0.000098, giou_loss:   7.50, conf_loss:  76.95, prob_loss:  43.17, total_loss: 127.62\n",
      "epoch: 1 step:  240/250, lr:0.000098, giou_loss:  10.12, conf_loss:  77.55, prob_loss:  65.20, total_loss: 152.88\n",
      "epoch: 1 step:  241/250, lr:0.000098, giou_loss:   7.78, conf_loss:  76.90, prob_loss:  46.59, total_loss: 131.27\n",
      "epoch: 1 step:  242/250, lr:0.000098, giou_loss:   8.05, conf_loss:  76.62, prob_loss:  50.21, total_loss: 134.88\n",
      "epoch: 1 step:  243/250, lr:0.000099, giou_loss:   7.64, conf_loss:  76.02, prob_loss:  45.40, total_loss: 129.05\n",
      "epoch: 1 step:  244/250, lr:0.000099, giou_loss:   8.50, conf_loss:  76.69, prob_loss:  60.49, total_loss: 145.68\n",
      "epoch: 1 step:  245/250, lr:0.000099, giou_loss:   9.44, conf_loss:  75.87, prob_loss:  57.05, total_loss: 142.36\n",
      "epoch: 1 step:  246/250, lr:0.000099, giou_loss:   6.80, conf_loss:  74.68, prob_loss:  47.03, total_loss: 128.51\n",
      "epoch: 1 step:  247/250, lr:0.000099, giou_loss:   6.61, conf_loss:  73.77, prob_loss:  37.86, total_loss: 118.24\n",
      "epoch: 1 step:  248/250, lr:0.000100, giou_loss:   6.67, conf_loss:  72.78, prob_loss:  41.26, total_loss: 120.71\n",
      "epoch: 1 step:  249/250, lr:0.000100, giou_loss:   7.34, conf_loss:  73.40, prob_loss:  61.26, total_loss: 141.99\n",
      "epoch: 1 step:    0/250, lr:0.000100, giou_loss:   8.23, conf_loss:  73.12, prob_loss:  59.52, total_loss: 140.87\n",
      "epoch: 1 step:    1/250, lr:0.000100, giou_loss:   9.80, conf_loss:  74.59, prob_loss:  73.46, total_loss: 157.84\n",
      "\n",
      "\n",
      "giou_val_loss:  17.12, conf_val_loss: 117.40, prob_val_loss: 117.31, total_val_loss: 251.82\n",
      "\n",
      "\n",
      "epoch: 2 step:    2/250, lr:0.000100, giou_loss:   8.48, conf_loss:  73.75, prob_loss:  58.29, total_loss: 140.52\n",
      "epoch: 2 step:    3/250, lr:0.000100, giou_loss:   9.34, conf_loss:  72.39, prob_loss:  56.70, total_loss: 138.43\n",
      "epoch: 2 step:    4/250, lr:0.000100, giou_loss:   9.08, conf_loss:  72.58, prob_loss:  60.34, total_loss: 142.01\n",
      "epoch: 2 step:    5/250, lr:0.000100, giou_loss:   5.04, conf_loss:  70.37, prob_loss:  36.32, total_loss: 111.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step:    6/250, lr:0.000100, giou_loss:   5.98, conf_loss:  71.38, prob_loss:  41.12, total_loss: 118.48\n",
      "epoch: 2 step:    7/250, lr:0.000100, giou_loss:   8.57, conf_loss:  71.36, prob_loss:  66.79, total_loss: 146.72\n",
      "epoch: 2 step:    8/250, lr:0.000100, giou_loss:   7.24, conf_loss:  69.19, prob_loss:  54.20, total_loss: 130.63\n",
      "epoch: 2 step:    9/250, lr:0.000100, giou_loss:   7.38, conf_loss:  69.83, prob_loss:  49.98, total_loss: 127.19\n",
      "epoch: 2 step:   10/250, lr:0.000100, giou_loss:   7.40, conf_loss:  68.99, prob_loss:  48.44, total_loss: 124.83\n",
      "epoch: 2 step:   11/250, lr:0.000100, giou_loss:   5.12, conf_loss:  67.67, prob_loss:  39.82, total_loss: 112.62\n",
      "epoch: 2 step:   12/250, lr:0.000100, giou_loss:   6.39, conf_loss:  67.89, prob_loss:  44.68, total_loss: 118.95\n",
      "epoch: 2 step:   13/250, lr:0.000100, giou_loss:   8.96, conf_loss:  71.03, prob_loss:  51.70, total_loss: 131.70\n",
      "epoch: 2 step:   14/250, lr:0.000100, giou_loss:   5.78, conf_loss:  65.42, prob_loss:  35.52, total_loss: 106.72\n",
      "epoch: 2 step:   15/250, lr:0.000100, giou_loss:   9.18, conf_loss:  67.62, prob_loss:  56.42, total_loss: 133.22\n",
      "epoch: 2 step:   16/250, lr:0.000100, giou_loss:   6.56, conf_loss:  66.79, prob_loss:  46.54, total_loss: 119.89\n",
      "epoch: 2 step:   17/250, lr:0.000100, giou_loss:   8.01, conf_loss:  66.02, prob_loss:  53.91, total_loss: 127.94\n",
      "epoch: 2 step:   18/250, lr:0.000100, giou_loss:   7.14, conf_loss:  66.38, prob_loss:  51.80, total_loss: 125.31\n",
      "epoch: 2 step:   19/250, lr:0.000100, giou_loss:   4.43, conf_loss:  64.62, prob_loss:  34.69, total_loss: 103.74\n",
      "epoch: 2 step:   20/250, lr:0.000100, giou_loss:   7.22, conf_loss:  64.60, prob_loss:  50.05, total_loss: 121.88\n",
      "epoch: 2 step:   21/250, lr:0.000100, giou_loss:   8.93, conf_loss:  66.26, prob_loss:  60.42, total_loss: 135.61\n",
      "epoch: 2 step:   22/250, lr:0.000100, giou_loss:   4.76, conf_loss:  62.86, prob_loss:  34.07, total_loss: 101.69\n",
      "epoch: 2 step:   23/250, lr:0.000100, giou_loss:   6.92, conf_loss:  65.23, prob_loss:  43.29, total_loss: 115.44\n",
      "epoch: 2 step:   24/250, lr:0.000100, giou_loss:   7.30, conf_loss:  63.56, prob_loss:  45.94, total_loss: 116.79\n",
      "epoch: 2 step:   25/250, lr:0.000100, giou_loss:   8.42, conf_loss:  62.49, prob_loss:  48.29, total_loss: 119.20\n",
      "epoch: 2 step:   26/250, lr:0.000100, giou_loss:   8.56, conf_loss:  62.76, prob_loss:  63.71, total_loss: 135.03\n",
      "epoch: 2 step:   27/250, lr:0.000100, giou_loss:   7.37, conf_loss:  63.68, prob_loss:  58.57, total_loss: 129.62\n",
      "epoch: 2 step:   28/250, lr:0.000100, giou_loss:   6.76, conf_loss:  62.16, prob_loss:  45.97, total_loss: 114.89\n",
      "epoch: 2 step:   29/250, lr:0.000100, giou_loss:   6.61, conf_loss:  60.85, prob_loss:  46.60, total_loss: 114.06\n",
      "epoch: 2 step:   30/250, lr:0.000100, giou_loss:   7.05, conf_loss:  63.19, prob_loss:  43.16, total_loss: 113.41\n",
      "epoch: 2 step:   31/250, lr:0.000100, giou_loss:   8.56, conf_loss:  61.79, prob_loss:  57.67, total_loss: 128.01\n",
      "epoch: 2 step:   32/250, lr:0.000100, giou_loss:   7.76, conf_loss:  61.56, prob_loss:  49.15, total_loss: 118.47\n",
      "epoch: 2 step:   33/250, lr:0.000100, giou_loss:   5.54, conf_loss:  59.31, prob_loss:  45.00, total_loss: 109.85\n",
      "epoch: 2 step:   34/250, lr:0.000100, giou_loss:   8.02, conf_loss:  59.65, prob_loss:  58.35, total_loss: 126.02\n",
      "epoch: 2 step:   35/250, lr:0.000100, giou_loss:   8.26, conf_loss:  60.81, prob_loss:  57.19, total_loss: 126.26\n",
      "epoch: 2 step:   36/250, lr:0.000100, giou_loss:   8.94, conf_loss:  60.51, prob_loss:  65.54, total_loss: 134.99\n",
      "epoch: 2 step:   37/250, lr:0.000100, giou_loss:   6.58, conf_loss:  58.92, prob_loss:  49.63, total_loss: 115.13\n",
      "epoch: 2 step:   38/250, lr:0.000100, giou_loss:   4.40, conf_loss:  57.75, prob_loss:  34.51, total_loss:  96.66\n",
      "epoch: 2 step:   39/250, lr:0.000100, giou_loss:   5.10, conf_loss:  59.14, prob_loss:  29.56, total_loss:  93.80\n",
      "epoch: 2 step:   40/250, lr:0.000100, giou_loss:   7.12, conf_loss:  58.61, prob_loss:  49.30, total_loss: 115.03\n",
      "epoch: 2 step:   41/250, lr:0.000100, giou_loss:   9.40, conf_loss:  60.85, prob_loss:  60.93, total_loss: 131.17\n",
      "epoch: 2 step:   42/250, lr:0.000100, giou_loss:   6.86, conf_loss:  57.59, prob_loss:  45.70, total_loss: 110.16\n",
      "epoch: 2 step:   43/250, lr:0.000100, giou_loss:   8.48, conf_loss:  58.90, prob_loss:  53.03, total_loss: 120.42\n",
      "epoch: 2 step:   44/250, lr:0.000100, giou_loss:   9.02, conf_loss:  59.22, prob_loss:  61.13, total_loss: 129.37\n",
      "epoch: 2 step:   45/250, lr:0.000100, giou_loss:   8.09, conf_loss:  57.62, prob_loss:  48.65, total_loss: 114.35\n",
      "epoch: 2 step:   46/250, lr:0.000100, giou_loss:   5.13, conf_loss:  56.69, prob_loss:  41.18, total_loss: 103.00\n",
      "epoch: 2 step:   47/250, lr:0.000100, giou_loss:   8.16, conf_loss:  56.56, prob_loss:  61.09, total_loss: 125.82\n",
      "epoch: 2 step:   48/250, lr:0.000100, giou_loss:   9.91, conf_loss:  56.88, prob_loss:  57.61, total_loss: 124.40\n",
      "epoch: 2 step:   49/250, lr:0.000100, giou_loss:   6.10, conf_loss:  56.75, prob_loss:  40.17, total_loss: 103.02\n",
      "epoch: 2 step:   50/250, lr:0.000100, giou_loss:  10.66, conf_loss:  57.53, prob_loss:  73.95, total_loss: 142.14\n",
      "epoch: 2 step:   51/250, lr:0.000100, giou_loss:   8.14, conf_loss:  57.06, prob_loss:  50.82, total_loss: 116.02\n",
      "epoch: 2 step:   52/250, lr:0.000100, giou_loss:   9.24, conf_loss:  56.79, prob_loss:  54.44, total_loss: 120.47\n",
      "epoch: 2 step:   53/250, lr:0.000100, giou_loss:   8.54, conf_loss:  56.02, prob_loss:  62.12, total_loss: 126.68\n",
      "epoch: 2 step:   54/250, lr:0.000100, giou_loss:   6.23, conf_loss:  54.28, prob_loss:  36.72, total_loss:  97.23\n",
      "epoch: 2 step:   55/250, lr:0.000100, giou_loss:  10.12, conf_loss:  55.91, prob_loss:  64.44, total_loss: 130.46\n",
      "epoch: 2 step:   56/250, lr:0.000100, giou_loss:   9.08, conf_loss:  56.21, prob_loss:  53.04, total_loss: 118.33\n",
      "epoch: 2 step:   57/250, lr:0.000100, giou_loss:   6.92, conf_loss:  54.39, prob_loss:  36.75, total_loss:  98.06\n",
      "epoch: 2 step:   58/250, lr:0.000100, giou_loss:   7.04, conf_loss:  54.17, prob_loss:  37.82, total_loss:  99.03\n",
      "epoch: 2 step:   59/250, lr:0.000100, giou_loss:   6.70, conf_loss:  52.92, prob_loss:  28.54, total_loss:  88.15\n",
      "epoch: 2 step:   60/250, lr:0.000100, giou_loss:   7.84, conf_loss:  53.92, prob_loss:  39.73, total_loss: 101.49\n",
      "epoch: 2 step:   61/250, lr:0.000100, giou_loss:  10.50, conf_loss:  55.41, prob_loss:  64.61, total_loss: 130.52\n",
      "epoch: 2 step:   62/250, lr:0.000100, giou_loss:   9.91, conf_loss:  54.25, prob_loss:  53.10, total_loss: 117.26\n",
      "epoch: 2 step:   63/250, lr:0.000100, giou_loss:   5.95, conf_loss:  52.68, prob_loss:  34.15, total_loss:  92.78\n",
      "epoch: 2 step:   64/250, lr:0.000100, giou_loss:   5.94, conf_loss:  54.63, prob_loss:  30.57, total_loss:  91.13\n",
      "epoch: 2 step:   65/250, lr:0.000100, giou_loss:   8.72, conf_loss:  52.97, prob_loss:  49.83, total_loss: 111.52\n",
      "epoch: 2 step:   66/250, lr:0.000100, giou_loss:   8.36, conf_loss:  52.60, prob_loss:  50.00, total_loss: 110.96\n",
      "epoch: 2 step:   67/250, lr:0.000100, giou_loss:  10.89, conf_loss:  54.96, prob_loss:  59.62, total_loss: 125.47\n",
      "epoch: 2 step:   68/250, lr:0.000100, giou_loss:   7.52, conf_loss:  53.08, prob_loss:  37.12, total_loss:  97.72\n",
      "epoch: 2 step:   69/250, lr:0.000100, giou_loss:   7.10, conf_loss:  51.03, prob_loss:  46.20, total_loss: 104.33\n",
      "epoch: 2 step:   70/250, lr:0.000100, giou_loss:   6.87, conf_loss:  50.22, prob_loss:  39.52, total_loss:  96.61\n",
      "epoch: 2 step:   71/250, lr:0.000100, giou_loss:   8.10, conf_loss:  50.72, prob_loss:  52.20, total_loss: 111.01\n",
      "epoch: 2 step:   72/250, lr:0.000100, giou_loss:   8.83, conf_loss:  51.01, prob_loss:  49.88, total_loss: 109.73\n",
      "epoch: 2 step:   73/250, lr:0.000100, giou_loss:   6.10, conf_loss:  50.57, prob_loss:  31.06, total_loss:  87.73\n",
      "epoch: 2 step:   74/250, lr:0.000100, giou_loss:  10.10, conf_loss:  51.21, prob_loss:  58.92, total_loss: 120.23\n",
      "epoch: 2 step:   75/250, lr:0.000100, giou_loss:  11.04, conf_loss:  53.34, prob_loss:  68.27, total_loss: 132.65\n",
      "epoch: 2 step:   76/250, lr:0.000100, giou_loss:   5.73, conf_loss:  48.98, prob_loss:  30.85, total_loss:  85.56\n",
      "epoch: 2 step:   77/250, lr:0.000100, giou_loss:   6.44, conf_loss:  48.82, prob_loss:  37.75, total_loss:  93.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step:   78/250, lr:0.000100, giou_loss:   9.74, conf_loss:  51.45, prob_loss:  59.40, total_loss: 120.60\n",
      "epoch: 2 step:   79/250, lr:0.000100, giou_loss:  10.24, conf_loss:  50.38, prob_loss:  53.20, total_loss: 113.82\n",
      "epoch: 2 step:   80/250, lr:0.000100, giou_loss:  11.50, conf_loss:  51.96, prob_loss:  66.13, total_loss: 129.59\n",
      "epoch: 2 step:   81/250, lr:0.000100, giou_loss:   6.95, conf_loss:  48.44, prob_loss:  50.05, total_loss: 105.44\n",
      "epoch: 2 step:   82/250, lr:0.000100, giou_loss:   8.33, conf_loss:  49.76, prob_loss:  55.66, total_loss: 113.76\n",
      "epoch: 2 step:   83/250, lr:0.000100, giou_loss:   7.73, conf_loss:  48.67, prob_loss:  52.42, total_loss: 108.82\n",
      "epoch: 2 step:   84/250, lr:0.000100, giou_loss:   5.92, conf_loss:  47.89, prob_loss:  39.11, total_loss:  92.92\n",
      "epoch: 2 step:   85/250, lr:0.000100, giou_loss:   8.92, conf_loss:  48.47, prob_loss:  58.79, total_loss: 116.19\n",
      "epoch: 2 step:   86/250, lr:0.000100, giou_loss:   8.42, conf_loss:  48.46, prob_loss:  56.09, total_loss: 112.97\n",
      "epoch: 2 step:   87/250, lr:0.000100, giou_loss:   7.04, conf_loss:  46.45, prob_loss:  44.37, total_loss:  97.86\n",
      "epoch: 2 step:   88/250, lr:0.000100, giou_loss:   6.86, conf_loss:  47.98, prob_loss:  38.05, total_loss:  92.89\n",
      "epoch: 2 step:   89/250, lr:0.000100, giou_loss:   6.73, conf_loss:  47.05, prob_loss:  42.47, total_loss:  96.25\n",
      "epoch: 2 step:   90/250, lr:0.000100, giou_loss:   6.84, conf_loss:  47.00, prob_loss:  44.87, total_loss:  98.71\n",
      "epoch: 2 step:   91/250, lr:0.000100, giou_loss:   8.41, conf_loss:  47.37, prob_loss:  52.82, total_loss: 108.61\n",
      "epoch: 2 step:   92/250, lr:0.000100, giou_loss:   7.50, conf_loss:  47.02, prob_loss:  45.07, total_loss:  99.59\n",
      "epoch: 2 step:   93/250, lr:0.000100, giou_loss:   9.24, conf_loss:  47.03, prob_loss:  51.50, total_loss: 107.77\n",
      "epoch: 2 step:   94/250, lr:0.000100, giou_loss:   8.63, conf_loss:  46.20, prob_loss:  50.95, total_loss: 105.78\n",
      "epoch: 2 step:   95/250, lr:0.000100, giou_loss:   7.28, conf_loss:  44.86, prob_loss:  44.68, total_loss:  96.83\n",
      "epoch: 2 step:   96/250, lr:0.000100, giou_loss:   5.99, conf_loss:  46.04, prob_loss:  33.18, total_loss:  85.21\n",
      "epoch: 2 step:   97/250, lr:0.000100, giou_loss:   8.33, conf_loss:  45.83, prob_loss:  45.53, total_loss:  99.69\n",
      "epoch: 2 step:   98/250, lr:0.000100, giou_loss:   6.99, conf_loss:  45.81, prob_loss:  35.12, total_loss:  87.91\n",
      "epoch: 2 step:   99/250, lr:0.000100, giou_loss:   9.24, conf_loss:  46.91, prob_loss:  52.65, total_loss: 108.79\n",
      "epoch: 2 step:  100/250, lr:0.000100, giou_loss:   8.85, conf_loss:  45.81, prob_loss:  48.70, total_loss: 103.36\n",
      "epoch: 2 step:  101/250, lr:0.000100, giou_loss:  11.36, conf_loss:  45.70, prob_loss:  59.17, total_loss: 116.22\n",
      "epoch: 2 step:  102/250, lr:0.000100, giou_loss:   8.30, conf_loss:  45.53, prob_loss:  43.61, total_loss:  97.43\n",
      "epoch: 2 step:  103/250, lr:0.000100, giou_loss:   9.39, conf_loss:  45.72, prob_loss:  52.07, total_loss: 107.18\n",
      "epoch: 2 step:  104/250, lr:0.000100, giou_loss:   5.47, conf_loss:  43.95, prob_loss:  34.35, total_loss:  83.77\n",
      "epoch: 2 step:  105/250, lr:0.000100, giou_loss:  11.58, conf_loss:  46.59, prob_loss:  64.06, total_loss: 122.24\n",
      "epoch: 2 step:  106/250, lr:0.000100, giou_loss:   8.13, conf_loss:  44.39, prob_loss:  44.41, total_loss:  96.93\n",
      "epoch: 2 step:  107/250, lr:0.000100, giou_loss:   6.40, conf_loss:  43.04, prob_loss:  33.04, total_loss:  82.48\n",
      "epoch: 2 step:  108/250, lr:0.000100, giou_loss:   8.07, conf_loss:  43.45, prob_loss:  39.47, total_loss:  90.99\n",
      "epoch: 2 step:  109/250, lr:0.000100, giou_loss:  11.14, conf_loss:  46.19, prob_loss:  69.96, total_loss: 127.29\n",
      "epoch: 2 step:  110/250, lr:0.000100, giou_loss:   8.06, conf_loss:  43.33, prob_loss:  49.09, total_loss: 100.47\n",
      "epoch: 2 step:  111/250, lr:0.000100, giou_loss:   6.84, conf_loss:  42.84, prob_loss:  26.41, total_loss:  76.09\n",
      "epoch: 2 step:  112/250, lr:0.000100, giou_loss:   8.02, conf_loss:  42.44, prob_loss:  46.34, total_loss:  96.79\n",
      "epoch: 2 step:  113/250, lr:0.000100, giou_loss:  10.66, conf_loss:  43.44, prob_loss:  57.82, total_loss: 111.91\n",
      "epoch: 2 step:  114/250, lr:0.000100, giou_loss:   6.80, conf_loss:  41.41, prob_loss:  39.75, total_loss:  87.96\n",
      "epoch: 2 step:  115/250, lr:0.000100, giou_loss:   7.86, conf_loss:  42.78, prob_loss:  37.63, total_loss:  88.26\n",
      "epoch: 2 step:  116/250, lr:0.000100, giou_loss:   6.58, conf_loss:  42.21, prob_loss:  35.74, total_loss:  84.53\n",
      "epoch: 2 step:  117/250, lr:0.000100, giou_loss:   6.44, conf_loss:  41.81, prob_loss:  36.91, total_loss:  85.15\n",
      "epoch: 2 step:  118/250, lr:0.000100, giou_loss:   7.82, conf_loss:  40.99, prob_loss:  52.48, total_loss: 101.28\n",
      "epoch: 2 step:  119/250, lr:0.000100, giou_loss:   7.45, conf_loss:  41.07, prob_loss:  39.54, total_loss:  88.07\n",
      "epoch: 2 step:  120/250, lr:0.000100, giou_loss:   6.24, conf_loss:  41.38, prob_loss:  30.20, total_loss:  77.82\n",
      "epoch: 2 step:  121/250, lr:0.000100, giou_loss:   9.32, conf_loss:  42.06, prob_loss:  48.44, total_loss:  99.82\n",
      "epoch: 2 step:  122/250, lr:0.000100, giou_loss:   6.13, conf_loss:  39.69, prob_loss:  36.88, total_loss:  82.70\n",
      "epoch: 2 step:  123/250, lr:0.000100, giou_loss:   5.23, conf_loss:  39.92, prob_loss:  34.05, total_loss:  79.20\n",
      "epoch: 2 step:  124/250, lr:0.000100, giou_loss:   6.95, conf_loss:  41.42, prob_loss:  32.34, total_loss:  80.71\n",
      "epoch: 2 step:  125/250, lr:0.000100, giou_loss:   9.98, conf_loss:  41.57, prob_loss:  56.87, total_loss: 108.42\n",
      "epoch: 2 step:  126/250, lr:0.000100, giou_loss:   6.39, conf_loss:  40.09, prob_loss:  32.00, total_loss:  78.48\n",
      "epoch: 2 step:  127/250, lr:0.000100, giou_loss:   9.07, conf_loss:  42.45, prob_loss:  50.35, total_loss: 101.87\n",
      "epoch: 2 step:  128/250, lr:0.000100, giou_loss:   6.43, conf_loss:  38.80, prob_loss:  33.38, total_loss:  78.61\n",
      "epoch: 2 step:  129/250, lr:0.000100, giou_loss:   6.33, conf_loss:  40.03, prob_loss:  28.29, total_loss:  74.65\n",
      "epoch: 2 step:  130/250, lr:0.000100, giou_loss:   7.44, conf_loss:  39.71, prob_loss:  43.47, total_loss:  90.62\n",
      "epoch: 2 step:  131/250, lr:0.000100, giou_loss:   9.29, conf_loss:  40.42, prob_loss:  50.27, total_loss:  99.97\n",
      "epoch: 2 step:  132/250, lr:0.000100, giou_loss:   5.51, conf_loss:  39.26, prob_loss:  28.75, total_loss:  73.52\n",
      "epoch: 2 step:  133/250, lr:0.000100, giou_loss:   7.89, conf_loss:  40.01, prob_loss:  44.22, total_loss:  92.13\n",
      "epoch: 2 step:  134/250, lr:0.000100, giou_loss:  10.46, conf_loss:  41.42, prob_loss:  63.10, total_loss: 114.98\n",
      "epoch: 2 step:  135/250, lr:0.000100, giou_loss:   7.15, conf_loss:  39.98, prob_loss:  45.21, total_loss:  92.34\n",
      "epoch: 2 step:  136/250, lr:0.000100, giou_loss:   6.52, conf_loss:  37.54, prob_loss:  41.86, total_loss:  85.91\n",
      "epoch: 2 step:  137/250, lr:0.000100, giou_loss:   7.58, conf_loss:  38.43, prob_loss:  40.63, total_loss:  86.63\n",
      "epoch: 2 step:  138/250, lr:0.000100, giou_loss:   7.20, conf_loss:  38.47, prob_loss:  42.26, total_loss:  87.93\n",
      "epoch: 2 step:  139/250, lr:0.000100, giou_loss:   6.61, conf_loss:  38.48, prob_loss:  40.89, total_loss:  85.98\n",
      "epoch: 2 step:  140/250, lr:0.000100, giou_loss:  12.30, conf_loss:  40.03, prob_loss:  61.60, total_loss: 113.94\n",
      "epoch: 2 step:  141/250, lr:0.000100, giou_loss:   6.65, conf_loss:  37.72, prob_loss:  33.91, total_loss:  78.28\n",
      "epoch: 2 step:  142/250, lr:0.000100, giou_loss:   7.60, conf_loss:  38.64, prob_loss:  39.87, total_loss:  86.10\n",
      "epoch: 2 step:  143/250, lr:0.000100, giou_loss:   8.12, conf_loss:  37.98, prob_loss:  45.25, total_loss:  91.35\n",
      "epoch: 2 step:  144/250, lr:0.000100, giou_loss:   7.49, conf_loss:  38.22, prob_loss:  34.13, total_loss:  79.84\n",
      "epoch: 2 step:  145/250, lr:0.000100, giou_loss:   9.42, conf_loss:  38.75, prob_loss:  49.84, total_loss:  98.01\n",
      "epoch: 2 step:  146/250, lr:0.000100, giou_loss:   6.65, conf_loss:  38.76, prob_loss:  25.95, total_loss:  71.36\n",
      "epoch: 2 step:  147/250, lr:0.000100, giou_loss:   7.47, conf_loss:  38.00, prob_loss:  40.49, total_loss:  85.96\n",
      "epoch: 2 step:  148/250, lr:0.000100, giou_loss:   8.04, conf_loss:  38.21, prob_loss:  38.85, total_loss:  85.10\n",
      "epoch: 2 step:  149/250, lr:0.000100, giou_loss:   9.54, conf_loss:  36.96, prob_loss:  46.65, total_loss:  93.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step:  150/250, lr:0.000100, giou_loss:   6.30, conf_loss:  36.95, prob_loss:  29.67, total_loss:  72.93\n",
      "epoch: 2 step:  151/250, lr:0.000100, giou_loss:   9.90, conf_loss:  38.94, prob_loss:  52.04, total_loss: 100.87\n",
      "epoch: 2 step:  152/250, lr:0.000100, giou_loss:   6.28, conf_loss:  36.38, prob_loss:  36.49, total_loss:  79.16\n",
      "epoch: 2 step:  153/250, lr:0.000100, giou_loss:   8.28, conf_loss:  37.29, prob_loss:  45.92, total_loss:  91.48\n",
      "epoch: 2 step:  154/250, lr:0.000100, giou_loss:   6.76, conf_loss:  36.43, prob_loss:  29.88, total_loss:  73.08\n",
      "epoch: 2 step:  155/250, lr:0.000100, giou_loss:   8.12, conf_loss:  36.71, prob_loss:  49.55, total_loss:  94.38\n",
      "epoch: 2 step:  156/250, lr:0.000100, giou_loss:   8.24, conf_loss:  36.52, prob_loss:  38.60, total_loss:  83.36\n",
      "epoch: 2 step:  157/250, lr:0.000100, giou_loss:   9.40, conf_loss:  37.27, prob_loss:  53.10, total_loss:  99.76\n",
      "epoch: 2 step:  158/250, lr:0.000100, giou_loss:   8.28, conf_loss:  36.01, prob_loss:  44.14, total_loss:  88.42\n",
      "epoch: 2 step:  159/250, lr:0.000100, giou_loss:   6.07, conf_loss:  34.28, prob_loss:  33.50, total_loss:  73.85\n",
      "epoch: 2 step:  160/250, lr:0.000100, giou_loss:   9.05, conf_loss:  36.59, prob_loss:  44.90, total_loss:  90.54\n",
      "epoch: 2 step:  161/250, lr:0.000100, giou_loss:   7.25, conf_loss:  34.64, prob_loss:  36.59, total_loss:  78.48\n",
      "epoch: 2 step:  162/250, lr:0.000100, giou_loss:   5.75, conf_loss:  34.81, prob_loss:  33.21, total_loss:  73.77\n",
      "epoch: 2 step:  163/250, lr:0.000100, giou_loss:   5.93, conf_loss:  35.23, prob_loss:  32.73, total_loss:  73.88\n",
      "epoch: 2 step:  164/250, lr:0.000100, giou_loss:   6.76, conf_loss:  35.04, prob_loss:  34.28, total_loss:  76.08\n",
      "epoch: 2 step:  165/250, lr:0.000100, giou_loss:  11.72, conf_loss:  36.96, prob_loss:  59.45, total_loss: 108.13\n",
      "epoch: 2 step:  166/250, lr:0.000100, giou_loss:   5.32, conf_loss:  34.48, prob_loss:  34.70, total_loss:  74.50\n",
      "epoch: 2 step:  167/250, lr:0.000100, giou_loss:   6.61, conf_loss:  34.27, prob_loss:  34.39, total_loss:  75.27\n",
      "epoch: 2 step:  168/250, lr:0.000100, giou_loss:  10.54, conf_loss:  35.24, prob_loss:  47.42, total_loss:  93.20\n",
      "epoch: 2 step:  169/250, lr:0.000100, giou_loss:   8.15, conf_loss:  35.96, prob_loss:  41.44, total_loss:  85.55\n",
      "epoch: 2 step:  170/250, lr:0.000100, giou_loss:   6.40, conf_loss:  37.90, prob_loss:  28.99, total_loss:  73.29\n",
      "epoch: 2 step:  171/250, lr:0.000100, giou_loss:   9.38, conf_loss:  36.02, prob_loss:  46.36, total_loss:  91.75\n",
      "epoch: 2 step:  172/250, lr:0.000100, giou_loss:   6.85, conf_loss:  34.70, prob_loss:  30.72, total_loss:  72.27\n",
      "epoch: 2 step:  173/250, lr:0.000100, giou_loss:   8.71, conf_loss:  35.88, prob_loss:  39.63, total_loss:  84.22\n",
      "epoch: 2 step:  174/250, lr:0.000100, giou_loss:   7.42, conf_loss:  35.96, prob_loss:  37.24, total_loss:  80.63\n",
      "epoch: 2 step:  175/250, lr:0.000100, giou_loss:  10.68, conf_loss:  35.95, prob_loss:  55.56, total_loss: 102.19\n",
      "epoch: 2 step:  176/250, lr:0.000100, giou_loss:  12.16, conf_loss:  38.04, prob_loss:  57.14, total_loss: 107.34\n",
      "epoch: 2 step:  177/250, lr:0.000100, giou_loss:  10.34, conf_loss:  35.96, prob_loss:  43.61, total_loss:  89.90\n",
      "epoch: 2 step:  178/250, lr:0.000100, giou_loss:   4.94, conf_loss:  34.64, prob_loss:  23.55, total_loss:  63.14\n",
      "epoch: 2 step:  179/250, lr:0.000100, giou_loss:  11.63, conf_loss:  35.71, prob_loss:  47.75, total_loss:  95.09\n",
      "epoch: 2 step:  180/250, lr:0.000100, giou_loss:   4.73, conf_loss:  32.31, prob_loss:  26.48, total_loss:  63.52\n",
      "epoch: 2 step:  181/250, lr:0.000100, giou_loss:   6.98, conf_loss:  33.07, prob_loss:  35.78, total_loss:  75.83\n",
      "epoch: 2 step:  182/250, lr:0.000100, giou_loss:   7.01, conf_loss:  34.48, prob_loss:  42.73, total_loss:  84.22\n",
      "epoch: 2 step:  183/250, lr:0.000100, giou_loss:   8.41, conf_loss:  34.53, prob_loss:  43.86, total_loss:  86.80\n",
      "epoch: 2 step:  184/250, lr:0.000100, giou_loss:   9.63, conf_loss:  33.80, prob_loss:  45.27, total_loss:  88.70\n",
      "epoch: 2 step:  185/250, lr:0.000100, giou_loss:   8.07, conf_loss:  32.85, prob_loss:  41.75, total_loss:  82.67\n",
      "epoch: 2 step:  186/250, lr:0.000100, giou_loss:   6.55, conf_loss:  33.71, prob_loss:  31.18, total_loss:  71.44\n",
      "epoch: 2 step:  187/250, lr:0.000100, giou_loss:   8.24, conf_loss:  33.40, prob_loss:  47.17, total_loss:  88.81\n",
      "epoch: 2 step:  188/250, lr:0.000100, giou_loss:   8.98, conf_loss:  32.85, prob_loss:  45.83, total_loss:  87.66\n",
      "epoch: 2 step:  189/250, lr:0.000100, giou_loss:   9.32, conf_loss:  33.08, prob_loss:  41.32, total_loss:  83.73\n",
      "epoch: 2 step:  190/250, lr:0.000100, giou_loss:   7.98, conf_loss:  33.73, prob_loss:  48.32, total_loss:  90.02\n",
      "epoch: 2 step:  191/250, lr:0.000100, giou_loss:   6.44, conf_loss:  32.04, prob_loss:  37.32, total_loss:  75.80\n",
      "epoch: 2 step:  192/250, lr:0.000100, giou_loss:   8.21, conf_loss:  31.79, prob_loss:  43.46, total_loss:  83.45\n",
      "epoch: 2 step:  193/250, lr:0.000100, giou_loss:   6.76, conf_loss:  31.78, prob_loss:  32.17, total_loss:  70.71\n",
      "epoch: 2 step:  194/250, lr:0.000100, giou_loss:   7.88, conf_loss:  31.44, prob_loss:  40.81, total_loss:  80.13\n",
      "epoch: 2 step:  195/250, lr:0.000100, giou_loss:   8.22, conf_loss:  32.60, prob_loss:  42.68, total_loss:  83.50\n",
      "epoch: 2 step:  196/250, lr:0.000100, giou_loss:   6.78, conf_loss:  31.57, prob_loss:  36.15, total_loss:  74.50\n",
      "epoch: 2 step:  197/250, lr:0.000100, giou_loss:   9.40, conf_loss:  31.99, prob_loss:  45.47, total_loss:  86.86\n",
      "epoch: 2 step:  198/250, lr:0.000100, giou_loss:   7.88, conf_loss:  31.33, prob_loss:  39.06, total_loss:  78.27\n",
      "epoch: 2 step:  199/250, lr:0.000100, giou_loss:   7.78, conf_loss:  31.20, prob_loss:  35.45, total_loss:  74.42\n",
      "epoch: 2 step:  200/250, lr:0.000100, giou_loss:   8.86, conf_loss:  32.01, prob_loss:  36.01, total_loss:  76.89\n",
      "epoch: 2 step:  201/250, lr:0.000100, giou_loss:   5.14, conf_loss:  29.83, prob_loss:  28.50, total_loss:  63.48\n",
      "epoch: 2 step:  202/250, lr:0.000100, giou_loss:   9.31, conf_loss:  31.07, prob_loss:  46.09, total_loss:  86.47\n",
      "epoch: 2 step:  203/250, lr:0.000100, giou_loss:  10.95, conf_loss:  32.31, prob_loss:  48.59, total_loss:  91.85\n",
      "epoch: 2 step:  204/250, lr:0.000100, giou_loss:   7.42, conf_loss:  31.27, prob_loss:  36.75, total_loss:  75.43\n",
      "epoch: 2 step:  205/250, lr:0.000100, giou_loss:   9.79, conf_loss:  31.75, prob_loss:  47.72, total_loss:  89.26\n",
      "epoch: 2 step:  206/250, lr:0.000100, giou_loss:   9.74, conf_loss:  31.42, prob_loss:  52.57, total_loss:  93.73\n",
      "epoch: 2 step:  207/250, lr:0.000100, giou_loss:   6.23, conf_loss:  30.74, prob_loss:  27.01, total_loss:  63.98\n",
      "epoch: 2 step:  208/250, lr:0.000100, giou_loss:   5.82, conf_loss:  30.51, prob_loss:  22.50, total_loss:  58.83\n",
      "epoch: 2 step:  209/250, lr:0.000100, giou_loss:   9.48, conf_loss:  30.09, prob_loss:  39.06, total_loss:  78.62\n",
      "epoch: 2 step:  210/250, lr:0.000100, giou_loss:   7.01, conf_loss:  30.41, prob_loss:  24.85, total_loss:  62.26\n",
      "epoch: 2 step:  211/250, lr:0.000100, giou_loss:   8.93, conf_loss:  31.08, prob_loss:  38.77, total_loss:  78.77\n",
      "epoch: 2 step:  212/250, lr:0.000100, giou_loss:   8.52, conf_loss:  31.28, prob_loss:  36.41, total_loss:  76.21\n",
      "epoch: 2 step:  213/250, lr:0.000100, giou_loss:   9.26, conf_loss:  30.17, prob_loss:  42.63, total_loss:  82.06\n",
      "epoch: 2 step:  214/250, lr:0.000100, giou_loss:   9.70, conf_loss:  30.58, prob_loss:  43.62, total_loss:  83.90\n",
      "epoch: 2 step:  215/250, lr:0.000100, giou_loss:   8.52, conf_loss:  30.48, prob_loss:  36.23, total_loss:  75.23\n",
      "epoch: 2 step:  216/250, lr:0.000100, giou_loss:   9.52, conf_loss:  30.14, prob_loss:  50.11, total_loss:  89.77\n",
      "epoch: 2 step:  217/250, lr:0.000100, giou_loss:   8.04, conf_loss:  29.58, prob_loss:  38.57, total_loss:  76.19\n",
      "epoch: 2 step:  218/250, lr:0.000100, giou_loss:   7.52, conf_loss:  29.59, prob_loss:  31.41, total_loss:  68.53\n",
      "epoch: 2 step:  219/250, lr:0.000100, giou_loss:   5.02, conf_loss:  28.71, prob_loss:  18.22, total_loss:  51.95\n",
      "epoch: 2 step:  220/250, lr:0.000100, giou_loss:   7.38, conf_loss:  28.33, prob_loss:  28.43, total_loss:  64.14\n",
      "epoch: 2 step:  221/250, lr:0.000100, giou_loss:   8.11, conf_loss:  28.39, prob_loss:  40.44, total_loss:  76.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step:  222/250, lr:0.000100, giou_loss:   8.04, conf_loss:  30.30, prob_loss:  35.56, total_loss:  73.90\n",
      "epoch: 2 step:  223/250, lr:0.000100, giou_loss:   9.71, conf_loss:  30.92, prob_loss:  42.33, total_loss:  82.96\n",
      "epoch: 2 step:  224/250, lr:0.000100, giou_loss:   5.66, conf_loss:  29.00, prob_loss:  21.63, total_loss:  56.29\n",
      "epoch: 2 step:  225/250, lr:0.000100, giou_loss:   9.35, conf_loss:  29.91, prob_loss:  39.34, total_loss:  78.60\n",
      "epoch: 2 step:  226/250, lr:0.000100, giou_loss:   7.62, conf_loss:  28.49, prob_loss:  31.92, total_loss:  68.03\n",
      "epoch: 2 step:  227/250, lr:0.000100, giou_loss:   9.86, conf_loss:  29.66, prob_loss:  47.51, total_loss:  87.02\n",
      "epoch: 2 step:  228/250, lr:0.000100, giou_loss:   9.93, conf_loss:  29.59, prob_loss:  33.50, total_loss:  73.02\n",
      "epoch: 2 step:  229/250, lr:0.000100, giou_loss:   4.09, conf_loss:  29.03, prob_loss:  20.17, total_loss:  53.29\n",
      "epoch: 2 step:  230/250, lr:0.000100, giou_loss:   7.88, conf_loss:  30.08, prob_loss:  32.41, total_loss:  70.37\n",
      "epoch: 2 step:  231/250, lr:0.000100, giou_loss:   8.20, conf_loss:  28.78, prob_loss:  28.63, total_loss:  65.61\n",
      "epoch: 2 step:  232/250, lr:0.000100, giou_loss:  10.60, conf_loss:  30.29, prob_loss:  43.90, total_loss:  84.80\n",
      "epoch: 2 step:  233/250, lr:0.000100, giou_loss:   7.49, conf_loss:  28.56, prob_loss:  24.62, total_loss:  60.67\n",
      "epoch: 2 step:  234/250, lr:0.000100, giou_loss:   8.40, conf_loss:  28.92, prob_loss:  29.41, total_loss:  66.72\n",
      "epoch: 2 step:  235/250, lr:0.000100, giou_loss:   7.98, conf_loss:  28.61, prob_loss:  33.13, total_loss:  69.72\n",
      "epoch: 2 step:  236/250, lr:0.000100, giou_loss:   9.29, conf_loss:  30.80, prob_loss:  43.73, total_loss:  83.81\n",
      "epoch: 2 step:  237/250, lr:0.000100, giou_loss:   7.09, conf_loss:  27.75, prob_loss:  33.12, total_loss:  67.96\n",
      "epoch: 2 step:  238/250, lr:0.000100, giou_loss:   7.61, conf_loss:  27.77, prob_loss:  34.25, total_loss:  69.62\n",
      "epoch: 2 step:  239/250, lr:0.000100, giou_loss:  10.13, conf_loss:  28.66, prob_loss:  32.02, total_loss:  70.81\n",
      "epoch: 2 step:  240/250, lr:0.000100, giou_loss:   8.51, conf_loss:  29.25, prob_loss:  35.59, total_loss:  73.35\n",
      "epoch: 2 step:  241/250, lr:0.000100, giou_loss:   6.67, conf_loss:  27.78, prob_loss:  29.22, total_loss:  63.67\n",
      "epoch: 2 step:  242/250, lr:0.000100, giou_loss:   7.54, conf_loss:  28.82, prob_loss:  37.64, total_loss:  74.00\n",
      "epoch: 2 step:  243/250, lr:0.000100, giou_loss:   6.89, conf_loss:  27.21, prob_loss:  36.07, total_loss:  70.17\n",
      "epoch: 2 step:  244/250, lr:0.000100, giou_loss:   9.58, conf_loss:  28.10, prob_loss:  45.70, total_loss:  83.39\n",
      "epoch: 2 step:  245/250, lr:0.000100, giou_loss:   9.62, conf_loss:  28.27, prob_loss:  43.88, total_loss:  81.77\n",
      "epoch: 2 step:  246/250, lr:0.000100, giou_loss:   6.97, conf_loss:  26.66, prob_loss:  28.05, total_loss:  61.68\n",
      "epoch: 2 step:  247/250, lr:0.000100, giou_loss:   7.95, conf_loss:  28.38, prob_loss:  35.32, total_loss:  71.64\n",
      "epoch: 2 step:  248/250, lr:0.000100, giou_loss:   5.49, conf_loss:  26.90, prob_loss:  30.03, total_loss:  62.42\n",
      "epoch: 2 step:  249/250, lr:0.000100, giou_loss:  10.38, conf_loss:  27.92, prob_loss:  45.99, total_loss:  84.29\n",
      "epoch: 2 step:    0/250, lr:0.000100, giou_loss:   7.62, conf_loss:  27.17, prob_loss:  34.37, total_loss:  69.16\n",
      "epoch: 2 step:    1/250, lr:0.000100, giou_loss:   9.04, conf_loss:  27.37, prob_loss:  40.91, total_loss:  77.32\n",
      "\n",
      "\n",
      "giou_val_loss:  12.50, conf_val_loss:  96.47, prob_val_loss:  61.99, total_val_loss: 170.97\n",
      "\n",
      "\n",
      "epoch: 3 step:    2/250, lr:0.000100, giou_loss:   9.57, conf_loss:  26.95, prob_loss:  41.15, total_loss:  77.67\n",
      "epoch: 3 step:    3/250, lr:0.000100, giou_loss:   7.68, conf_loss:  27.35, prob_loss:  29.98, total_loss:  65.01\n",
      "epoch: 3 step:    4/250, lr:0.000100, giou_loss:   8.53, conf_loss:  26.56, prob_loss:  34.03, total_loss:  69.11\n",
      "epoch: 3 step:    5/250, lr:0.000100, giou_loss:   7.45, conf_loss:  25.78, prob_loss:  33.77, total_loss:  67.00\n",
      "epoch: 3 step:    6/250, lr:0.000100, giou_loss:   8.67, conf_loss:  27.26, prob_loss:  38.69, total_loss:  74.62\n",
      "epoch: 3 step:    7/250, lr:0.000100, giou_loss:   8.09, conf_loss:  26.95, prob_loss:  36.51, total_loss:  71.55\n",
      "epoch: 3 step:    8/250, lr:0.000100, giou_loss:   7.47, conf_loss:  27.52, prob_loss:  31.52, total_loss:  66.52\n",
      "epoch: 3 step:    9/250, lr:0.000100, giou_loss:   7.58, conf_loss:  26.61, prob_loss:  46.56, total_loss:  80.75\n",
      "epoch: 3 step:   10/250, lr:0.000100, giou_loss:   8.67, conf_loss:  27.82, prob_loss:  43.51, total_loss:  80.00\n",
      "epoch: 3 step:   11/250, lr:0.000100, giou_loss:   9.19, conf_loss:  26.96, prob_loss:  42.85, total_loss:  79.00\n",
      "epoch: 3 step:   12/250, lr:0.000100, giou_loss:  10.16, conf_loss:  26.92, prob_loss:  52.08, total_loss:  89.16\n",
      "epoch: 3 step:   13/250, lr:0.000100, giou_loss:   7.75, conf_loss:  27.03, prob_loss:  38.54, total_loss:  73.32\n",
      "epoch: 3 step:   14/250, lr:0.000100, giou_loss:   7.08, conf_loss:  25.37, prob_loss:  35.54, total_loss:  67.99\n",
      "epoch: 3 step:   15/250, lr:0.000100, giou_loss:   6.78, conf_loss:  25.62, prob_loss:  29.94, total_loss:  62.34\n",
      "epoch: 3 step:   16/250, lr:0.000100, giou_loss:   9.62, conf_loss:  26.53, prob_loss:  46.12, total_loss:  82.27\n",
      "epoch: 3 step:   17/250, lr:0.000100, giou_loss:   7.38, conf_loss:  25.83, prob_loss:  24.61, total_loss:  57.82\n",
      "epoch: 3 step:   18/250, lr:0.000100, giou_loss:   8.31, conf_loss:  25.72, prob_loss:  33.52, total_loss:  67.55\n",
      "epoch: 3 step:   19/250, lr:0.000100, giou_loss:   6.88, conf_loss:  25.32, prob_loss:  32.83, total_loss:  65.04\n",
      "epoch: 3 step:   20/250, lr:0.000100, giou_loss:   7.71, conf_loss:  26.16, prob_loss:  34.32, total_loss:  68.19\n",
      "epoch: 3 step:   21/250, lr:0.000100, giou_loss:  10.14, conf_loss:  27.60, prob_loss:  43.04, total_loss:  80.77\n",
      "epoch: 3 step:   22/250, lr:0.000100, giou_loss:   7.11, conf_loss:  23.95, prob_loss:  36.82, total_loss:  67.89\n",
      "epoch: 3 step:   23/250, lr:0.000100, giou_loss:   7.43, conf_loss:  24.66, prob_loss:  32.52, total_loss:  64.61\n",
      "epoch: 3 step:   24/250, lr:0.000100, giou_loss:   4.91, conf_loss:  24.27, prob_loss:  21.57, total_loss:  50.75\n",
      "epoch: 3 step:   25/250, lr:0.000100, giou_loss:   9.19, conf_loss:  25.95, prob_loss:  38.92, total_loss:  74.07\n",
      "epoch: 3 step:   26/250, lr:0.000100, giou_loss:   8.35, conf_loss:  24.99, prob_loss:  32.76, total_loss:  66.10\n",
      "epoch: 3 step:   27/250, lr:0.000100, giou_loss:   9.98, conf_loss:  24.12, prob_loss:  31.69, total_loss:  65.79\n",
      "epoch: 3 step:   28/250, lr:0.000100, giou_loss:   5.94, conf_loss:  24.99, prob_loss:  26.39, total_loss:  57.33\n",
      "epoch: 3 step:   29/250, lr:0.000100, giou_loss:   7.92, conf_loss:  24.48, prob_loss:  33.00, total_loss:  65.40\n",
      "epoch: 3 step:   30/250, lr:0.000100, giou_loss:   8.22, conf_loss:  25.49, prob_loss:  43.01, total_loss:  76.72\n",
      "epoch: 3 step:   31/250, lr:0.000100, giou_loss:   6.26, conf_loss:  25.36, prob_loss:  32.22, total_loss:  63.85\n",
      "epoch: 3 step:   32/250, lr:0.000100, giou_loss:   6.17, conf_loss:  24.35, prob_loss:  29.57, total_loss:  60.09\n",
      "epoch: 3 step:   33/250, lr:0.000100, giou_loss:   7.90, conf_loss:  25.61, prob_loss:  28.33, total_loss:  61.85\n",
      "epoch: 3 step:   34/250, lr:0.000100, giou_loss:   6.44, conf_loss:  23.83, prob_loss:  23.06, total_loss:  53.33\n",
      "epoch: 3 step:   35/250, lr:0.000100, giou_loss:  10.56, conf_loss:  26.35, prob_loss:  39.81, total_loss:  76.71\n",
      "epoch: 3 step:   36/250, lr:0.000100, giou_loss:   4.75, conf_loss:  23.59, prob_loss:  21.76, total_loss:  50.10\n",
      "epoch: 3 step:   37/250, lr:0.000100, giou_loss:   8.95, conf_loss:  25.49, prob_loss:  37.82, total_loss:  72.27\n",
      "epoch: 3 step:   38/250, lr:0.000100, giou_loss:   7.21, conf_loss:  24.40, prob_loss:  24.96, total_loss:  56.58\n",
      "epoch: 3 step:   39/250, lr:0.000100, giou_loss:   7.87, conf_loss:  24.24, prob_loss:  28.45, total_loss:  60.55\n",
      "epoch: 3 step:   40/250, lr:0.000100, giou_loss:   9.18, conf_loss:  24.79, prob_loss:  38.20, total_loss:  72.17\n",
      "epoch: 3 step:   41/250, lr:0.000100, giou_loss:   9.64, conf_loss:  25.25, prob_loss:  41.75, total_loss:  76.64\n",
      "epoch: 3 step:   42/250, lr:0.000100, giou_loss:   6.82, conf_loss:  24.27, prob_loss:  30.54, total_loss:  61.64\n",
      "epoch: 3 step:   43/250, lr:0.000100, giou_loss:   8.25, conf_loss:  23.66, prob_loss:  28.81, total_loss:  60.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step:   44/250, lr:0.000100, giou_loss:   5.82, conf_loss:  24.13, prob_loss:  19.42, total_loss:  49.37\n",
      "epoch: 3 step:   45/250, lr:0.000100, giou_loss:   8.95, conf_loss:  24.09, prob_loss:  27.56, total_loss:  60.60\n",
      "epoch: 3 step:   46/250, lr:0.000100, giou_loss:   7.25, conf_loss:  24.07, prob_loss:  22.69, total_loss:  54.00\n",
      "epoch: 3 step:   47/250, lr:0.000100, giou_loss:   8.84, conf_loss:  25.37, prob_loss:  38.63, total_loss:  72.84\n",
      "epoch: 3 step:   48/250, lr:0.000100, giou_loss:   6.92, conf_loss:  23.89, prob_loss:  28.41, total_loss:  59.22\n",
      "epoch: 3 step:   49/250, lr:0.000100, giou_loss:   9.18, conf_loss:  25.34, prob_loss:  28.46, total_loss:  62.98\n",
      "epoch: 3 step:   50/250, lr:0.000100, giou_loss:  10.67, conf_loss:  26.14, prob_loss:  34.24, total_loss:  71.05\n",
      "epoch: 3 step:   51/250, lr:0.000100, giou_loss:  10.00, conf_loss:  24.83, prob_loss:  37.32, total_loss:  72.16\n",
      "epoch: 3 step:   52/250, lr:0.000100, giou_loss:  11.46, conf_loss:  24.75, prob_loss:  50.51, total_loss:  86.72\n",
      "epoch: 3 step:   53/250, lr:0.000100, giou_loss:  11.73, conf_loss:  25.10, prob_loss:  43.80, total_loss:  80.62\n",
      "epoch: 3 step:   54/250, lr:0.000100, giou_loss:   8.49, conf_loss:  23.44, prob_loss:  31.85, total_loss:  63.78\n",
      "epoch: 3 step:   55/250, lr:0.000100, giou_loss:  10.58, conf_loss:  24.31, prob_loss:  42.24, total_loss:  77.13\n",
      "epoch: 3 step:   56/250, lr:0.000100, giou_loss:   8.52, conf_loss:  25.55, prob_loss:  25.06, total_loss:  59.13\n",
      "epoch: 3 step:   57/250, lr:0.000100, giou_loss:   9.06, conf_loss:  23.66, prob_loss:  31.73, total_loss:  64.45\n",
      "epoch: 3 step:   58/250, lr:0.000100, giou_loss:   6.39, conf_loss:  22.18, prob_loss:  25.66, total_loss:  54.24\n",
      "epoch: 3 step:   59/250, lr:0.000100, giou_loss:   8.85, conf_loss:  23.60, prob_loss:  40.80, total_loss:  73.25\n",
      "epoch: 3 step:   60/250, lr:0.000100, giou_loss:   9.56, conf_loss:  23.51, prob_loss:  35.59, total_loss:  68.67\n",
      "epoch: 3 step:   61/250, lr:0.000100, giou_loss:   8.92, conf_loss:  23.51, prob_loss:  40.66, total_loss:  73.09\n",
      "epoch: 3 step:   62/250, lr:0.000100, giou_loss:  11.10, conf_loss:  24.89, prob_loss:  41.21, total_loss:  77.20\n",
      "epoch: 3 step:   63/250, lr:0.000100, giou_loss:   6.40, conf_loss:  21.60, prob_loss:  29.31, total_loss:  57.31\n",
      "epoch: 3 step:   64/250, lr:0.000100, giou_loss:   7.40, conf_loss:  22.96, prob_loss:  28.54, total_loss:  58.90\n",
      "epoch: 3 step:   65/250, lr:0.000100, giou_loss:   6.15, conf_loss:  23.92, prob_loss:  28.10, total_loss:  58.17\n",
      "epoch: 3 step:   66/250, lr:0.000100, giou_loss:   6.15, conf_loss:  25.26, prob_loss:  24.64, total_loss:  56.05\n",
      "epoch: 3 step:   67/250, lr:0.000100, giou_loss:   8.06, conf_loss:  23.89, prob_loss:  33.77, total_loss:  65.73\n",
      "epoch: 3 step:   68/250, lr:0.000100, giou_loss:   7.99, conf_loss:  23.09, prob_loss:  31.05, total_loss:  62.13\n",
      "epoch: 3 step:   69/250, lr:0.000100, giou_loss:  10.05, conf_loss:  25.44, prob_loss:  33.42, total_loss:  68.91\n",
      "epoch: 3 step:   70/250, lr:0.000100, giou_loss:  10.98, conf_loss:  25.09, prob_loss:  35.06, total_loss:  71.13\n",
      "epoch: 3 step:   71/250, lr:0.000100, giou_loss:   7.05, conf_loss:  22.70, prob_loss:  30.74, total_loss:  60.49\n",
      "epoch: 3 step:   72/250, lr:0.000100, giou_loss:   6.75, conf_loss:  22.85, prob_loss:  30.94, total_loss:  60.53\n",
      "epoch: 3 step:   73/250, lr:0.000100, giou_loss:   9.55, conf_loss:  24.46, prob_loss:  27.77, total_loss:  61.78\n",
      "epoch: 3 step:   74/250, lr:0.000100, giou_loss:   9.25, conf_loss:  23.28, prob_loss:  33.34, total_loss:  65.87\n",
      "epoch: 3 step:   75/250, lr:0.000100, giou_loss:   9.87, conf_loss:  23.99, prob_loss:  34.85, total_loss:  68.71\n",
      "epoch: 3 step:   76/250, lr:0.000100, giou_loss:   7.16, conf_loss:  22.55, prob_loss:  24.55, total_loss:  54.25\n",
      "epoch: 3 step:   77/250, lr:0.000100, giou_loss:   7.45, conf_loss:  22.28, prob_loss:  26.61, total_loss:  56.35\n",
      "epoch: 3 step:   78/250, lr:0.000100, giou_loss:   4.73, conf_loss:  22.30, prob_loss:  24.74, total_loss:  51.77\n",
      "epoch: 3 step:   79/250, lr:0.000100, giou_loss:  10.13, conf_loss:  22.72, prob_loss:  35.26, total_loss:  68.12\n",
      "epoch: 3 step:   80/250, lr:0.000100, giou_loss:   9.30, conf_loss:  22.13, prob_loss:  39.47, total_loss:  70.90\n",
      "epoch: 3 step:   81/250, lr:0.000100, giou_loss:   9.29, conf_loss:  22.76, prob_loss:  28.16, total_loss:  60.21\n",
      "epoch: 3 step:   82/250, lr:0.000100, giou_loss:   6.55, conf_loss:  21.12, prob_loss:  20.58, total_loss:  48.26\n",
      "epoch: 3 step:   83/250, lr:0.000100, giou_loss:   7.82, conf_loss:  21.96, prob_loss:  25.65, total_loss:  55.43\n",
      "epoch: 3 step:   84/250, lr:0.000100, giou_loss:   9.19, conf_loss:  21.74, prob_loss:  32.86, total_loss:  63.79\n",
      "epoch: 3 step:   85/250, lr:0.000100, giou_loss:   8.96, conf_loss:  22.58, prob_loss:  35.10, total_loss:  66.64\n",
      "epoch: 3 step:   86/250, lr:0.000100, giou_loss:  10.66, conf_loss:  22.79, prob_loss:  49.20, total_loss:  82.66\n",
      "epoch: 3 step:   87/250, lr:0.000100, giou_loss:   9.45, conf_loss:  22.15, prob_loss:  37.68, total_loss:  69.28\n",
      "epoch: 3 step:   88/250, lr:0.000100, giou_loss:   7.75, conf_loss:  20.80, prob_loss:  23.10, total_loss:  51.64\n",
      "epoch: 3 step:   89/250, lr:0.000100, giou_loss:   8.90, conf_loss:  21.88, prob_loss:  41.70, total_loss:  72.47\n",
      "epoch: 3 step:   90/250, lr:0.000100, giou_loss:   8.88, conf_loss:  22.10, prob_loss:  36.01, total_loss:  66.98\n",
      "epoch: 3 step:   91/250, lr:0.000100, giou_loss:   6.86, conf_loss:  20.87, prob_loss:  35.40, total_loss:  63.13\n",
      "epoch: 3 step:   92/250, lr:0.000100, giou_loss:   7.22, conf_loss:  20.86, prob_loss:  28.32, total_loss:  56.40\n",
      "epoch: 3 step:   93/250, lr:0.000100, giou_loss:   7.29, conf_loss:  20.67, prob_loss:  25.86, total_loss:  53.82\n",
      "epoch: 3 step:   94/250, lr:0.000100, giou_loss:   9.19, conf_loss:  21.17, prob_loss:  36.29, total_loss:  66.65\n",
      "epoch: 3 step:   95/250, lr:0.000100, giou_loss:   5.50, conf_loss:  19.90, prob_loss:  19.90, total_loss:  45.30\n",
      "epoch: 3 step:   96/250, lr:0.000100, giou_loss:   7.83, conf_loss:  20.86, prob_loss:  24.74, total_loss:  53.43\n",
      "epoch: 3 step:   97/250, lr:0.000100, giou_loss:   7.40, conf_loss:  21.09, prob_loss:  29.54, total_loss:  58.02\n",
      "epoch: 3 step:   98/250, lr:0.000100, giou_loss:   5.79, conf_loss:  20.52, prob_loss:  18.07, total_loss:  44.39\n",
      "epoch: 3 step:   99/250, lr:0.000100, giou_loss:   6.75, conf_loss:  20.90, prob_loss:  30.40, total_loss:  58.05\n",
      "epoch: 3 step:  100/250, lr:0.000100, giou_loss:   6.79, conf_loss:  20.25, prob_loss:  24.11, total_loss:  51.14\n",
      "epoch: 3 step:  101/250, lr:0.000100, giou_loss:   7.89, conf_loss:  20.42, prob_loss:  27.15, total_loss:  55.46\n",
      "epoch: 3 step:  102/250, lr:0.000100, giou_loss:   8.91, conf_loss:  20.81, prob_loss:  36.55, total_loss:  66.26\n",
      "epoch: 3 step:  103/250, lr:0.000100, giou_loss:   7.42, conf_loss:  20.35, prob_loss:  24.88, total_loss:  52.65\n",
      "epoch: 3 step:  104/250, lr:0.000100, giou_loss:   7.89, conf_loss:  19.90, prob_loss:  29.27, total_loss:  57.07\n",
      "epoch: 3 step:  105/250, lr:0.000100, giou_loss:   7.79, conf_loss:  21.30, prob_loss:  30.02, total_loss:  59.12\n",
      "epoch: 3 step:  106/250, lr:0.000100, giou_loss:   7.51, conf_loss:  20.77, prob_loss:  27.19, total_loss:  55.47\n",
      "epoch: 3 step:  107/250, lr:0.000100, giou_loss:   8.33, conf_loss:  21.15, prob_loss:  28.11, total_loss:  57.59\n",
      "epoch: 3 step:  108/250, lr:0.000100, giou_loss:   8.32, conf_loss:  20.57, prob_loss:  22.50, total_loss:  51.39\n",
      "epoch: 3 step:  109/250, lr:0.000100, giou_loss:   9.01, conf_loss:  20.95, prob_loss:  29.97, total_loss:  59.94\n",
      "epoch: 3 step:  110/250, lr:0.000100, giou_loss:   7.01, conf_loss:  19.66, prob_loss:  25.75, total_loss:  52.42\n",
      "epoch: 3 step:  111/250, lr:0.000100, giou_loss:   7.85, conf_loss:  20.51, prob_loss:  35.24, total_loss:  63.60\n",
      "epoch: 3 step:  112/250, lr:0.000100, giou_loss:   6.05, conf_loss:  19.63, prob_loss:  20.39, total_loss:  46.07\n",
      "epoch: 3 step:  113/250, lr:0.000100, giou_loss:   6.68, conf_loss:  19.73, prob_loss:  20.02, total_loss:  46.43\n",
      "epoch: 3 step:  114/250, lr:0.000100, giou_loss:   6.91, conf_loss:  19.48, prob_loss:  22.37, total_loss:  48.76\n",
      "epoch: 3 step:  115/250, lr:0.000100, giou_loss:   7.18, conf_loss:  20.06, prob_loss:  25.91, total_loss:  53.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step:  116/250, lr:0.000100, giou_loss:   5.92, conf_loss:  19.61, prob_loss:  23.66, total_loss:  49.19\n",
      "epoch: 3 step:  117/250, lr:0.000100, giou_loss:   8.98, conf_loss:  21.04, prob_loss:  31.92, total_loss:  61.94\n",
      "epoch: 3 step:  118/250, lr:0.000100, giou_loss:   4.70, conf_loss:  19.14, prob_loss:  17.01, total_loss:  40.85\n",
      "epoch: 3 step:  119/250, lr:0.000100, giou_loss:   5.56, conf_loss:  19.59, prob_loss:  21.84, total_loss:  46.99\n",
      "epoch: 3 step:  120/250, lr:0.000100, giou_loss:   6.40, conf_loss:  19.10, prob_loss:  24.90, total_loss:  50.40\n",
      "epoch: 3 step:  121/250, lr:0.000100, giou_loss:   5.18, conf_loss:  19.08, prob_loss:  23.97, total_loss:  48.24\n",
      "epoch: 3 step:  122/250, lr:0.000100, giou_loss:   6.45, conf_loss:  19.48, prob_loss:  22.53, total_loss:  48.46\n",
      "epoch: 3 step:  123/250, lr:0.000100, giou_loss:   5.65, conf_loss:  19.41, prob_loss:  22.40, total_loss:  47.45\n",
      "epoch: 3 step:  124/250, lr:0.000100, giou_loss:  10.44, conf_loss:  21.11, prob_loss:  33.00, total_loss:  64.55\n",
      "epoch: 3 step:  125/250, lr:0.000100, giou_loss:   7.80, conf_loss:  19.05, prob_loss:  30.79, total_loss:  57.65\n",
      "epoch: 3 step:  126/250, lr:0.000100, giou_loss:   6.07, conf_loss:  18.73, prob_loss:  22.67, total_loss:  47.46\n",
      "epoch: 3 step:  127/250, lr:0.000100, giou_loss:   7.30, conf_loss:  18.90, prob_loss:  33.04, total_loss:  59.24\n",
      "epoch: 3 step:  128/250, lr:0.000100, giou_loss:   5.79, conf_loss:  19.90, prob_loss:  22.70, total_loss:  48.38\n",
      "epoch: 3 step:  129/250, lr:0.000100, giou_loss:   7.79, conf_loss:  19.20, prob_loss:  21.04, total_loss:  48.04\n",
      "epoch: 3 step:  130/250, lr:0.000100, giou_loss:   7.35, conf_loss:  19.72, prob_loss:  24.94, total_loss:  52.01\n",
      "epoch: 3 step:  131/250, lr:0.000100, giou_loss:   8.67, conf_loss:  19.66, prob_loss:  34.13, total_loss:  62.46\n",
      "epoch: 3 step:  132/250, lr:0.000100, giou_loss:   7.80, conf_loss:  18.43, prob_loss:  26.11, total_loss:  52.34\n",
      "epoch: 3 step:  133/250, lr:0.000100, giou_loss:   8.47, conf_loss:  19.63, prob_loss:  41.09, total_loss:  69.19\n",
      "epoch: 3 step:  134/250, lr:0.000100, giou_loss:   6.61, conf_loss:  18.32, prob_loss:  22.03, total_loss:  46.96\n",
      "epoch: 3 step:  135/250, lr:0.000100, giou_loss:   5.13, conf_loss:  18.15, prob_loss:  25.27, total_loss:  48.56\n",
      "epoch: 3 step:  136/250, lr:0.000100, giou_loss:  11.40, conf_loss:  19.62, prob_loss:  42.52, total_loss:  73.53\n",
      "epoch: 3 step:  137/250, lr:0.000100, giou_loss:   7.48, conf_loss:  19.04, prob_loss:  33.49, total_loss:  60.01\n",
      "epoch: 3 step:  138/250, lr:0.000100, giou_loss:   6.77, conf_loss:  18.49, prob_loss:  30.57, total_loss:  55.83\n",
      "epoch: 3 step:  139/250, lr:0.000100, giou_loss:   7.23, conf_loss:  18.34, prob_loss:  31.80, total_loss:  57.37\n",
      "epoch: 3 step:  140/250, lr:0.000100, giou_loss:   8.28, conf_loss:  19.50, prob_loss:  32.37, total_loss:  60.15\n",
      "epoch: 3 step:  141/250, lr:0.000100, giou_loss:   6.50, conf_loss:  21.25, prob_loss:  24.65, total_loss:  52.39\n",
      "epoch: 3 step:  142/250, lr:0.000100, giou_loss:   6.00, conf_loss:  17.62, prob_loss:  17.86, total_loss:  41.49\n",
      "epoch: 3 step:  143/250, lr:0.000100, giou_loss:   7.93, conf_loss:  19.47, prob_loss:  27.72, total_loss:  55.13\n",
      "epoch: 3 step:  144/250, lr:0.000100, giou_loss:   6.74, conf_loss:  18.94, prob_loss:  25.49, total_loss:  51.16\n",
      "epoch: 3 step:  145/250, lr:0.000100, giou_loss:   7.74, conf_loss:  18.92, prob_loss:  29.27, total_loss:  55.92\n",
      "epoch: 3 step:  146/250, lr:0.000100, giou_loss:   4.25, conf_loss:  17.70, prob_loss:  17.80, total_loss:  39.75\n",
      "epoch: 3 step:  147/250, lr:0.000100, giou_loss:   8.88, conf_loss:  19.08, prob_loss:  33.44, total_loss:  61.40\n",
      "epoch: 3 step:  148/250, lr:0.000100, giou_loss:  10.54, conf_loss:  19.90, prob_loss:  34.44, total_loss:  64.88\n",
      "epoch: 3 step:  149/250, lr:0.000100, giou_loss:  10.75, conf_loss:  20.86, prob_loss:  35.77, total_loss:  67.38\n",
      "epoch: 3 step:  150/250, lr:0.000100, giou_loss:   7.16, conf_loss:  19.05, prob_loss:  26.76, total_loss:  52.96\n",
      "epoch: 3 step:  151/250, lr:0.000100, giou_loss:   7.29, conf_loss:  18.96, prob_loss:  24.33, total_loss:  50.59\n",
      "epoch: 3 step:  152/250, lr:0.000100, giou_loss:  10.38, conf_loss:  19.48, prob_loss:  32.14, total_loss:  62.00\n",
      "epoch: 3 step:  153/250, lr:0.000100, giou_loss:   6.89, conf_loss:  18.06, prob_loss:  21.99, total_loss:  46.94\n",
      "epoch: 3 step:  154/250, lr:0.000100, giou_loss:   4.98, conf_loss:  17.96, prob_loss:  13.93, total_loss:  36.86\n",
      "epoch: 3 step:  155/250, lr:0.000100, giou_loss:  12.10, conf_loss:  20.11, prob_loss:  31.43, total_loss:  63.64\n",
      "epoch: 3 step:  156/250, lr:0.000100, giou_loss:   9.77, conf_loss:  19.21, prob_loss:  30.06, total_loss:  59.04\n",
      "epoch: 3 step:  157/250, lr:0.000100, giou_loss:   8.81, conf_loss:  20.17, prob_loss:  28.03, total_loss:  57.00\n",
      "epoch: 3 step:  158/250, lr:0.000100, giou_loss:   8.38, conf_loss:  18.31, prob_loss:  24.12, total_loss:  50.82\n",
      "epoch: 3 step:  159/250, lr:0.000100, giou_loss:   6.67, conf_loss:  17.84, prob_loss:  27.48, total_loss:  51.99\n",
      "epoch: 3 step:  160/250, lr:0.000100, giou_loss:   8.41, conf_loss:  18.46, prob_loss:  27.44, total_loss:  54.31\n",
      "epoch: 3 step:  161/250, lr:0.000100, giou_loss:   5.17, conf_loss:  16.90, prob_loss:  15.05, total_loss:  37.11\n",
      "epoch: 3 step:  162/250, lr:0.000100, giou_loss:   6.05, conf_loss:  18.65, prob_loss:  23.17, total_loss:  47.88\n",
      "epoch: 3 step:  163/250, lr:0.000100, giou_loss:   9.12, conf_loss:  18.92, prob_loss:  36.05, total_loss:  64.09\n",
      "epoch: 3 step:  164/250, lr:0.000100, giou_loss:  11.26, conf_loss:  19.69, prob_loss:  41.19, total_loss:  72.13\n",
      "epoch: 3 step:  165/250, lr:0.000100, giou_loss:  11.09, conf_loss:  18.61, prob_loss:  33.56, total_loss:  63.26\n",
      "epoch: 3 step:  166/250, lr:0.000100, giou_loss:   6.15, conf_loss:  17.23, prob_loss:  28.66, total_loss:  52.04\n",
      "epoch: 3 step:  167/250, lr:0.000100, giou_loss:   7.55, conf_loss:  16.83, prob_loss:  23.93, total_loss:  48.31\n",
      "epoch: 3 step:  168/250, lr:0.000100, giou_loss:   6.07, conf_loss:  17.41, prob_loss:  21.56, total_loss:  45.05\n",
      "epoch: 3 step:  169/250, lr:0.000100, giou_loss:   5.63, conf_loss:  17.19, prob_loss:  12.30, total_loss:  35.12\n",
      "epoch: 3 step:  170/250, lr:0.000100, giou_loss:   5.74, conf_loss:  17.08, prob_loss:  20.16, total_loss:  42.98\n",
      "epoch: 3 step:  171/250, lr:0.000100, giou_loss:   9.49, conf_loss:  18.79, prob_loss:  38.39, total_loss:  66.67\n",
      "epoch: 3 step:  172/250, lr:0.000100, giou_loss:   9.07, conf_loss:  19.61, prob_loss:  33.22, total_loss:  61.90\n",
      "epoch: 3 step:  173/250, lr:0.000100, giou_loss:   8.74, conf_loss:  18.15, prob_loss:  29.12, total_loss:  56.00\n",
      "epoch: 3 step:  174/250, lr:0.000100, giou_loss:   7.28, conf_loss:  17.01, prob_loss:  21.68, total_loss:  45.96\n",
      "epoch: 3 step:  175/250, lr:0.000100, giou_loss:   7.72, conf_loss:  17.13, prob_loss:  24.58, total_loss:  49.43\n",
      "epoch: 3 step:  176/250, lr:0.000100, giou_loss:   7.59, conf_loss:  17.49, prob_loss:  23.17, total_loss:  48.24\n",
      "epoch: 3 step:  177/250, lr:0.000100, giou_loss:   6.39, conf_loss:  16.87, prob_loss:  24.39, total_loss:  47.65\n",
      "epoch: 3 step:  178/250, lr:0.000100, giou_loss:   6.05, conf_loss:  17.34, prob_loss:  17.67, total_loss:  41.06\n",
      "epoch: 3 step:  179/250, lr:0.000100, giou_loss:   5.65, conf_loss:  16.96, prob_loss:  18.92, total_loss:  41.53\n",
      "epoch: 3 step:  180/250, lr:0.000100, giou_loss:   6.77, conf_loss:  16.74, prob_loss:  14.77, total_loss:  38.28\n",
      "epoch: 3 step:  181/250, lr:0.000100, giou_loss:   7.66, conf_loss:  16.90, prob_loss:  22.54, total_loss:  47.10\n",
      "epoch: 3 step:  182/250, lr:0.000100, giou_loss:   8.14, conf_loss:  17.32, prob_loss:  26.30, total_loss:  51.76\n",
      "epoch: 3 step:  183/250, lr:0.000100, giou_loss:   5.28, conf_loss:  16.20, prob_loss:  18.01, total_loss:  39.49\n",
      "epoch: 3 step:  184/250, lr:0.000100, giou_loss:   9.31, conf_loss:  17.90, prob_loss:  23.69, total_loss:  50.89\n",
      "epoch: 3 step:  185/250, lr:0.000100, giou_loss:   6.98, conf_loss:  16.95, prob_loss:  20.45, total_loss:  44.37\n",
      "epoch: 3 step:  186/250, lr:0.000100, giou_loss:   8.76, conf_loss:  17.86, prob_loss:  27.67, total_loss:  54.30\n",
      "epoch: 3 step:  187/250, lr:0.000100, giou_loss:   7.11, conf_loss:  16.85, prob_loss:  25.28, total_loss:  49.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step:  188/250, lr:0.000100, giou_loss:   7.68, conf_loss:  17.32, prob_loss:  28.97, total_loss:  53.97\n",
      "epoch: 3 step:  189/250, lr:0.000100, giou_loss:   6.42, conf_loss:  17.06, prob_loss:  15.96, total_loss:  39.44\n",
      "epoch: 3 step:  190/250, lr:0.000100, giou_loss:   5.25, conf_loss:  15.92, prob_loss:  19.89, total_loss:  41.06\n",
      "epoch: 3 step:  191/250, lr:0.000100, giou_loss:   9.16, conf_loss:  17.41, prob_loss:  30.75, total_loss:  57.32\n",
      "epoch: 3 step:  192/250, lr:0.000100, giou_loss:   8.62, conf_loss:  17.68, prob_loss:  29.98, total_loss:  56.28\n",
      "epoch: 3 step:  193/250, lr:0.000100, giou_loss:   7.85, conf_loss:  17.56, prob_loss:  25.46, total_loss:  50.87\n",
      "epoch: 3 step:  194/250, lr:0.000100, giou_loss:   8.22, conf_loss:  17.43, prob_loss:  27.95, total_loss:  53.60\n",
      "epoch: 3 step:  195/250, lr:0.000100, giou_loss:   5.44, conf_loss:  16.01, prob_loss:  17.76, total_loss:  39.21\n",
      "epoch: 3 step:  196/250, lr:0.000100, giou_loss:   9.33, conf_loss:  16.80, prob_loss:  26.08, total_loss:  52.22\n",
      "epoch: 3 step:  197/250, lr:0.000100, giou_loss:   9.38, conf_loss:  17.79, prob_loss:  28.97, total_loss:  56.14\n",
      "epoch: 3 step:  198/250, lr:0.000100, giou_loss:   7.81, conf_loss:  16.93, prob_loss:  31.14, total_loss:  55.88\n",
      "epoch: 3 step:  199/250, lr:0.000100, giou_loss:   6.80, conf_loss:  15.91, prob_loss:  22.58, total_loss:  45.28\n",
      "epoch: 3 step:  200/250, lr:0.000100, giou_loss:   6.03, conf_loss:  15.58, prob_loss:  13.01, total_loss:  34.62\n",
      "epoch: 3 step:  201/250, lr:0.000100, giou_loss:   6.97, conf_loss:  16.05, prob_loss:  19.42, total_loss:  42.45\n",
      "epoch: 3 step:  202/250, lr:0.000100, giou_loss:   9.59, conf_loss:  16.79, prob_loss:  30.09, total_loss:  56.48\n",
      "epoch: 3 step:  203/250, lr:0.000100, giou_loss:   8.05, conf_loss:  16.60, prob_loss:  25.69, total_loss:  50.34\n",
      "epoch: 3 step:  204/250, lr:0.000100, giou_loss:   6.99, conf_loss:  16.17, prob_loss:  31.72, total_loss:  54.88\n",
      "epoch: 3 step:  205/250, lr:0.000100, giou_loss:   7.70, conf_loss:  15.94, prob_loss:  31.97, total_loss:  55.61\n",
      "epoch: 3 step:  206/250, lr:0.000100, giou_loss:   6.66, conf_loss:  15.39, prob_loss:  20.51, total_loss:  42.56\n",
      "epoch: 3 step:  207/250, lr:0.000100, giou_loss:   6.03, conf_loss:  15.62, prob_loss:  21.85, total_loss:  43.50\n",
      "epoch: 3 step:  208/250, lr:0.000100, giou_loss:   6.26, conf_loss:  15.64, prob_loss:  19.77, total_loss:  41.66\n",
      "epoch: 3 step:  209/250, lr:0.000100, giou_loss:   5.44, conf_loss:  15.11, prob_loss:  20.06, total_loss:  40.61\n",
      "epoch: 3 step:  210/250, lr:0.000100, giou_loss:   9.23, conf_loss:  16.81, prob_loss:  30.96, total_loss:  57.00\n",
      "epoch: 3 step:  211/250, lr:0.000100, giou_loss:   8.69, conf_loss:  17.37, prob_loss:  38.89, total_loss:  64.96\n",
      "epoch: 3 step:  212/250, lr:0.000100, giou_loss:   8.69, conf_loss:  17.65, prob_loss:  32.41, total_loss:  58.74\n",
      "epoch: 3 step:  213/250, lr:0.000100, giou_loss:   8.72, conf_loss:  18.85, prob_loss:  20.96, total_loss:  48.54\n",
      "epoch: 3 step:  214/250, lr:0.000100, giou_loss:   6.54, conf_loss:  16.50, prob_loss:  19.87, total_loss:  42.91\n",
      "epoch: 3 step:  215/250, lr:0.000100, giou_loss:   8.25, conf_loss:  17.40, prob_loss:  34.77, total_loss:  60.42\n",
      "epoch: 3 step:  216/250, lr:0.000100, giou_loss:   7.99, conf_loss:  15.37, prob_loss:  22.53, total_loss:  45.89\n",
      "epoch: 3 step:  217/250, lr:0.000100, giou_loss:   6.48, conf_loss:  15.91, prob_loss:  21.41, total_loss:  43.79\n",
      "epoch: 3 step:  218/250, lr:0.000100, giou_loss:   9.44, conf_loss:  17.19, prob_loss:  26.77, total_loss:  53.40\n",
      "epoch: 3 step:  219/250, lr:0.000100, giou_loss:   5.44, conf_loss:  15.01, prob_loss:  12.15, total_loss:  32.61\n",
      "epoch: 3 step:  220/250, lr:0.000100, giou_loss:   9.11, conf_loss:  17.00, prob_loss:  24.92, total_loss:  51.02\n",
      "epoch: 3 step:  221/250, lr:0.000100, giou_loss:   8.27, conf_loss:  15.41, prob_loss:  22.64, total_loss:  46.32\n",
      "epoch: 3 step:  222/250, lr:0.000100, giou_loss:   7.11, conf_loss:  15.21, prob_loss:  26.37, total_loss:  48.69\n",
      "epoch: 3 step:  223/250, lr:0.000100, giou_loss:   7.67, conf_loss:  16.41, prob_loss:  28.60, total_loss:  52.69\n",
      "epoch: 3 step:  224/250, lr:0.000100, giou_loss:   7.24, conf_loss:  16.00, prob_loss:  19.20, total_loss:  42.44\n",
      "epoch: 3 step:  225/250, lr:0.000100, giou_loss:   7.63, conf_loss:  15.34, prob_loss:  18.90, total_loss:  41.87\n",
      "epoch: 3 step:  226/250, lr:0.000100, giou_loss:   8.06, conf_loss:  15.63, prob_loss:  24.89, total_loss:  48.58\n",
      "epoch: 3 step:  227/250, lr:0.000100, giou_loss:   7.21, conf_loss:  15.86, prob_loss:  29.37, total_loss:  52.45\n",
      "epoch: 3 step:  228/250, lr:0.000100, giou_loss:   6.57, conf_loss:  15.29, prob_loss:  27.01, total_loss:  48.87\n",
      "epoch: 3 step:  229/250, lr:0.000100, giou_loss:   6.65, conf_loss:  15.70, prob_loss:  21.77, total_loss:  44.12\n",
      "epoch: 3 step:  230/250, lr:0.000100, giou_loss:   8.24, conf_loss:  16.02, prob_loss:  19.37, total_loss:  43.63\n",
      "epoch: 3 step:  231/250, lr:0.000100, giou_loss:   8.06, conf_loss:  15.54, prob_loss:  22.41, total_loss:  46.02\n",
      "epoch: 3 step:  232/250, lr:0.000100, giou_loss:   8.00, conf_loss:  15.80, prob_loss:  23.24, total_loss:  47.04\n",
      "epoch: 3 step:  233/250, lr:0.000100, giou_loss:   8.33, conf_loss:  16.95, prob_loss:  29.52, total_loss:  54.79\n",
      "epoch: 3 step:  234/250, lr:0.000100, giou_loss:   8.28, conf_loss:  15.62, prob_loss:  25.18, total_loss:  49.08\n",
      "epoch: 3 step:  235/250, lr:0.000100, giou_loss:   6.65, conf_loss:  14.73, prob_loss:  15.40, total_loss:  36.79\n",
      "epoch: 3 step:  236/250, lr:0.000100, giou_loss:   8.51, conf_loss:  16.02, prob_loss:  28.25, total_loss:  52.77\n",
      "epoch: 3 step:  237/250, lr:0.000100, giou_loss:   6.82, conf_loss:  15.32, prob_loss:  20.25, total_loss:  42.40\n",
      "epoch: 3 step:  238/250, lr:0.000100, giou_loss:   7.79, conf_loss:  15.34, prob_loss:  24.71, total_loss:  47.84\n",
      "epoch: 3 step:  239/250, lr:0.000100, giou_loss:   7.16, conf_loss:  16.07, prob_loss:  21.39, total_loss:  44.62\n",
      "epoch: 3 step:  240/250, lr:0.000100, giou_loss:   7.59, conf_loss:  14.68, prob_loss:  21.15, total_loss:  43.43\n",
      "epoch: 3 step:  241/250, lr:0.000100, giou_loss:   5.81, conf_loss:  14.67, prob_loss:  14.85, total_loss:  35.32\n",
      "epoch: 3 step:  242/250, lr:0.000100, giou_loss:   8.89, conf_loss:  15.92, prob_loss:  28.39, total_loss:  53.19\n",
      "epoch: 3 step:  243/250, lr:0.000100, giou_loss:   7.47, conf_loss:  15.51, prob_loss:  24.59, total_loss:  47.57\n",
      "epoch: 3 step:  244/250, lr:0.000100, giou_loss:   9.81, conf_loss:  16.75, prob_loss:  26.11, total_loss:  52.67\n",
      "epoch: 3 step:  245/250, lr:0.000100, giou_loss:   5.72, conf_loss:  14.77, prob_loss:  21.46, total_loss:  41.96\n",
      "epoch: 3 step:  246/250, lr:0.000100, giou_loss:   7.30, conf_loss:  15.29, prob_loss:  23.67, total_loss:  46.25\n",
      "epoch: 3 step:  247/250, lr:0.000100, giou_loss:   6.06, conf_loss:  14.90, prob_loss:  19.82, total_loss:  40.78\n",
      "epoch: 3 step:  248/250, lr:0.000100, giou_loss:   6.68, conf_loss:  13.77, prob_loss:  17.80, total_loss:  38.24\n",
      "epoch: 3 step:  249/250, lr:0.000100, giou_loss:   8.44, conf_loss:  14.77, prob_loss:  22.14, total_loss:  45.35\n",
      "epoch: 3 step:    0/250, lr:0.000100, giou_loss:   8.05, conf_loss:  14.87, prob_loss:  26.63, total_loss:  49.55\n",
      "epoch: 3 step:    1/250, lr:0.000100, giou_loss:   7.35, conf_loss:  14.55, prob_loss:  26.66, total_loss:  48.57\n",
      "\n",
      "\n",
      "giou_val_loss:  16.73, conf_val_loss: 136.69, prob_val_loss: 132.78, total_val_loss: 286.21\n",
      "\n",
      "\n",
      "epoch: 4 step:    2/250, lr:0.000100, giou_loss:   7.63, conf_loss:  14.77, prob_loss:  18.80, total_loss:  41.20\n",
      "epoch: 4 step:    3/250, lr:0.000100, giou_loss:   7.52, conf_loss:  14.45, prob_loss:  21.27, total_loss:  43.24\n",
      "epoch: 4 step:    4/250, lr:0.000100, giou_loss:   7.37, conf_loss:  15.77, prob_loss:  22.34, total_loss:  45.48\n",
      "epoch: 4 step:    5/250, lr:0.000100, giou_loss:   5.65, conf_loss:  14.63, prob_loss:  20.37, total_loss:  40.66\n",
      "epoch: 4 step:    6/250, lr:0.000100, giou_loss:   9.66, conf_loss:  16.33, prob_loss:  25.38, total_loss:  51.37\n",
      "epoch: 4 step:    7/250, lr:0.000100, giou_loss:   7.91, conf_loss:  14.61, prob_loss:  24.33, total_loss:  46.85\n",
      "epoch: 4 step:    8/250, lr:0.000100, giou_loss:   4.81, conf_loss:  14.16, prob_loss:  21.53, total_loss:  40.50\n",
      "epoch: 4 step:    9/250, lr:0.000100, giou_loss:   7.52, conf_loss:  15.37, prob_loss:  23.03, total_loss:  45.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step:   10/250, lr:0.000100, giou_loss:   6.39, conf_loss:  15.01, prob_loss:  13.47, total_loss:  34.88\n",
      "epoch: 4 step:   11/250, lr:0.000100, giou_loss:   7.66, conf_loss:  14.84, prob_loss:  21.32, total_loss:  43.83\n",
      "epoch: 4 step:   12/250, lr:0.000100, giou_loss:   7.16, conf_loss:  15.59, prob_loss:  22.38, total_loss:  45.14\n",
      "epoch: 4 step:   13/250, lr:0.000100, giou_loss:   7.66, conf_loss:  15.08, prob_loss:  22.34, total_loss:  45.08\n",
      "epoch: 4 step:   14/250, lr:0.000100, giou_loss:   6.07, conf_loss:  14.12, prob_loss:  18.47, total_loss:  38.66\n",
      "epoch: 4 step:   15/250, lr:0.000100, giou_loss:   5.20, conf_loss:  15.23, prob_loss:  17.64, total_loss:  38.07\n",
      "epoch: 4 step:   16/250, lr:0.000100, giou_loss:   7.70, conf_loss:  15.40, prob_loss:  19.89, total_loss:  42.99\n",
      "epoch: 4 step:   17/250, lr:0.000100, giou_loss:   8.52, conf_loss:  15.84, prob_loss:  26.09, total_loss:  50.45\n",
      "epoch: 4 step:   18/250, lr:0.000100, giou_loss:   7.78, conf_loss:  15.15, prob_loss:  16.66, total_loss:  39.60\n",
      "epoch: 4 step:   19/250, lr:0.000100, giou_loss:   5.47, conf_loss:  14.21, prob_loss:  15.76, total_loss:  35.44\n",
      "epoch: 4 step:   20/250, lr:0.000100, giou_loss:   7.22, conf_loss:  14.31, prob_loss:  22.80, total_loss:  44.33\n",
      "epoch: 4 step:   21/250, lr:0.000100, giou_loss:   8.64, conf_loss:  16.13, prob_loss:  17.09, total_loss:  41.86\n",
      "epoch: 4 step:   22/250, lr:0.000100, giou_loss:   6.73, conf_loss:  14.09, prob_loss:  10.11, total_loss:  30.93\n",
      "epoch: 4 step:   23/250, lr:0.000100, giou_loss:   8.61, conf_loss:  15.42, prob_loss:  25.05, total_loss:  49.09\n",
      "epoch: 4 step:   24/250, lr:0.000100, giou_loss:   6.69, conf_loss:  14.36, prob_loss:  18.22, total_loss:  39.27\n",
      "epoch: 4 step:   25/250, lr:0.000100, giou_loss:   8.32, conf_loss:  15.02, prob_loss:  22.07, total_loss:  45.41\n",
      "epoch: 4 step:   26/250, lr:0.000100, giou_loss:   6.83, conf_loss:  15.45, prob_loss:  24.02, total_loss:  46.31\n",
      "epoch: 4 step:   27/250, lr:0.000100, giou_loss:   8.43, conf_loss:  15.03, prob_loss:  24.80, total_loss:  48.26\n",
      "epoch: 4 step:   28/250, lr:0.000100, giou_loss:   7.91, conf_loss:  13.94, prob_loss:  17.25, total_loss:  39.10\n",
      "epoch: 4 step:   29/250, lr:0.000100, giou_loss:   6.52, conf_loss:  13.67, prob_loss:  26.43, total_loss:  46.62\n",
      "epoch: 4 step:   30/250, lr:0.000100, giou_loss:   5.50, conf_loss:  15.04, prob_loss:  13.04, total_loss:  33.58\n",
      "epoch: 4 step:   31/250, lr:0.000100, giou_loss:   5.50, conf_loss:  12.88, prob_loss:  19.67, total_loss:  38.05\n",
      "epoch: 4 step:   32/250, lr:0.000100, giou_loss:   7.24, conf_loss:  14.82, prob_loss:  25.96, total_loss:  48.02\n",
      "epoch: 4 step:   33/250, lr:0.000100, giou_loss:   6.47, conf_loss:  14.50, prob_loss:  17.12, total_loss:  38.09\n",
      "epoch: 4 step:   34/250, lr:0.000100, giou_loss:   7.43, conf_loss:  13.72, prob_loss:  22.29, total_loss:  43.44\n",
      "epoch: 4 step:   35/250, lr:0.000100, giou_loss:   9.06, conf_loss:  14.97, prob_loss:  27.54, total_loss:  51.57\n",
      "epoch: 4 step:   36/250, lr:0.000100, giou_loss:   9.80, conf_loss:  14.16, prob_loss:  21.83, total_loss:  45.78\n",
      "epoch: 4 step:   37/250, lr:0.000100, giou_loss:  10.04, conf_loss:  15.22, prob_loss:  32.99, total_loss:  58.26\n",
      "epoch: 4 step:   38/250, lr:0.000100, giou_loss:   7.53, conf_loss:  13.47, prob_loss:  16.78, total_loss:  37.78\n",
      "epoch: 4 step:   39/250, lr:0.000100, giou_loss:   9.38, conf_loss:  15.57, prob_loss:  35.41, total_loss:  60.37\n",
      "epoch: 4 step:   40/250, lr:0.000100, giou_loss:   8.97, conf_loss:  15.05, prob_loss:  21.85, total_loss:  45.87\n",
      "epoch: 4 step:   41/250, lr:0.000100, giou_loss:   8.95, conf_loss:  14.02, prob_loss:  23.14, total_loss:  46.11\n",
      "epoch: 4 step:   42/250, lr:0.000100, giou_loss:  11.82, conf_loss:  16.71, prob_loss:  32.79, total_loss:  61.32\n",
      "epoch: 4 step:   43/250, lr:0.000100, giou_loss:   5.09, conf_loss:  13.90, prob_loss:  20.32, total_loss:  39.31\n",
      "epoch: 4 step:   44/250, lr:0.000100, giou_loss:   5.47, conf_loss:  13.07, prob_loss:  12.32, total_loss:  30.85\n",
      "epoch: 4 step:   45/250, lr:0.000100, giou_loss:   7.34, conf_loss:  13.10, prob_loss:  23.96, total_loss:  44.40\n",
      "epoch: 4 step:   46/250, lr:0.000100, giou_loss:   7.89, conf_loss:  13.64, prob_loss:  22.88, total_loss:  44.41\n",
      "epoch: 4 step:   47/250, lr:0.000100, giou_loss:   8.14, conf_loss:  14.62, prob_loss:  24.77, total_loss:  47.52\n",
      "epoch: 4 step:   48/250, lr:0.000100, giou_loss:   7.11, conf_loss:  13.09, prob_loss:  19.92, total_loss:  40.12\n",
      "epoch: 4 step:   49/250, lr:0.000100, giou_loss:   8.80, conf_loss:  13.80, prob_loss:  23.73, total_loss:  46.33\n",
      "epoch: 4 step:   50/250, lr:0.000100, giou_loss:   6.98, conf_loss:  13.58, prob_loss:  13.52, total_loss:  34.08\n",
      "epoch: 4 step:   51/250, lr:0.000100, giou_loss:   6.69, conf_loss:  13.56, prob_loss:  17.13, total_loss:  37.39\n",
      "epoch: 4 step:   52/250, lr:0.000100, giou_loss:   7.23, conf_loss:  14.54, prob_loss:  28.90, total_loss:  50.67\n",
      "epoch: 4 step:   53/250, lr:0.000100, giou_loss:   7.87, conf_loss:  13.47, prob_loss:  25.63, total_loss:  46.96\n",
      "epoch: 4 step:   54/250, lr:0.000100, giou_loss:   6.64, conf_loss:  13.04, prob_loss:  20.84, total_loss:  40.53\n",
      "epoch: 4 step:   55/250, lr:0.000100, giou_loss:   8.52, conf_loss:  14.09, prob_loss:  22.84, total_loss:  45.45\n",
      "epoch: 4 step:   56/250, lr:0.000100, giou_loss:   6.41, conf_loss:  13.72, prob_loss:  24.06, total_loss:  44.19\n",
      "epoch: 4 step:   57/250, lr:0.000100, giou_loss:   9.67, conf_loss:  14.45, prob_loss:  21.89, total_loss:  46.02\n",
      "epoch: 4 step:   58/250, lr:0.000100, giou_loss:   5.50, conf_loss:  13.08, prob_loss:  15.08, total_loss:  33.66\n",
      "epoch: 4 step:   59/250, lr:0.000100, giou_loss:   6.78, conf_loss:  13.89, prob_loss:  23.03, total_loss:  43.71\n",
      "epoch: 4 step:   60/250, lr:0.000100, giou_loss:   8.92, conf_loss:  14.66, prob_loss:  32.69, total_loss:  56.27\n",
      "epoch: 4 step:   61/250, lr:0.000100, giou_loss:   6.35, conf_loss:  13.26, prob_loss:  11.83, total_loss:  31.44\n",
      "epoch: 4 step:   62/250, lr:0.000100, giou_loss:   8.86, conf_loss:  15.13, prob_loss:  25.65, total_loss:  49.64\n",
      "epoch: 4 step:   63/250, lr:0.000100, giou_loss:   8.65, conf_loss:  13.67, prob_loss:  24.31, total_loss:  46.63\n",
      "epoch: 4 step:   64/250, lr:0.000100, giou_loss:   7.16, conf_loss:  13.14, prob_loss:  20.61, total_loss:  40.91\n",
      "epoch: 4 step:   65/250, lr:0.000100, giou_loss:   4.45, conf_loss:  13.23, prob_loss:  13.88, total_loss:  31.56\n",
      "epoch: 4 step:   66/250, lr:0.000100, giou_loss:   5.47, conf_loss:  12.58, prob_loss:  17.06, total_loss:  35.10\n",
      "epoch: 4 step:   67/250, lr:0.000100, giou_loss:   7.82, conf_loss:  13.25, prob_loss:  19.14, total_loss:  40.21\n",
      "epoch: 4 step:   68/250, lr:0.000100, giou_loss:   6.28, conf_loss:  12.74, prob_loss:  13.99, total_loss:  33.01\n",
      "epoch: 4 step:   69/250, lr:0.000100, giou_loss:   7.08, conf_loss:  13.04, prob_loss:  28.33, total_loss:  48.44\n",
      "epoch: 4 step:   70/250, lr:0.000100, giou_loss:   7.42, conf_loss:  12.65, prob_loss:  12.85, total_loss:  32.92\n",
      "epoch: 4 step:   71/250, lr:0.000100, giou_loss:   7.70, conf_loss:  13.26, prob_loss:  27.11, total_loss:  48.07\n",
      "epoch: 4 step:   72/250, lr:0.000100, giou_loss:   7.83, conf_loss:  12.45, prob_loss:  16.36, total_loss:  36.64\n",
      "epoch: 4 step:   73/250, lr:0.000100, giou_loss:   9.34, conf_loss:  13.37, prob_loss:  18.44, total_loss:  41.15\n",
      "epoch: 4 step:   74/250, lr:0.000100, giou_loss:   8.81, conf_loss:  12.88, prob_loss:  28.79, total_loss:  50.48\n",
      "epoch: 4 step:   75/250, lr:0.000100, giou_loss:  12.93, conf_loss:  14.95, prob_loss:  41.87, total_loss:  69.75\n",
      "epoch: 4 step:   76/250, lr:0.000100, giou_loss:   6.18, conf_loss:  12.35, prob_loss:  12.26, total_loss:  30.78\n",
      "epoch: 4 step:   77/250, lr:0.000100, giou_loss:   6.83, conf_loss:  12.59, prob_loss:  18.23, total_loss:  37.65\n",
      "epoch: 4 step:   78/250, lr:0.000100, giou_loss:   8.15, conf_loss:  14.65, prob_loss:  16.66, total_loss:  39.46\n",
      "epoch: 4 step:   79/250, lr:0.000100, giou_loss:   8.73, conf_loss:  13.47, prob_loss:  19.06, total_loss:  41.26\n",
      "epoch: 4 step:   80/250, lr:0.000100, giou_loss:   9.66, conf_loss:  13.67, prob_loss:  23.54, total_loss:  46.87\n",
      "epoch: 4 step:   81/250, lr:0.000100, giou_loss:   8.09, conf_loss:  13.67, prob_loss:  21.91, total_loss:  43.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step:   82/250, lr:0.000100, giou_loss:   6.19, conf_loss:  13.05, prob_loss:  15.89, total_loss:  35.12\n",
      "epoch: 4 step:   83/250, lr:0.000100, giou_loss:   7.08, conf_loss:  13.15, prob_loss:  20.98, total_loss:  41.21\n",
      "epoch: 4 step:   84/250, lr:0.000100, giou_loss:   8.02, conf_loss:  13.31, prob_loss:  27.11, total_loss:  48.44\n",
      "epoch: 4 step:   85/250, lr:0.000100, giou_loss:   6.81, conf_loss:  13.16, prob_loss:  23.14, total_loss:  43.11\n",
      "epoch: 4 step:   86/250, lr:0.000100, giou_loss:   6.44, conf_loss:  11.81, prob_loss:  16.40, total_loss:  34.65\n",
      "epoch: 4 step:   87/250, lr:0.000100, giou_loss:   8.92, conf_loss:  13.27, prob_loss:  24.69, total_loss:  46.87\n",
      "epoch: 4 step:   88/250, lr:0.000100, giou_loss:   9.27, conf_loss:  12.97, prob_loss:  16.25, total_loss:  38.49\n",
      "epoch: 4 step:   89/250, lr:0.000100, giou_loss:   8.28, conf_loss:  12.94, prob_loss:  21.50, total_loss:  42.72\n",
      "epoch: 4 step:   90/250, lr:0.000100, giou_loss:   9.50, conf_loss:  14.30, prob_loss:  18.20, total_loss:  42.00\n",
      "epoch: 4 step:   91/250, lr:0.000100, giou_loss:   6.96, conf_loss:  12.82, prob_loss:  19.20, total_loss:  38.98\n",
      "epoch: 4 step:   92/250, lr:0.000100, giou_loss:   9.60, conf_loss:  13.69, prob_loss:  29.00, total_loss:  52.29\n",
      "epoch: 4 step:   93/250, lr:0.000100, giou_loss:   6.75, conf_loss:  12.65, prob_loss:  18.18, total_loss:  37.58\n",
      "epoch: 4 step:   94/250, lr:0.000100, giou_loss:   5.05, conf_loss:  11.53, prob_loss:   7.76, total_loss:  24.34\n",
      "epoch: 4 step:   95/250, lr:0.000100, giou_loss:   6.68, conf_loss:  12.23, prob_loss:  17.92, total_loss:  36.82\n",
      "epoch: 4 step:   96/250, lr:0.000100, giou_loss:   6.79, conf_loss:  12.90, prob_loss:  21.26, total_loss:  40.96\n",
      "epoch: 4 step:   97/250, lr:0.000100, giou_loss:   6.79, conf_loss:  11.98, prob_loss:  16.22, total_loss:  34.99\n",
      "epoch: 4 step:   98/250, lr:0.000100, giou_loss:   8.46, conf_loss:  12.44, prob_loss:  19.31, total_loss:  40.21\n",
      "epoch: 4 step:   99/250, lr:0.000100, giou_loss:   6.56, conf_loss:  12.46, prob_loss:  15.26, total_loss:  34.27\n",
      "epoch: 4 step:  100/250, lr:0.000100, giou_loss:   3.35, conf_loss:  11.30, prob_loss:   5.92, total_loss:  20.58\n",
      "epoch: 4 step:  101/250, lr:0.000100, giou_loss:   6.72, conf_loss:  11.82, prob_loss:  13.06, total_loss:  31.61\n",
      "epoch: 4 step:  102/250, lr:0.000100, giou_loss:   7.34, conf_loss:  12.10, prob_loss:  19.61, total_loss:  39.05\n",
      "epoch: 4 step:  103/250, lr:0.000100, giou_loss:   6.06, conf_loss:  13.62, prob_loss:  18.08, total_loss:  37.76\n",
      "epoch: 4 step:  104/250, lr:0.000100, giou_loss:   9.10, conf_loss:  14.02, prob_loss:  22.75, total_loss:  45.87\n",
      "epoch: 4 step:  105/250, lr:0.000100, giou_loss:   7.86, conf_loss:  12.00, prob_loss:  22.93, total_loss:  42.79\n",
      "epoch: 4 step:  106/250, lr:0.000100, giou_loss:   8.04, conf_loss:  11.95, prob_loss:  16.10, total_loss:  36.09\n",
      "epoch: 4 step:  107/250, lr:0.000100, giou_loss:   7.45, conf_loss:  11.11, prob_loss:  19.39, total_loss:  37.96\n",
      "epoch: 4 step:  108/250, lr:0.000100, giou_loss:   7.33, conf_loss:  12.14, prob_loss:  12.48, total_loss:  31.95\n",
      "epoch: 4 step:  109/250, lr:0.000100, giou_loss:   9.41, conf_loss:  13.10, prob_loss:  24.50, total_loss:  47.01\n",
      "epoch: 4 step:  110/250, lr:0.000100, giou_loss:   7.57, conf_loss:  11.55, prob_loss:  16.85, total_loss:  35.98\n",
      "epoch: 4 step:  111/250, lr:0.000100, giou_loss:   9.41, conf_loss:  13.85, prob_loss:  26.23, total_loss:  49.49\n",
      "epoch: 4 step:  112/250, lr:0.000100, giou_loss:   9.36, conf_loss:  12.20, prob_loss:  22.06, total_loss:  43.62\n",
      "epoch: 4 step:  113/250, lr:0.000100, giou_loss:   3.92, conf_loss:  11.25, prob_loss:  10.63, total_loss:  25.80\n",
      "epoch: 4 step:  114/250, lr:0.000100, giou_loss:   7.10, conf_loss:  12.16, prob_loss:  14.08, total_loss:  33.34\n",
      "epoch: 4 step:  115/250, lr:0.000100, giou_loss:   8.51, conf_loss:  13.32, prob_loss:  23.70, total_loss:  45.54\n",
      "epoch: 4 step:  116/250, lr:0.000100, giou_loss:   6.60, conf_loss:  11.94, prob_loss:  18.88, total_loss:  37.42\n",
      "epoch: 4 step:  117/250, lr:0.000100, giou_loss:   7.06, conf_loss:  11.96, prob_loss:  16.25, total_loss:  35.27\n",
      "epoch: 4 step:  118/250, lr:0.000100, giou_loss:   6.29, conf_loss:  11.76, prob_loss:  15.94, total_loss:  33.99\n",
      "epoch: 4 step:  119/250, lr:0.000100, giou_loss:   7.56, conf_loss:  13.32, prob_loss:  20.96, total_loss:  41.85\n",
      "epoch: 4 step:  120/250, lr:0.000100, giou_loss:   7.58, conf_loss:  12.07, prob_loss:  19.92, total_loss:  39.58\n",
      "epoch: 4 step:  121/250, lr:0.000100, giou_loss:   5.15, conf_loss:  12.17, prob_loss:  12.28, total_loss:  29.60\n",
      "epoch: 4 step:  122/250, lr:0.000100, giou_loss:   6.80, conf_loss:  11.44, prob_loss:  22.53, total_loss:  40.77\n",
      "epoch: 4 step:  123/250, lr:0.000100, giou_loss:   6.00, conf_loss:  11.51, prob_loss:  15.92, total_loss:  33.43\n",
      "epoch: 4 step:  124/250, lr:0.000100, giou_loss:   9.49, conf_loss:  12.56, prob_loss:  21.66, total_loss:  43.72\n",
      "epoch: 4 step:  125/250, lr:0.000100, giou_loss:   7.59, conf_loss:  12.95, prob_loss:  23.15, total_loss:  43.70\n",
      "epoch: 4 step:  126/250, lr:0.000100, giou_loss:   5.52, conf_loss:  11.44, prob_loss:  12.04, total_loss:  29.00\n",
      "epoch: 4 step:  127/250, lr:0.000100, giou_loss:   9.58, conf_loss:  11.85, prob_loss:  37.18, total_loss:  58.61\n",
      "epoch: 4 step:  128/250, lr:0.000100, giou_loss:   9.82, conf_loss:  11.88, prob_loss:  21.74, total_loss:  43.45\n",
      "epoch: 4 step:  129/250, lr:0.000100, giou_loss:   7.09, conf_loss:  11.76, prob_loss:  25.59, total_loss:  44.45\n",
      "epoch: 4 step:  130/250, lr:0.000100, giou_loss:   8.90, conf_loss:  12.68, prob_loss:  29.18, total_loss:  50.76\n",
      "epoch: 4 step:  131/250, lr:0.000100, giou_loss:   8.26, conf_loss:  12.08, prob_loss:  13.26, total_loss:  33.59\n",
      "epoch: 4 step:  132/250, lr:0.000100, giou_loss:   9.07, conf_loss:  13.02, prob_loss:  34.28, total_loss:  56.37\n",
      "epoch: 4 step:  133/250, lr:0.000100, giou_loss:   8.57, conf_loss:  13.76, prob_loss:  29.14, total_loss:  51.47\n",
      "epoch: 4 step:  134/250, lr:0.000100, giou_loss:   8.70, conf_loss:  12.28, prob_loss:  25.70, total_loss:  46.68\n",
      "epoch: 4 step:  135/250, lr:0.000100, giou_loss:   7.54, conf_loss:  12.59, prob_loss:  13.68, total_loss:  33.82\n",
      "epoch: 4 step:  136/250, lr:0.000100, giou_loss:   7.25, conf_loss:  11.68, prob_loss:  23.44, total_loss:  42.38\n",
      "epoch: 4 step:  137/250, lr:0.000100, giou_loss:   8.57, conf_loss:  11.90, prob_loss:  20.89, total_loss:  41.35\n",
      "epoch: 4 step:  138/250, lr:0.000100, giou_loss:   8.99, conf_loss:  12.94, prob_loss:  29.44, total_loss:  51.37\n",
      "epoch: 4 step:  139/250, lr:0.000100, giou_loss:   8.70, conf_loss:  12.27, prob_loss:  19.81, total_loss:  40.78\n",
      "epoch: 4 step:  140/250, lr:0.000100, giou_loss:   6.05, conf_loss:  11.29, prob_loss:  17.62, total_loss:  34.96\n",
      "epoch: 4 step:  141/250, lr:0.000100, giou_loss:   7.57, conf_loss:  11.85, prob_loss:  19.17, total_loss:  38.58\n",
      "epoch: 4 step:  142/250, lr:0.000100, giou_loss:   5.46, conf_loss:  10.78, prob_loss:  14.16, total_loss:  30.39\n",
      "epoch: 4 step:  143/250, lr:0.000100, giou_loss:   9.46, conf_loss:  12.90, prob_loss:  25.29, total_loss:  47.65\n",
      "epoch: 4 step:  144/250, lr:0.000100, giou_loss:   8.54, conf_loss:  13.09, prob_loss:  25.54, total_loss:  47.17\n",
      "epoch: 4 step:  145/250, lr:0.000100, giou_loss:   9.31, conf_loss:  12.64, prob_loss:  24.16, total_loss:  46.12\n",
      "epoch: 4 step:  146/250, lr:0.000100, giou_loss:   7.36, conf_loss:  11.44, prob_loss:  24.52, total_loss:  43.31\n",
      "epoch: 4 step:  147/250, lr:0.000100, giou_loss:   9.02, conf_loss:  11.98, prob_loss:  24.93, total_loss:  45.92\n",
      "epoch: 4 step:  148/250, lr:0.000100, giou_loss:   6.37, conf_loss:  11.28, prob_loss:  17.34, total_loss:  35.00\n",
      "epoch: 4 step:  149/250, lr:0.000100, giou_loss:   6.33, conf_loss:  11.36, prob_loss:  11.96, total_loss:  29.66\n",
      "epoch: 4 step:  150/250, lr:0.000100, giou_loss:   6.46, conf_loss:  10.57, prob_loss:  21.18, total_loss:  38.21\n",
      "epoch: 4 step:  151/250, lr:0.000100, giou_loss:   8.61, conf_loss:  11.79, prob_loss:  19.25, total_loss:  39.65\n",
      "epoch: 4 step:  152/250, lr:0.000100, giou_loss:   7.35, conf_loss:  11.32, prob_loss:  14.43, total_loss:  33.10\n",
      "epoch: 4 step:  153/250, lr:0.000100, giou_loss:   8.11, conf_loss:  12.89, prob_loss:  19.65, total_loss:  40.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step:  154/250, lr:0.000100, giou_loss:   6.88, conf_loss:  11.06, prob_loss:  21.72, total_loss:  39.66\n",
      "epoch: 4 step:  155/250, lr:0.000100, giou_loss:   7.76, conf_loss:  11.47, prob_loss:  24.41, total_loss:  43.64\n",
      "epoch: 4 step:  156/250, lr:0.000100, giou_loss:   8.01, conf_loss:  11.64, prob_loss:  16.67, total_loss:  36.32\n",
      "epoch: 4 step:  157/250, lr:0.000100, giou_loss:   6.89, conf_loss:  11.21, prob_loss:  10.28, total_loss:  28.39\n",
      "epoch: 4 step:  158/250, lr:0.000100, giou_loss:   7.05, conf_loss:  11.55, prob_loss:  12.86, total_loss:  31.46\n",
      "epoch: 4 step:  159/250, lr:0.000100, giou_loss:   5.35, conf_loss:  10.68, prob_loss:   9.15, total_loss:  25.17\n",
      "epoch: 4 step:  160/250, lr:0.000100, giou_loss:   5.44, conf_loss:  11.04, prob_loss:  15.18, total_loss:  31.67\n",
      "epoch: 4 step:  161/250, lr:0.000100, giou_loss:   6.99, conf_loss:  12.15, prob_loss:  17.60, total_loss:  36.74\n",
      "epoch: 4 step:  162/250, lr:0.000100, giou_loss:   6.53, conf_loss:  10.48, prob_loss:  25.95, total_loss:  42.96\n",
      "epoch: 4 step:  163/250, lr:0.000100, giou_loss:   5.64, conf_loss:  11.33, prob_loss:  14.91, total_loss:  31.88\n",
      "epoch: 4 step:  164/250, lr:0.000100, giou_loss:   8.84, conf_loss:  11.85, prob_loss:  23.13, total_loss:  43.82\n",
      "epoch: 4 step:  165/250, lr:0.000100, giou_loss:   6.67, conf_loss:  10.95, prob_loss:  16.04, total_loss:  33.65\n",
      "epoch: 4 step:  166/250, lr:0.000100, giou_loss:   6.81, conf_loss:  11.35, prob_loss:  20.96, total_loss:  39.12\n",
      "epoch: 4 step:  167/250, lr:0.000100, giou_loss:   8.59, conf_loss:  11.80, prob_loss:  19.09, total_loss:  39.48\n",
      "epoch: 4 step:  168/250, lr:0.000100, giou_loss:   7.13, conf_loss:  11.89, prob_loss:  21.91, total_loss:  40.93\n",
      "epoch: 4 step:  169/250, lr:0.000100, giou_loss:   8.07, conf_loss:  11.14, prob_loss:  18.68, total_loss:  37.89\n",
      "epoch: 4 step:  170/250, lr:0.000100, giou_loss:   7.58, conf_loss:  11.82, prob_loss:  21.40, total_loss:  40.79\n",
      "epoch: 4 step:  171/250, lr:0.000100, giou_loss:   8.50, conf_loss:  11.79, prob_loss:  22.95, total_loss:  43.24\n",
      "epoch: 4 step:  172/250, lr:0.000100, giou_loss:   6.36, conf_loss:  11.00, prob_loss:  16.48, total_loss:  33.84\n",
      "epoch: 4 step:  173/250, lr:0.000100, giou_loss:   5.53, conf_loss:  10.79, prob_loss:  17.04, total_loss:  33.36\n",
      "epoch: 4 step:  174/250, lr:0.000100, giou_loss:   5.32, conf_loss:  10.67, prob_loss:  16.66, total_loss:  32.65\n",
      "epoch: 4 step:  175/250, lr:0.000100, giou_loss:   6.84, conf_loss:  11.16, prob_loss:  21.89, total_loss:  39.89\n",
      "epoch: 4 step:  176/250, lr:0.000100, giou_loss:   8.14, conf_loss:  11.57, prob_loss:  15.09, total_loss:  34.79\n",
      "epoch: 4 step:  177/250, lr:0.000100, giou_loss:  10.11, conf_loss:  11.53, prob_loss:  23.15, total_loss:  44.78\n",
      "epoch: 4 step:  178/250, lr:0.000100, giou_loss:   6.84, conf_loss:  10.73, prob_loss:  14.29, total_loss:  31.86\n",
      "epoch: 4 step:  179/250, lr:0.000100, giou_loss:   9.69, conf_loss:  12.71, prob_loss:  22.76, total_loss:  45.16\n",
      "epoch: 4 step:  180/250, lr:0.000100, giou_loss:   5.89, conf_loss:  10.37, prob_loss:  14.53, total_loss:  30.79\n",
      "epoch: 4 step:  181/250, lr:0.000100, giou_loss:   7.09, conf_loss:  10.59, prob_loss:  17.57, total_loss:  35.25\n",
      "epoch: 4 step:  182/250, lr:0.000100, giou_loss:   8.24, conf_loss:  11.23, prob_loss:  18.35, total_loss:  37.82\n",
      "epoch: 4 step:  183/250, lr:0.000100, giou_loss:   7.02, conf_loss:  10.66, prob_loss:  14.29, total_loss:  31.97\n",
      "epoch: 4 step:  184/250, lr:0.000100, giou_loss:   7.25, conf_loss:  10.52, prob_loss:  14.58, total_loss:  32.35\n",
      "epoch: 4 step:  185/250, lr:0.000100, giou_loss:   7.97, conf_loss:  11.40, prob_loss:  15.59, total_loss:  34.96\n",
      "epoch: 4 step:  186/250, lr:0.000100, giou_loss:   8.34, conf_loss:  10.59, prob_loss:  21.64, total_loss:  40.57\n",
      "epoch: 4 step:  187/250, lr:0.000100, giou_loss:  10.93, conf_loss:  12.38, prob_loss:  24.59, total_loss:  47.90\n",
      "epoch: 4 step:  188/250, lr:0.000100, giou_loss:   9.08, conf_loss:  12.04, prob_loss:  19.55, total_loss:  40.67\n",
      "epoch: 4 step:  189/250, lr:0.000100, giou_loss:   9.84, conf_loss:  11.14, prob_loss:  22.66, total_loss:  43.65\n",
      "epoch: 4 step:  190/250, lr:0.000100, giou_loss:   7.57, conf_loss:  11.91, prob_loss:  17.81, total_loss:  37.29\n",
      "epoch: 4 step:  191/250, lr:0.000100, giou_loss:   7.31, conf_loss:  12.05, prob_loss:  14.05, total_loss:  33.41\n",
      "epoch: 4 step:  192/250, lr:0.000100, giou_loss:   6.70, conf_loss:  10.49, prob_loss:  10.39, total_loss:  27.58\n",
      "epoch: 4 step:  193/250, lr:0.000100, giou_loss:   7.71, conf_loss:  10.49, prob_loss:  19.49, total_loss:  37.70\n",
      "epoch: 4 step:  194/250, lr:0.000100, giou_loss:   6.71, conf_loss:  10.45, prob_loss:  16.41, total_loss:  33.56\n",
      "epoch: 4 step:  195/250, lr:0.000100, giou_loss:   9.59, conf_loss:  12.43, prob_loss:  27.21, total_loss:  49.23\n",
      "epoch: 4 step:  196/250, lr:0.000100, giou_loss:   6.69, conf_loss:  10.63, prob_loss:  12.68, total_loss:  30.00\n",
      "epoch: 4 step:  197/250, lr:0.000100, giou_loss:   7.95, conf_loss:  11.76, prob_loss:  19.02, total_loss:  38.73\n",
      "epoch: 4 step:  198/250, lr:0.000100, giou_loss:   7.49, conf_loss:  10.65, prob_loss:  19.35, total_loss:  37.48\n",
      "epoch: 4 step:  199/250, lr:0.000100, giou_loss:   5.67, conf_loss:   9.64, prob_loss:   9.45, total_loss:  24.75\n",
      "epoch: 4 step:  200/250, lr:0.000100, giou_loss:   8.49, conf_loss:  11.01, prob_loss:  20.05, total_loss:  39.55\n",
      "epoch: 4 step:  201/250, lr:0.000100, giou_loss:   6.80, conf_loss:   9.91, prob_loss:  11.07, total_loss:  27.78\n",
      "epoch: 4 step:  202/250, lr:0.000100, giou_loss:  11.39, conf_loss:  14.72, prob_loss:  27.59, total_loss:  53.70\n",
      "epoch: 4 step:  203/250, lr:0.000100, giou_loss:   7.47, conf_loss:  10.93, prob_loss:  19.06, total_loss:  37.45\n",
      "epoch: 4 step:  204/250, lr:0.000100, giou_loss:   6.57, conf_loss:  10.40, prob_loss:  13.37, total_loss:  30.34\n",
      "epoch: 4 step:  205/250, lr:0.000100, giou_loss:   7.72, conf_loss:  10.98, prob_loss:  16.21, total_loss:  34.91\n",
      "epoch: 4 step:  206/250, lr:0.000100, giou_loss:   8.32, conf_loss:  11.70, prob_loss:  17.13, total_loss:  37.15\n",
      "epoch: 4 step:  207/250, lr:0.000100, giou_loss:   8.97, conf_loss:  11.19, prob_loss:  16.87, total_loss:  37.04\n",
      "epoch: 4 step:  208/250, lr:0.000100, giou_loss:   6.91, conf_loss:  10.14, prob_loss:  16.10, total_loss:  33.15\n",
      "epoch: 4 step:  209/250, lr:0.000100, giou_loss:   7.52, conf_loss:  10.05, prob_loss:  19.61, total_loss:  37.19\n",
      "epoch: 4 step:  210/250, lr:0.000100, giou_loss:   7.27, conf_loss:  10.15, prob_loss:  12.35, total_loss:  29.76\n",
      "epoch: 4 step:  211/250, lr:0.000100, giou_loss:   8.07, conf_loss:  10.79, prob_loss:  21.23, total_loss:  40.10\n",
      "epoch: 4 step:  212/250, lr:0.000100, giou_loss:   5.76, conf_loss:  10.45, prob_loss:  19.45, total_loss:  35.65\n",
      "epoch: 4 step:  213/250, lr:0.000100, giou_loss:   6.51, conf_loss:  10.46, prob_loss:  15.48, total_loss:  32.45\n",
      "epoch: 4 step:  214/250, lr:0.000100, giou_loss:   6.20, conf_loss:  10.30, prob_loss:  20.78, total_loss:  37.28\n",
      "epoch: 4 step:  215/250, lr:0.000100, giou_loss:   7.87, conf_loss:  10.10, prob_loss:  21.00, total_loss:  38.97\n",
      "epoch: 4 step:  216/250, lr:0.000100, giou_loss:  11.04, conf_loss:  13.40, prob_loss:  26.92, total_loss:  51.37\n",
      "epoch: 4 step:  217/250, lr:0.000100, giou_loss:   5.67, conf_loss:   9.65, prob_loss:  13.92, total_loss:  29.25\n",
      "epoch: 4 step:  218/250, lr:0.000100, giou_loss:   8.17, conf_loss:  10.50, prob_loss:  26.51, total_loss:  45.18\n",
      "epoch: 4 step:  219/250, lr:0.000100, giou_loss:   4.74, conf_loss:   9.29, prob_loss:  11.32, total_loss:  25.35\n",
      "epoch: 4 step:  220/250, lr:0.000100, giou_loss:   5.89, conf_loss:   9.54, prob_loss:  12.77, total_loss:  28.19\n",
      "epoch: 4 step:  221/250, lr:0.000100, giou_loss:   6.08, conf_loss:   9.56, prob_loss:  11.49, total_loss:  27.13\n",
      "epoch: 4 step:  222/250, lr:0.000100, giou_loss:   6.05, conf_loss:   9.85, prob_loss:  15.42, total_loss:  31.32\n",
      "epoch: 4 step:  223/250, lr:0.000100, giou_loss:  10.80, conf_loss:  12.28, prob_loss:  25.52, total_loss:  48.60\n",
      "epoch: 4 step:  224/250, lr:0.000100, giou_loss:   7.63, conf_loss:  11.35, prob_loss:  20.38, total_loss:  39.36\n",
      "epoch: 4 step:  225/250, lr:0.000100, giou_loss:   6.75, conf_loss:  10.66, prob_loss:  18.03, total_loss:  35.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step:  226/250, lr:0.000100, giou_loss:   7.58, conf_loss:  12.51, prob_loss:  24.17, total_loss:  44.26\n",
      "epoch: 4 step:  227/250, lr:0.000100, giou_loss:   5.85, conf_loss:  13.04, prob_loss:  13.23, total_loss:  32.12\n",
      "epoch: 4 step:  228/250, lr:0.000100, giou_loss:   8.29, conf_loss:   9.93, prob_loss:  15.54, total_loss:  33.76\n",
      "epoch: 4 step:  229/250, lr:0.000100, giou_loss:   6.13, conf_loss:   9.60, prob_loss:  16.01, total_loss:  31.74\n",
      "epoch: 4 step:  230/250, lr:0.000100, giou_loss:   7.59, conf_loss:  11.85, prob_loss:  22.26, total_loss:  41.71\n",
      "epoch: 4 step:  231/250, lr:0.000100, giou_loss:   7.27, conf_loss:  11.82, prob_loss:  24.18, total_loss:  43.26\n",
      "epoch: 4 step:  232/250, lr:0.000100, giou_loss:   6.64, conf_loss:  10.14, prob_loss:  18.64, total_loss:  35.41\n",
      "epoch: 4 step:  233/250, lr:0.000100, giou_loss:   7.14, conf_loss:  10.83, prob_loss:  14.71, total_loss:  32.68\n",
      "epoch: 4 step:  234/250, lr:0.000100, giou_loss:   8.87, conf_loss:  11.98, prob_loss:  19.40, total_loss:  40.26\n",
      "epoch: 4 step:  235/250, lr:0.000100, giou_loss:   8.61, conf_loss:  11.37, prob_loss:  18.67, total_loss:  38.65\n",
      "epoch: 4 step:  236/250, lr:0.000100, giou_loss:   7.07, conf_loss:  10.53, prob_loss:  14.76, total_loss:  32.35\n",
      "epoch: 4 step:  237/250, lr:0.000100, giou_loss:   7.71, conf_loss:  10.40, prob_loss:  14.18, total_loss:  32.29\n",
      "epoch: 4 step:  238/250, lr:0.000100, giou_loss:  10.20, conf_loss:  11.33, prob_loss:  21.53, total_loss:  43.06\n",
      "epoch: 4 step:  239/250, lr:0.000100, giou_loss:   7.00, conf_loss:  10.88, prob_loss:  21.74, total_loss:  39.62\n",
      "epoch: 4 step:  240/250, lr:0.000100, giou_loss:   9.89, conf_loss:  11.06, prob_loss:  33.34, total_loss:  54.29\n",
      "epoch: 4 step:  241/250, lr:0.000100, giou_loss:   9.11, conf_loss:  10.04, prob_loss:  16.28, total_loss:  35.43\n",
      "epoch: 4 step:  242/250, lr:0.000100, giou_loss:   8.39, conf_loss:  11.44, prob_loss:  24.36, total_loss:  44.20\n",
      "epoch: 4 step:  243/250, lr:0.000100, giou_loss:   7.04, conf_loss:   9.78, prob_loss:  16.45, total_loss:  33.27\n",
      "epoch: 4 step:  244/250, lr:0.000100, giou_loss:   6.19, conf_loss:   9.87, prob_loss:  25.05, total_loss:  41.11\n",
      "epoch: 4 step:  245/250, lr:0.000100, giou_loss:   5.94, conf_loss:   9.31, prob_loss:  11.83, total_loss:  27.08\n",
      "epoch: 4 step:  246/250, lr:0.000100, giou_loss:   6.60, conf_loss:  10.60, prob_loss:  15.65, total_loss:  32.86\n",
      "epoch: 4 step:  247/250, lr:0.000100, giou_loss:   8.11, conf_loss:  10.01, prob_loss:  20.78, total_loss:  38.90\n",
      "epoch: 4 step:  248/250, lr:0.000100, giou_loss:   7.38, conf_loss:   9.69, prob_loss:  17.19, total_loss:  34.27\n",
      "epoch: 4 step:  249/250, lr:0.000100, giou_loss:   7.59, conf_loss:   9.61, prob_loss:  25.61, total_loss:  42.81\n",
      "epoch: 4 step:    0/250, lr:0.000100, giou_loss:   9.47, conf_loss:  10.86, prob_loss:  28.08, total_loss:  48.40\n",
      "epoch: 4 step:    1/250, lr:0.000100, giou_loss:   8.32, conf_loss:  10.44, prob_loss:  23.48, total_loss:  42.24\n",
      "\n",
      "\n",
      "giou_val_loss:  11.96, conf_val_loss:  66.85, prob_val_loss:  56.66, total_val_loss: 135.47\n",
      "\n",
      "\n",
      "epoch: 5 step:    2/250, lr:0.000100, giou_loss:   8.26, conf_loss:   9.79, prob_loss:  16.39, total_loss:  34.44\n",
      "epoch: 5 step:    3/250, lr:0.000100, giou_loss:   6.55, conf_loss:   9.94, prob_loss:  16.74, total_loss:  33.23\n",
      "epoch: 5 step:    4/250, lr:0.000100, giou_loss:   7.48, conf_loss:   9.75, prob_loss:  13.63, total_loss:  30.86\n",
      "epoch: 5 step:    5/250, lr:0.000100, giou_loss:   7.51, conf_loss:  10.20, prob_loss:  17.29, total_loss:  35.00\n",
      "epoch: 5 step:    6/250, lr:0.000100, giou_loss:   6.52, conf_loss:  10.64, prob_loss:  12.98, total_loss:  30.14\n",
      "epoch: 5 step:    7/250, lr:0.000100, giou_loss:   8.92, conf_loss:  11.21, prob_loss:  17.06, total_loss:  37.20\n",
      "epoch: 5 step:    8/250, lr:0.000100, giou_loss:   6.91, conf_loss:   9.45, prob_loss:  20.63, total_loss:  36.99\n",
      "epoch: 5 step:    9/250, lr:0.000100, giou_loss:   7.00, conf_loss:   9.83, prob_loss:  17.39, total_loss:  34.23\n",
      "epoch: 5 step:   10/250, lr:0.000100, giou_loss:   7.11, conf_loss:   9.45, prob_loss:  14.48, total_loss:  31.04\n",
      "epoch: 5 step:   11/250, lr:0.000100, giou_loss:   8.02, conf_loss:  12.14, prob_loss:  20.88, total_loss:  41.05\n",
      "epoch: 5 step:   12/250, lr:0.000100, giou_loss:   4.38, conf_loss:  10.60, prob_loss:  12.08, total_loss:  27.06\n",
      "epoch: 5 step:   13/250, lr:0.000100, giou_loss:   5.02, conf_loss:   9.45, prob_loss:  11.39, total_loss:  25.86\n",
      "epoch: 5 step:   14/250, lr:0.000100, giou_loss:   8.26, conf_loss:   9.43, prob_loss:  16.15, total_loss:  33.84\n",
      "epoch: 5 step:   15/250, lr:0.000100, giou_loss:  10.20, conf_loss:   9.99, prob_loss:  16.35, total_loss:  36.55\n",
      "epoch: 5 step:   16/250, lr:0.000100, giou_loss:  10.28, conf_loss:  11.22, prob_loss:  17.96, total_loss:  39.46\n",
      "epoch: 5 step:   17/250, lr:0.000100, giou_loss:   6.77, conf_loss:   9.49, prob_loss:  12.28, total_loss:  28.54\n",
      "epoch: 5 step:   18/250, lr:0.000100, giou_loss:   9.23, conf_loss:  11.47, prob_loss:  18.22, total_loss:  38.92\n",
      "epoch: 5 step:   19/250, lr:0.000100, giou_loss:   6.98, conf_loss:   9.54, prob_loss:   9.66, total_loss:  26.18\n",
      "epoch: 5 step:   20/250, lr:0.000100, giou_loss:   7.49, conf_loss:   9.80, prob_loss:  13.98, total_loss:  31.27\n",
      "epoch: 5 step:   21/250, lr:0.000100, giou_loss:   4.96, conf_loss:   9.45, prob_loss:   8.95, total_loss:  23.36\n",
      "epoch: 5 step:   22/250, lr:0.000100, giou_loss:   7.86, conf_loss:  10.53, prob_loss:  20.69, total_loss:  39.08\n",
      "epoch: 5 step:   23/250, lr:0.000100, giou_loss:   7.28, conf_loss:  10.24, prob_loss:  18.24, total_loss:  35.76\n",
      "epoch: 5 step:   24/250, lr:0.000100, giou_loss:   7.60, conf_loss:   9.18, prob_loss:  10.94, total_loss:  27.72\n",
      "epoch: 5 step:   25/250, lr:0.000100, giou_loss:   8.41, conf_loss:   9.71, prob_loss:  15.38, total_loss:  33.50\n",
      "epoch: 5 step:   26/250, lr:0.000100, giou_loss:   6.92, conf_loss:   9.63, prob_loss:   9.85, total_loss:  26.40\n",
      "epoch: 5 step:   27/250, lr:0.000100, giou_loss:   5.95, conf_loss:  12.55, prob_loss:  13.20, total_loss:  31.70\n",
      "epoch: 5 step:   28/250, lr:0.000100, giou_loss:   7.94, conf_loss:   9.45, prob_loss:  12.09, total_loss:  29.48\n",
      "epoch: 5 step:   29/250, lr:0.000100, giou_loss:   7.36, conf_loss:   9.27, prob_loss:  14.32, total_loss:  30.96\n",
      "epoch: 5 step:   30/250, lr:0.000100, giou_loss:   8.64, conf_loss:  11.22, prob_loss:  20.88, total_loss:  40.74\n",
      "epoch: 5 step:   31/250, lr:0.000100, giou_loss:   7.26, conf_loss:  10.56, prob_loss:  12.21, total_loss:  30.03\n",
      "epoch: 5 step:   32/250, lr:0.000100, giou_loss:  10.64, conf_loss:  10.69, prob_loss:  22.57, total_loss:  43.89\n",
      "epoch: 5 step:   33/250, lr:0.000100, giou_loss:   7.59, conf_loss:   9.97, prob_loss:   5.68, total_loss:  23.25\n",
      "epoch: 5 step:   34/250, lr:0.000100, giou_loss:   6.84, conf_loss:   9.96, prob_loss:  13.50, total_loss:  30.30\n",
      "epoch: 5 step:   35/250, lr:0.000100, giou_loss:   5.72, conf_loss:   9.04, prob_loss:  10.33, total_loss:  25.08\n",
      "epoch: 5 step:   36/250, lr:0.000100, giou_loss:   5.91, conf_loss:   8.81, prob_loss:  11.55, total_loss:  26.27\n",
      "epoch: 5 step:   37/250, lr:0.000100, giou_loss:   7.40, conf_loss:   9.39, prob_loss:  15.83, total_loss:  32.62\n",
      "epoch: 5 step:   38/250, lr:0.000100, giou_loss:   5.89, conf_loss:   8.73, prob_loss:   7.79, total_loss:  22.41\n",
      "epoch: 5 step:   39/250, lr:0.000100, giou_loss:   6.65, conf_loss:  10.68, prob_loss:  12.61, total_loss:  29.95\n",
      "epoch: 5 step:   40/250, lr:0.000100, giou_loss:   6.01, conf_loss:   9.67, prob_loss:  11.32, total_loss:  27.00\n",
      "epoch: 5 step:   41/250, lr:0.000100, giou_loss:   6.15, conf_loss:   9.34, prob_loss:  12.51, total_loss:  28.00\n",
      "epoch: 5 step:   42/250, lr:0.000100, giou_loss:   5.22, conf_loss:   8.97, prob_loss:   8.50, total_loss:  22.69\n",
      "epoch: 5 step:   43/250, lr:0.000100, giou_loss:   7.76, conf_loss:  10.18, prob_loss:  21.55, total_loss:  39.48\n",
      "epoch: 5 step:   44/250, lr:0.000100, giou_loss:   5.47, conf_loss:   9.05, prob_loss:   8.88, total_loss:  23.41\n",
      "epoch: 5 step:   45/250, lr:0.000100, giou_loss:   5.34, conf_loss:   8.56, prob_loss:  15.24, total_loss:  29.13\n",
      "epoch: 5 step:   46/250, lr:0.000100, giou_loss:   5.46, conf_loss:   8.75, prob_loss:  13.37, total_loss:  27.57\n",
      "epoch: 5 step:   47/250, lr:0.000100, giou_loss:  10.56, conf_loss:  10.75, prob_loss:  23.04, total_loss:  44.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step:   48/250, lr:0.000100, giou_loss:   7.56, conf_loss:   9.01, prob_loss:  16.02, total_loss:  32.58\n",
      "epoch: 5 step:   49/250, lr:0.000100, giou_loss:   6.63, conf_loss:   8.91, prob_loss:  14.57, total_loss:  30.11\n",
      "epoch: 5 step:   50/250, lr:0.000100, giou_loss:   8.27, conf_loss:   9.76, prob_loss:  17.41, total_loss:  35.44\n",
      "epoch: 5 step:   51/250, lr:0.000100, giou_loss:   5.90, conf_loss:   9.40, prob_loss:  13.86, total_loss:  29.16\n",
      "epoch: 5 step:   52/250, lr:0.000100, giou_loss:   5.89, conf_loss:   9.40, prob_loss:  12.33, total_loss:  27.61\n",
      "epoch: 5 step:   53/250, lr:0.000100, giou_loss:   8.52, conf_loss:   9.62, prob_loss:  16.78, total_loss:  34.92\n",
      "epoch: 5 step:   54/250, lr:0.000100, giou_loss:   6.35, conf_loss:  10.18, prob_loss:  12.35, total_loss:  28.88\n",
      "epoch: 5 step:   55/250, lr:0.000100, giou_loss:   7.07, conf_loss:   8.97, prob_loss:  15.89, total_loss:  31.93\n",
      "epoch: 5 step:   56/250, lr:0.000100, giou_loss:   5.10, conf_loss:   8.45, prob_loss:  10.56, total_loss:  24.12\n",
      "epoch: 5 step:   57/250, lr:0.000100, giou_loss:   7.23, conf_loss:   9.55, prob_loss:  17.26, total_loss:  34.04\n",
      "epoch: 5 step:   58/250, lr:0.000100, giou_loss:   5.51, conf_loss:   9.28, prob_loss:  10.94, total_loss:  25.73\n",
      "epoch: 5 step:   59/250, lr:0.000100, giou_loss:   8.59, conf_loss:   9.31, prob_loss:  13.54, total_loss:  31.44\n",
      "epoch: 5 step:   60/250, lr:0.000100, giou_loss:   6.41, conf_loss:   8.83, prob_loss:  13.55, total_loss:  28.79\n",
      "epoch: 5 step:   61/250, lr:0.000100, giou_loss:   8.07, conf_loss:   9.31, prob_loss:  12.17, total_loss:  29.56\n",
      "epoch: 5 step:   62/250, lr:0.000100, giou_loss:   8.35, conf_loss:   9.56, prob_loss:  14.03, total_loss:  31.95\n",
      "epoch: 5 step:   63/250, lr:0.000100, giou_loss:   5.57, conf_loss:   8.96, prob_loss:  11.81, total_loss:  26.34\n",
      "epoch: 5 step:   64/250, lr:0.000100, giou_loss:   8.25, conf_loss:  10.08, prob_loss:  19.15, total_loss:  37.47\n",
      "epoch: 5 step:   65/250, lr:0.000100, giou_loss:   8.10, conf_loss:   9.46, prob_loss:  13.75, total_loss:  31.30\n",
      "epoch: 5 step:   66/250, lr:0.000100, giou_loss:   6.66, conf_loss:   8.69, prob_loss:  10.38, total_loss:  25.74\n",
      "epoch: 5 step:   67/250, lr:0.000100, giou_loss:   7.04, conf_loss:   8.76, prob_loss:  12.27, total_loss:  28.07\n",
      "epoch: 5 step:   68/250, lr:0.000100, giou_loss:   6.31, conf_loss:   8.68, prob_loss:  11.99, total_loss:  26.98\n",
      "epoch: 5 step:   69/250, lr:0.000100, giou_loss:   6.57, conf_loss:   8.10, prob_loss:   9.51, total_loss:  24.18\n",
      "epoch: 5 step:   70/250, lr:0.000100, giou_loss:   7.47, conf_loss:   9.01, prob_loss:  17.89, total_loss:  34.38\n",
      "epoch: 5 step:   71/250, lr:0.000100, giou_loss:   9.54, conf_loss:  11.04, prob_loss:  20.09, total_loss:  40.67\n",
      "epoch: 5 step:   72/250, lr:0.000100, giou_loss:   8.23, conf_loss:   8.81, prob_loss:  13.66, total_loss:  30.70\n",
      "epoch: 5 step:   73/250, lr:0.000100, giou_loss:   5.94, conf_loss:   9.13, prob_loss:  17.19, total_loss:  32.26\n",
      "epoch: 5 step:   74/250, lr:0.000100, giou_loss:   9.12, conf_loss:  10.11, prob_loss:  14.70, total_loss:  33.93\n",
      "epoch: 5 step:   75/250, lr:0.000100, giou_loss:   6.23, conf_loss:   8.55, prob_loss:  10.22, total_loss:  25.00\n",
      "epoch: 5 step:   76/250, lr:0.000100, giou_loss:   6.18, conf_loss:   8.80, prob_loss:   9.84, total_loss:  24.83\n",
      "epoch: 5 step:   77/250, lr:0.000100, giou_loss:   7.45, conf_loss:   9.83, prob_loss:  12.70, total_loss:  29.98\n",
      "epoch: 5 step:   78/250, lr:0.000100, giou_loss:   6.11, conf_loss:   8.93, prob_loss:   9.40, total_loss:  24.44\n",
      "epoch: 5 step:   79/250, lr:0.000100, giou_loss:   8.06, conf_loss:   8.46, prob_loss:  14.40, total_loss:  30.91\n",
      "epoch: 5 step:   80/250, lr:0.000100, giou_loss:   6.36, conf_loss:   8.41, prob_loss:   9.27, total_loss:  24.03\n",
      "epoch: 5 step:   81/250, lr:0.000100, giou_loss:   7.53, conf_loss:   8.30, prob_loss:  12.73, total_loss:  28.56\n",
      "epoch: 5 step:   82/250, lr:0.000100, giou_loss:   7.47, conf_loss:   8.69, prob_loss:   9.39, total_loss:  25.55\n",
      "epoch: 5 step:   83/250, lr:0.000100, giou_loss:   9.83, conf_loss:  10.28, prob_loss:  18.77, total_loss:  38.88\n",
      "epoch: 5 step:   84/250, lr:0.000100, giou_loss:   6.47, conf_loss:   8.11, prob_loss:  10.99, total_loss:  25.57\n",
      "epoch: 5 step:   85/250, lr:0.000100, giou_loss:   7.67, conf_loss:   8.36, prob_loss:  14.71, total_loss:  30.75\n",
      "epoch: 5 step:   86/250, lr:0.000100, giou_loss:   5.88, conf_loss:   8.30, prob_loss:   5.87, total_loss:  20.05\n",
      "epoch: 5 step:   87/250, lr:0.000100, giou_loss:   5.66, conf_loss:   8.10, prob_loss:   7.54, total_loss:  21.30\n",
      "epoch: 5 step:   88/250, lr:0.000100, giou_loss:   7.44, conf_loss:   8.33, prob_loss:  18.00, total_loss:  33.78\n",
      "epoch: 5 step:   89/250, lr:0.000100, giou_loss:   7.58, conf_loss:   8.41, prob_loss:  15.54, total_loss:  31.53\n",
      "epoch: 5 step:   90/250, lr:0.000100, giou_loss:   4.96, conf_loss:   8.99, prob_loss:   9.71, total_loss:  23.66\n",
      "epoch: 5 step:   91/250, lr:0.000100, giou_loss:   5.95, conf_loss:   8.27, prob_loss:   9.11, total_loss:  23.33\n",
      "epoch: 5 step:   92/250, lr:0.000100, giou_loss:   6.72, conf_loss:   8.03, prob_loss:  12.56, total_loss:  27.32\n",
      "epoch: 5 step:   93/250, lr:0.000100, giou_loss:   7.94, conf_loss:   8.53, prob_loss:   9.58, total_loss:  26.05\n",
      "epoch: 5 step:   94/250, lr:0.000100, giou_loss:   6.34, conf_loss:   8.27, prob_loss:  18.39, total_loss:  33.00\n",
      "epoch: 5 step:   95/250, lr:0.000100, giou_loss:   6.91, conf_loss:  10.17, prob_loss:  14.86, total_loss:  31.94\n",
      "epoch: 5 step:   96/250, lr:0.000100, giou_loss:   7.19, conf_loss:   8.53, prob_loss:  16.10, total_loss:  31.82\n",
      "epoch: 5 step:   97/250, lr:0.000100, giou_loss:   6.78, conf_loss:   9.55, prob_loss:  13.30, total_loss:  29.62\n",
      "epoch: 5 step:   98/250, lr:0.000100, giou_loss:   5.93, conf_loss:   7.94, prob_loss:   6.92, total_loss:  20.79\n",
      "epoch: 5 step:   99/250, lr:0.000100, giou_loss:   5.05, conf_loss:   7.94, prob_loss:  18.29, total_loss:  31.29\n",
      "epoch: 5 step:  100/250, lr:0.000100, giou_loss:   6.89, conf_loss:   7.84, prob_loss:  11.09, total_loss:  25.82\n",
      "epoch: 5 step:  101/250, lr:0.000100, giou_loss:   5.39, conf_loss:   7.78, prob_loss:  14.80, total_loss:  27.98\n",
      "epoch: 5 step:  102/250, lr:0.000100, giou_loss:   8.25, conf_loss:   8.76, prob_loss:   8.67, total_loss:  25.68\n",
      "epoch: 5 step:  103/250, lr:0.000100, giou_loss:   5.84, conf_loss:   7.89, prob_loss:  11.77, total_loss:  25.50\n",
      "epoch: 5 step:  104/250, lr:0.000100, giou_loss:   7.83, conf_loss:   8.71, prob_loss:  12.84, total_loss:  29.38\n",
      "epoch: 5 step:  105/250, lr:0.000100, giou_loss:   7.64, conf_loss:   8.22, prob_loss:  11.29, total_loss:  27.15\n",
      "epoch: 5 step:  106/250, lr:0.000100, giou_loss:   6.40, conf_loss:   8.50, prob_loss:   7.91, total_loss:  22.81\n",
      "epoch: 5 step:  107/250, lr:0.000100, giou_loss:   7.53, conf_loss:   9.80, prob_loss:  14.38, total_loss:  31.71\n",
      "epoch: 5 step:  108/250, lr:0.000100, giou_loss:   6.49, conf_loss:   8.22, prob_loss:   7.12, total_loss:  21.82\n",
      "epoch: 5 step:  109/250, lr:0.000100, giou_loss:   7.30, conf_loss:   8.84, prob_loss:  14.99, total_loss:  31.12\n",
      "epoch: 5 step:  110/250, lr:0.000100, giou_loss:   9.14, conf_loss:   8.99, prob_loss:  12.27, total_loss:  30.41\n",
      "epoch: 5 step:  111/250, lr:0.000100, giou_loss:   6.84, conf_loss:   9.97, prob_loss:  14.05, total_loss:  30.86\n",
      "epoch: 5 step:  112/250, lr:0.000100, giou_loss:   5.84, conf_loss:   7.64, prob_loss:   5.36, total_loss:  18.85\n",
      "epoch: 5 step:  113/250, lr:0.000100, giou_loss:   6.01, conf_loss:   7.88, prob_loss:   9.14, total_loss:  23.03\n",
      "epoch: 5 step:  114/250, lr:0.000100, giou_loss:   5.93, conf_loss:   9.14, prob_loss:   7.96, total_loss:  23.02\n",
      "epoch: 5 step:  115/250, lr:0.000100, giou_loss:   9.74, conf_loss:   8.56, prob_loss:  16.58, total_loss:  34.87\n",
      "epoch: 5 step:  116/250, lr:0.000100, giou_loss:   8.14, conf_loss:   8.14, prob_loss:  13.10, total_loss:  29.37\n",
      "epoch: 5 step:  117/250, lr:0.000100, giou_loss:   7.11, conf_loss:   9.17, prob_loss:  15.72, total_loss:  32.00\n",
      "epoch: 5 step:  118/250, lr:0.000100, giou_loss:   7.55, conf_loss:   8.55, prob_loss:  10.21, total_loss:  26.30\n",
      "epoch: 5 step:  119/250, lr:0.000100, giou_loss:   7.63, conf_loss:   8.52, prob_loss:  16.94, total_loss:  33.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step:  120/250, lr:0.000100, giou_loss:   7.00, conf_loss:   8.24, prob_loss:  14.62, total_loss:  29.87\n",
      "epoch: 5 step:  121/250, lr:0.000100, giou_loss:   6.74, conf_loss:   7.95, prob_loss:  11.32, total_loss:  26.00\n",
      "epoch: 5 step:  122/250, lr:0.000100, giou_loss:  10.35, conf_loss:  11.33, prob_loss:  25.38, total_loss:  47.06\n",
      "epoch: 5 step:  123/250, lr:0.000100, giou_loss:   6.25, conf_loss:   8.02, prob_loss:   9.63, total_loss:  23.90\n",
      "epoch: 5 step:  124/250, lr:0.000100, giou_loss:   5.78, conf_loss:   7.77, prob_loss:  14.69, total_loss:  28.24\n",
      "epoch: 5 step:  125/250, lr:0.000100, giou_loss:   5.93, conf_loss:   9.44, prob_loss:  14.01, total_loss:  29.37\n",
      "epoch: 5 step:  126/250, lr:0.000100, giou_loss:   8.44, conf_loss:   9.54, prob_loss:  18.67, total_loss:  36.65\n",
      "epoch: 5 step:  127/250, lr:0.000100, giou_loss:   7.03, conf_loss:   7.81, prob_loss:  17.99, total_loss:  32.83\n",
      "epoch: 5 step:  128/250, lr:0.000100, giou_loss:   7.17, conf_loss:   8.81, prob_loss:  14.85, total_loss:  30.83\n",
      "epoch: 5 step:  129/250, lr:0.000100, giou_loss:   7.11, conf_loss:   8.01, prob_loss:  14.77, total_loss:  29.88\n",
      "epoch: 5 step:  130/250, lr:0.000100, giou_loss:   8.31, conf_loss:   8.44, prob_loss:  17.75, total_loss:  34.49\n",
      "epoch: 5 step:  131/250, lr:0.000100, giou_loss:   6.04, conf_loss:   7.59, prob_loss:   5.12, total_loss:  18.75\n",
      "epoch: 5 step:  132/250, lr:0.000100, giou_loss:   8.79, conf_loss:   8.57, prob_loss:  15.76, total_loss:  33.12\n",
      "epoch: 5 step:  133/250, lr:0.000100, giou_loss:   7.96, conf_loss:   7.65, prob_loss:  21.12, total_loss:  36.74\n",
      "epoch: 5 step:  134/250, lr:0.000100, giou_loss:   6.24, conf_loss:   7.77, prob_loss:  15.15, total_loss:  29.16\n",
      "epoch: 5 step:  135/250, lr:0.000100, giou_loss:   6.63, conf_loss:   7.94, prob_loss:  11.46, total_loss:  26.04\n",
      "epoch: 5 step:  136/250, lr:0.000100, giou_loss:   7.40, conf_loss:   8.37, prob_loss:  18.91, total_loss:  34.68\n",
      "epoch: 5 step:  137/250, lr:0.000100, giou_loss:   5.47, conf_loss:   7.42, prob_loss:   7.32, total_loss:  20.21\n",
      "epoch: 5 step:  138/250, lr:0.000100, giou_loss:   6.83, conf_loss:   7.69, prob_loss:  15.34, total_loss:  29.85\n",
      "epoch: 5 step:  139/250, lr:0.000100, giou_loss:   5.84, conf_loss:   7.72, prob_loss:  11.53, total_loss:  25.09\n",
      "epoch: 5 step:  140/250, lr:0.000100, giou_loss:   7.38, conf_loss:   8.19, prob_loss:  21.28, total_loss:  36.85\n",
      "epoch: 5 step:  141/250, lr:0.000100, giou_loss:   8.34, conf_loss:   8.14, prob_loss:  14.14, total_loss:  30.61\n",
      "epoch: 5 step:  142/250, lr:0.000100, giou_loss:   7.57, conf_loss:   9.65, prob_loss:  21.40, total_loss:  38.63\n",
      "epoch: 5 step:  143/250, lr:0.000100, giou_loss:   8.25, conf_loss:   7.83, prob_loss:  25.95, total_loss:  42.03\n",
      "epoch: 5 step:  144/250, lr:0.000100, giou_loss:   8.75, conf_loss:   8.12, prob_loss:  15.91, total_loss:  32.78\n",
      "epoch: 5 step:  145/250, lr:0.000100, giou_loss:   8.22, conf_loss:   8.17, prob_loss:  15.64, total_loss:  32.03\n",
      "epoch: 5 step:  146/250, lr:0.000100, giou_loss:   8.85, conf_loss:   8.33, prob_loss:  16.79, total_loss:  33.97\n",
      "epoch: 5 step:  147/250, lr:0.000100, giou_loss:   7.82, conf_loss:   8.26, prob_loss:  11.42, total_loss:  27.49\n",
      "epoch: 5 step:  148/250, lr:0.000100, giou_loss:   9.09, conf_loss:   9.79, prob_loss:  16.86, total_loss:  35.74\n",
      "epoch: 5 step:  149/250, lr:0.000100, giou_loss:   8.73, conf_loss:   8.36, prob_loss:  30.02, total_loss:  47.11\n",
      "epoch: 5 step:  150/250, lr:0.000100, giou_loss:   6.87, conf_loss:   8.32, prob_loss:  12.05, total_loss:  27.24\n",
      "epoch: 5 step:  151/250, lr:0.000100, giou_loss:   4.31, conf_loss:   7.45, prob_loss:  15.42, total_loss:  27.18\n",
      "epoch: 5 step:  152/250, lr:0.000100, giou_loss:   9.22, conf_loss:   7.99, prob_loss:  10.51, total_loss:  27.72\n",
      "epoch: 5 step:  153/250, lr:0.000100, giou_loss:   6.94, conf_loss:   7.88, prob_loss:   8.66, total_loss:  23.48\n",
      "epoch: 5 step:  154/250, lr:0.000100, giou_loss:   9.47, conf_loss:   9.48, prob_loss:  21.06, total_loss:  40.00\n",
      "epoch: 5 step:  155/250, lr:0.000100, giou_loss:   8.74, conf_loss:   8.74, prob_loss:  13.09, total_loss:  30.57\n",
      "epoch: 5 step:  156/250, lr:0.000100, giou_loss:   9.41, conf_loss:   8.84, prob_loss:  20.15, total_loss:  38.39\n",
      "epoch: 5 step:  157/250, lr:0.000100, giou_loss:   7.15, conf_loss:   7.32, prob_loss:  16.30, total_loss:  30.77\n",
      "epoch: 5 step:  158/250, lr:0.000100, giou_loss:   6.03, conf_loss:   7.45, prob_loss:   8.99, total_loss:  22.48\n",
      "epoch: 5 step:  159/250, lr:0.000100, giou_loss:   8.64, conf_loss:   7.80, prob_loss:  12.00, total_loss:  28.44\n",
      "epoch: 5 step:  160/250, lr:0.000100, giou_loss:   8.92, conf_loss:   9.49, prob_loss:  23.16, total_loss:  41.56\n",
      "epoch: 5 step:  161/250, lr:0.000100, giou_loss:   3.56, conf_loss:   8.19, prob_loss:  10.94, total_loss:  22.68\n",
      "epoch: 5 step:  162/250, lr:0.000100, giou_loss:   9.67, conf_loss:   9.54, prob_loss:  24.47, total_loss:  43.67\n",
      "epoch: 5 step:  163/250, lr:0.000100, giou_loss:   6.79, conf_loss:   7.64, prob_loss:  11.14, total_loss:  25.57\n",
      "epoch: 5 step:  164/250, lr:0.000100, giou_loss:   8.05, conf_loss:   8.25, prob_loss:  15.58, total_loss:  31.87\n",
      "epoch: 5 step:  165/250, lr:0.000100, giou_loss:   5.21, conf_loss:   6.98, prob_loss:   8.16, total_loss:  20.35\n",
      "epoch: 5 step:  166/250, lr:0.000100, giou_loss:   6.94, conf_loss:   7.81, prob_loss:  15.06, total_loss:  29.80\n",
      "epoch: 5 step:  167/250, lr:0.000100, giou_loss:   8.26, conf_loss:   8.14, prob_loss:  17.58, total_loss:  33.98\n",
      "epoch: 5 step:  168/250, lr:0.000100, giou_loss:   7.16, conf_loss:   8.24, prob_loss:  15.15, total_loss:  30.55\n",
      "epoch: 5 step:  169/250, lr:0.000100, giou_loss:   7.11, conf_loss:   8.71, prob_loss:  11.82, total_loss:  27.64\n",
      "epoch: 5 step:  170/250, lr:0.000100, giou_loss:  10.67, conf_loss:   9.34, prob_loss:  17.57, total_loss:  37.57\n",
      "epoch: 5 step:  171/250, lr:0.000100, giou_loss:   6.90, conf_loss:   7.70, prob_loss:  10.45, total_loss:  25.05\n",
      "epoch: 5 step:  172/250, lr:0.000100, giou_loss:   6.78, conf_loss:   7.15, prob_loss:  16.78, total_loss:  30.71\n",
      "epoch: 5 step:  173/250, lr:0.000100, giou_loss:   7.65, conf_loss:   7.78, prob_loss:  15.79, total_loss:  31.22\n",
      "epoch: 5 step:  174/250, lr:0.000100, giou_loss:   5.39, conf_loss:   7.20, prob_loss:   6.96, total_loss:  19.55\n",
      "epoch: 5 step:  175/250, lr:0.000100, giou_loss:   7.77, conf_loss:   7.60, prob_loss:  15.37, total_loss:  30.74\n",
      "epoch: 5 step:  176/250, lr:0.000100, giou_loss:   5.15, conf_loss:   8.05, prob_loss:   9.56, total_loss:  22.75\n",
      "epoch: 5 step:  177/250, lr:0.000100, giou_loss:   6.65, conf_loss:   7.78, prob_loss:  12.07, total_loss:  26.50\n",
      "epoch: 5 step:  178/250, lr:0.000100, giou_loss:   6.94, conf_loss:  10.21, prob_loss:  12.10, total_loss:  29.25\n",
      "epoch: 5 step:  179/250, lr:0.000100, giou_loss:   6.34, conf_loss:   7.58, prob_loss:   8.73, total_loss:  22.64\n",
      "epoch: 5 step:  180/250, lr:0.000100, giou_loss:   7.58, conf_loss:   8.08, prob_loss:  13.87, total_loss:  29.53\n",
      "epoch: 5 step:  181/250, lr:0.000100, giou_loss:   5.53, conf_loss:   7.80, prob_loss:  11.84, total_loss:  25.17\n",
      "epoch: 5 step:  182/250, lr:0.000100, giou_loss:   6.37, conf_loss:   7.50, prob_loss:  17.57, total_loss:  31.44\n",
      "epoch: 5 step:  183/250, lr:0.000100, giou_loss:   5.28, conf_loss:   7.40, prob_loss:   9.53, total_loss:  22.21\n",
      "epoch: 5 step:  184/250, lr:0.000100, giou_loss:   6.50, conf_loss:   7.53, prob_loss:  10.13, total_loss:  24.16\n",
      "epoch: 5 step:  185/250, lr:0.000100, giou_loss:   9.65, conf_loss:   8.39, prob_loss:  16.55, total_loss:  34.59\n",
      "epoch: 5 step:  186/250, lr:0.000100, giou_loss:   4.29, conf_loss:   7.58, prob_loss:   5.59, total_loss:  17.47\n",
      "epoch: 5 step:  187/250, lr:0.000100, giou_loss:   7.95, conf_loss:   8.23, prob_loss:  17.91, total_loss:  34.09\n",
      "epoch: 5 step:  188/250, lr:0.000100, giou_loss:   6.89, conf_loss:   8.29, prob_loss:  17.31, total_loss:  32.50\n",
      "epoch: 5 step:  189/250, lr:0.000100, giou_loss:   5.28, conf_loss:   7.03, prob_loss:  12.85, total_loss:  25.17\n",
      "epoch: 5 step:  190/250, lr:0.000100, giou_loss:   7.05, conf_loss:   7.41, prob_loss:  13.27, total_loss:  27.72\n",
      "epoch: 5 step:  191/250, lr:0.000100, giou_loss:   6.70, conf_loss:   7.12, prob_loss:  16.78, total_loss:  30.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step:  192/250, lr:0.000100, giou_loss:   8.80, conf_loss:   8.98, prob_loss:  23.98, total_loss:  41.77\n",
      "epoch: 5 step:  193/250, lr:0.000100, giou_loss:   7.18, conf_loss:   7.92, prob_loss:  18.06, total_loss:  33.16\n",
      "epoch: 5 step:  194/250, lr:0.000100, giou_loss:   5.60, conf_loss:   7.52, prob_loss:  10.39, total_loss:  23.51\n",
      "epoch: 5 step:  195/250, lr:0.000100, giou_loss:   6.76, conf_loss:   7.91, prob_loss:  16.11, total_loss:  30.78\n",
      "epoch: 5 step:  196/250, lr:0.000100, giou_loss:   5.48, conf_loss:   8.07, prob_loss:  13.36, total_loss:  26.91\n",
      "epoch: 5 step:  197/250, lr:0.000100, giou_loss:   8.78, conf_loss:   7.71, prob_loss:  16.06, total_loss:  32.55\n",
      "epoch: 5 step:  198/250, lr:0.000100, giou_loss:   8.61, conf_loss:   9.34, prob_loss:  12.46, total_loss:  30.41\n",
      "epoch: 5 step:  199/250, lr:0.000100, giou_loss:   6.88, conf_loss:   7.56, prob_loss:   8.03, total_loss:  22.46\n",
      "epoch: 5 step:  200/250, lr:0.000100, giou_loss:   7.37, conf_loss:   7.87, prob_loss:  21.16, total_loss:  36.39\n",
      "epoch: 5 step:  201/250, lr:0.000100, giou_loss:   5.78, conf_loss:   8.33, prob_loss:  15.16, total_loss:  29.27\n",
      "epoch: 5 step:  202/250, lr:0.000100, giou_loss:  11.14, conf_loss:  10.09, prob_loss:  21.04, total_loss:  42.28\n",
      "epoch: 5 step:  203/250, lr:0.000100, giou_loss:   5.81, conf_loss:   7.29, prob_loss:   6.09, total_loss:  19.19\n",
      "epoch: 5 step:  204/250, lr:0.000100, giou_loss:   8.16, conf_loss:   8.81, prob_loss:  15.87, total_loss:  32.84\n",
      "epoch: 5 step:  205/250, lr:0.000100, giou_loss:   7.00, conf_loss:   7.45, prob_loss:  14.30, total_loss:  28.76\n",
      "epoch: 5 step:  206/250, lr:0.000100, giou_loss:   7.07, conf_loss:   8.52, prob_loss:  11.53, total_loss:  27.12\n",
      "epoch: 5 step:  207/250, lr:0.000100, giou_loss:   3.73, conf_loss:   9.45, prob_loss:  10.26, total_loss:  23.44\n",
      "epoch: 5 step:  208/250, lr:0.000100, giou_loss:   7.65, conf_loss:   9.11, prob_loss:  17.34, total_loss:  34.09\n",
      "epoch: 5 step:  209/250, lr:0.000100, giou_loss:   6.52, conf_loss:   7.16, prob_loss:  10.62, total_loss:  24.31\n",
      "epoch: 5 step:  210/250, lr:0.000100, giou_loss:   7.08, conf_loss:   6.92, prob_loss:  14.29, total_loss:  28.29\n",
      "epoch: 5 step:  211/250, lr:0.000100, giou_loss:   7.27, conf_loss:   7.84, prob_loss:  13.85, total_loss:  28.96\n",
      "epoch: 5 step:  212/250, lr:0.000100, giou_loss:   9.60, conf_loss:   9.20, prob_loss:  14.73, total_loss:  33.52\n",
      "epoch: 5 step:  213/250, lr:0.000100, giou_loss:   5.66, conf_loss:   7.34, prob_loss:   7.85, total_loss:  20.84\n",
      "epoch: 5 step:  214/250, lr:0.000100, giou_loss:   5.75, conf_loss:   7.26, prob_loss:  14.52, total_loss:  27.53\n",
      "epoch: 5 step:  215/250, lr:0.000100, giou_loss:   7.24, conf_loss:   7.80, prob_loss:  13.38, total_loss:  28.42\n",
      "epoch: 5 step:  216/250, lr:0.000100, giou_loss:   6.29, conf_loss:   7.93, prob_loss:  10.58, total_loss:  24.80\n",
      "epoch: 5 step:  217/250, lr:0.000100, giou_loss:   6.70, conf_loss:   6.76, prob_loss:  13.85, total_loss:  27.32\n",
      "epoch: 5 step:  218/250, lr:0.000100, giou_loss:   6.07, conf_loss:   6.69, prob_loss:  10.31, total_loss:  23.07\n",
      "epoch: 5 step:  219/250, lr:0.000100, giou_loss:   5.79, conf_loss:   7.22, prob_loss:  16.00, total_loss:  29.01\n",
      "epoch: 5 step:  220/250, lr:0.000100, giou_loss:   6.48, conf_loss:   6.56, prob_loss:  27.06, total_loss:  40.09\n",
      "epoch: 5 step:  221/250, lr:0.000100, giou_loss:   5.39, conf_loss:   6.67, prob_loss:  14.46, total_loss:  26.52\n",
      "epoch: 5 step:  222/250, lr:0.000100, giou_loss:   7.83, conf_loss:   8.09, prob_loss:  16.25, total_loss:  32.17\n",
      "epoch: 5 step:  223/250, lr:0.000100, giou_loss:   6.83, conf_loss:   7.22, prob_loss:  13.81, total_loss:  27.86\n",
      "epoch: 5 step:  224/250, lr:0.000100, giou_loss:   6.68, conf_loss:   7.96, prob_loss:  12.90, total_loss:  27.54\n",
      "epoch: 5 step:  225/250, lr:0.000100, giou_loss:   7.35, conf_loss:   7.61, prob_loss:  14.91, total_loss:  29.87\n",
      "epoch: 5 step:  226/250, lr:0.000100, giou_loss:   6.72, conf_loss:   7.59, prob_loss:  20.46, total_loss:  34.77\n",
      "epoch: 5 step:  227/250, lr:0.000100, giou_loss:   9.85, conf_loss:   8.20, prob_loss:  17.77, total_loss:  35.82\n",
      "epoch: 5 step:  228/250, lr:0.000100, giou_loss:   7.97, conf_loss:   7.83, prob_loss:  14.30, total_loss:  30.10\n",
      "epoch: 5 step:  229/250, lr:0.000100, giou_loss:   6.56, conf_loss:   7.36, prob_loss:  15.25, total_loss:  29.17\n",
      "epoch: 5 step:  230/250, lr:0.000100, giou_loss:   7.16, conf_loss:   7.10, prob_loss:  10.59, total_loss:  24.85\n",
      "epoch: 5 step:  231/250, lr:0.000100, giou_loss:   7.81, conf_loss:   7.49, prob_loss:  13.27, total_loss:  28.57\n",
      "epoch: 5 step:  232/250, lr:0.000100, giou_loss:   5.77, conf_loss:   7.05, prob_loss:  11.67, total_loss:  24.50\n",
      "epoch: 5 step:  233/250, lr:0.000100, giou_loss:   5.64, conf_loss:   8.48, prob_loss:   8.75, total_loss:  22.88\n",
      "epoch: 5 step:  234/250, lr:0.000100, giou_loss:   6.83, conf_loss:   7.43, prob_loss:  17.67, total_loss:  31.93\n",
      "epoch: 5 step:  235/250, lr:0.000100, giou_loss:   8.35, conf_loss:   7.80, prob_loss:  14.98, total_loss:  31.13\n",
      "epoch: 5 step:  236/250, lr:0.000100, giou_loss:   8.93, conf_loss:   7.61, prob_loss:  21.50, total_loss:  38.03\n",
      "epoch: 5 step:  237/250, lr:0.000100, giou_loss:   5.14, conf_loss:   6.93, prob_loss:   5.07, total_loss:  17.13\n",
      "epoch: 5 step:  238/250, lr:0.000100, giou_loss:  10.40, conf_loss:   8.62, prob_loss:  22.23, total_loss:  41.25\n",
      "epoch: 5 step:  239/250, lr:0.000100, giou_loss:   6.99, conf_loss:   7.34, prob_loss:  15.21, total_loss:  29.54\n",
      "epoch: 5 step:  240/250, lr:0.000100, giou_loss:   7.60, conf_loss:   9.14, prob_loss:  15.15, total_loss:  31.89\n",
      "epoch: 5 step:  241/250, lr:0.000100, giou_loss:   7.98, conf_loss:   8.03, prob_loss:  14.39, total_loss:  30.41\n",
      "epoch: 5 step:  242/250, lr:0.000100, giou_loss:   5.27, conf_loss:   7.05, prob_loss:  18.77, total_loss:  31.09\n",
      "epoch: 5 step:  243/250, lr:0.000100, giou_loss:   6.89, conf_loss:   6.82, prob_loss:  11.25, total_loss:  24.95\n",
      "epoch: 5 step:  244/250, lr:0.000100, giou_loss:   5.16, conf_loss:   7.99, prob_loss:  11.06, total_loss:  24.21\n",
      "epoch: 5 step:  245/250, lr:0.000100, giou_loss:   6.14, conf_loss:   7.96, prob_loss:  14.52, total_loss:  28.61\n",
      "epoch: 5 step:  246/250, lr:0.000100, giou_loss:   6.59, conf_loss:   7.42, prob_loss:  11.67, total_loss:  25.67\n",
      "epoch: 5 step:  247/250, lr:0.000100, giou_loss:   7.84, conf_loss:   7.12, prob_loss:  16.68, total_loss:  31.64\n",
      "epoch: 5 step:  248/250, lr:0.000100, giou_loss:   8.18, conf_loss:   7.13, prob_loss:  14.19, total_loss:  29.50\n",
      "epoch: 5 step:  249/250, lr:0.000100, giou_loss:   5.87, conf_loss:   6.60, prob_loss:   8.44, total_loss:  20.90\n",
      "epoch: 5 step:    0/250, lr:0.000100, giou_loss:   8.10, conf_loss:   7.26, prob_loss:  17.47, total_loss:  32.83\n",
      "epoch: 5 step:    1/250, lr:0.000100, giou_loss:   5.59, conf_loss:   6.78, prob_loss:  15.31, total_loss:  27.68\n",
      "\n",
      "\n",
      "giou_val_loss:  15.43, conf_val_loss:  60.53, prob_val_loss:  75.48, total_val_loss: 151.44\n",
      "\n",
      "\n",
      "epoch: 6 step:    2/250, lr:0.000100, giou_loss:   4.80, conf_loss:   7.25, prob_loss:   6.86, total_loss:  18.91\n",
      "epoch: 6 step:    3/250, lr:0.000100, giou_loss:   4.75, conf_loss:   7.12, prob_loss:   7.62, total_loss:  19.50\n",
      "epoch: 6 step:    4/250, lr:0.000100, giou_loss:   6.27, conf_loss:   6.60, prob_loss:  16.02, total_loss:  28.89\n",
      "epoch: 6 step:    5/250, lr:0.000100, giou_loss:   5.35, conf_loss:   6.43, prob_loss:   6.54, total_loss:  18.33\n",
      "epoch: 6 step:    6/250, lr:0.000100, giou_loss:   7.01, conf_loss:   6.65, prob_loss:   9.37, total_loss:  23.03\n",
      "epoch: 6 step:    7/250, lr:0.000100, giou_loss:   6.13, conf_loss:   6.72, prob_loss:   8.27, total_loss:  21.12\n",
      "epoch: 6 step:    8/250, lr:0.000100, giou_loss:   9.59, conf_loss:   8.06, prob_loss:  17.56, total_loss:  35.21\n",
      "epoch: 6 step:    9/250, lr:0.000100, giou_loss:   9.89, conf_loss:   8.81, prob_loss:  11.43, total_loss:  30.13\n",
      "epoch: 6 step:   10/250, lr:0.000100, giou_loss:   6.34, conf_loss:   6.71, prob_loss:  12.49, total_loss:  25.53\n",
      "epoch: 6 step:   11/250, lr:0.000100, giou_loss:   4.65, conf_loss:   6.45, prob_loss:  10.76, total_loss:  21.86\n",
      "epoch: 6 step:   12/250, lr:0.000100, giou_loss:   5.24, conf_loss:   6.70, prob_loss:   7.53, total_loss:  19.47\n",
      "epoch: 6 step:   13/250, lr:0.000100, giou_loss:   6.23, conf_loss:   6.69, prob_loss:   8.35, total_loss:  21.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step:   14/250, lr:0.000100, giou_loss:   3.93, conf_loss:   6.11, prob_loss:   8.70, total_loss:  18.73\n",
      "epoch: 6 step:   15/250, lr:0.000100, giou_loss:   7.88, conf_loss:   7.11, prob_loss:   9.74, total_loss:  24.73\n",
      "epoch: 6 step:   16/250, lr:0.000100, giou_loss:   6.95, conf_loss:   7.07, prob_loss:  14.94, total_loss:  28.97\n",
      "epoch: 6 step:   17/250, lr:0.000100, giou_loss:   6.06, conf_loss:   6.63, prob_loss:  10.36, total_loss:  23.06\n",
      "epoch: 6 step:   18/250, lr:0.000100, giou_loss:   8.33, conf_loss:   8.44, prob_loss:  14.13, total_loss:  30.90\n",
      "epoch: 6 step:   19/250, lr:0.000100, giou_loss:   8.11, conf_loss:   8.32, prob_loss:  12.94, total_loss:  29.37\n",
      "epoch: 6 step:   20/250, lr:0.000100, giou_loss:   8.24, conf_loss:   7.20, prob_loss:  11.51, total_loss:  26.95\n",
      "epoch: 6 step:   21/250, lr:0.000100, giou_loss:   8.41, conf_loss:   6.89, prob_loss:   9.74, total_loss:  25.05\n",
      "epoch: 6 step:   22/250, lr:0.000100, giou_loss:   6.39, conf_loss:   6.58, prob_loss:   6.25, total_loss:  19.22\n",
      "epoch: 6 step:   23/250, lr:0.000100, giou_loss:   8.32, conf_loss:   7.00, prob_loss:   9.18, total_loss:  24.50\n",
      "epoch: 6 step:   24/250, lr:0.000100, giou_loss:   6.17, conf_loss:   6.65, prob_loss:  10.61, total_loss:  23.42\n",
      "epoch: 6 step:   25/250, lr:0.000100, giou_loss:   5.43, conf_loss:   6.33, prob_loss:  11.02, total_loss:  22.78\n",
      "epoch: 6 step:   26/250, lr:0.000100, giou_loss:   6.94, conf_loss:   7.74, prob_loss:  10.48, total_loss:  25.16\n",
      "epoch: 6 step:   27/250, lr:0.000100, giou_loss:   5.20, conf_loss:   6.27, prob_loss:   8.44, total_loss:  19.91\n",
      "epoch: 6 step:   28/250, lr:0.000100, giou_loss:   7.61, conf_loss:   6.99, prob_loss:  11.05, total_loss:  25.65\n",
      "epoch: 6 step:   29/250, lr:0.000100, giou_loss:   5.04, conf_loss:   6.87, prob_loss:   3.82, total_loss:  15.73\n",
      "epoch: 6 step:   30/250, lr:0.000100, giou_loss:   8.50, conf_loss:   6.87, prob_loss:  11.60, total_loss:  26.97\n",
      "epoch: 6 step:   31/250, lr:0.000100, giou_loss:   7.74, conf_loss:   6.74, prob_loss:  11.97, total_loss:  26.45\n",
      "epoch: 6 step:   32/250, lr:0.000100, giou_loss:   8.78, conf_loss:   6.93, prob_loss:  13.82, total_loss:  29.54\n",
      "epoch: 6 step:   33/250, lr:0.000100, giou_loss:   7.22, conf_loss:   7.19, prob_loss:  15.89, total_loss:  30.30\n",
      "epoch: 6 step:   34/250, lr:0.000100, giou_loss:   8.35, conf_loss:   7.68, prob_loss:  10.20, total_loss:  26.23\n",
      "epoch: 6 step:   35/250, lr:0.000100, giou_loss:   6.78, conf_loss:   7.39, prob_loss:  14.49, total_loss:  28.66\n",
      "epoch: 6 step:   36/250, lr:0.000100, giou_loss:   5.56, conf_loss:   6.56, prob_loss:   6.14, total_loss:  18.26\n",
      "epoch: 6 step:   37/250, lr:0.000100, giou_loss:   6.39, conf_loss:   6.81, prob_loss:   7.25, total_loss:  20.45\n",
      "epoch: 6 step:   38/250, lr:0.000100, giou_loss:   6.44, conf_loss:   6.52, prob_loss:  11.50, total_loss:  24.46\n",
      "epoch: 6 step:   39/250, lr:0.000100, giou_loss:   7.83, conf_loss:   6.67, prob_loss:   9.36, total_loss:  23.86\n",
      "epoch: 6 step:   40/250, lr:0.000100, giou_loss:   8.63, conf_loss:   6.71, prob_loss:  18.06, total_loss:  33.40\n",
      "epoch: 6 step:   41/250, lr:0.000100, giou_loss:   5.21, conf_loss:   6.65, prob_loss:   5.37, total_loss:  17.23\n",
      "epoch: 6 step:   42/250, lr:0.000100, giou_loss:   6.68, conf_loss:   6.51, prob_loss:  13.76, total_loss:  26.95\n",
      "epoch: 6 step:   43/250, lr:0.000100, giou_loss:   5.91, conf_loss:   6.31, prob_loss:   9.03, total_loss:  21.25\n",
      "epoch: 6 step:   44/250, lr:0.000100, giou_loss:   7.32, conf_loss:   7.10, prob_loss:   8.50, total_loss:  22.92\n",
      "epoch: 6 step:   45/250, lr:0.000100, giou_loss:   6.66, conf_loss:   6.78, prob_loss:  11.63, total_loss:  25.07\n",
      "epoch: 6 step:   46/250, lr:0.000100, giou_loss:   7.07, conf_loss:   8.14, prob_loss:  15.51, total_loss:  30.71\n",
      "epoch: 6 step:   47/250, lr:0.000100, giou_loss:   7.26, conf_loss:   6.74, prob_loss:  12.11, total_loss:  26.10\n",
      "epoch: 6 step:   48/250, lr:0.000100, giou_loss:   5.83, conf_loss:   6.26, prob_loss:  12.87, total_loss:  24.97\n",
      "epoch: 6 step:   49/250, lr:0.000100, giou_loss:   6.21, conf_loss:   6.71, prob_loss:  10.26, total_loss:  23.18\n",
      "epoch: 6 step:   50/250, lr:0.000100, giou_loss:   7.93, conf_loss:   6.97, prob_loss:  10.26, total_loss:  25.16\n",
      "epoch: 6 step:   51/250, lr:0.000100, giou_loss:   7.15, conf_loss:   7.44, prob_loss:  11.83, total_loss:  26.42\n",
      "epoch: 6 step:   52/250, lr:0.000100, giou_loss:   7.04, conf_loss:   6.36, prob_loss:   7.39, total_loss:  20.79\n",
      "epoch: 6 step:   53/250, lr:0.000100, giou_loss:   6.13, conf_loss:   5.75, prob_loss:   8.70, total_loss:  20.58\n",
      "epoch: 6 step:   54/250, lr:0.000100, giou_loss:   7.06, conf_loss:   7.69, prob_loss:  16.52, total_loss:  31.27\n",
      "epoch: 6 step:   55/250, lr:0.000100, giou_loss:   8.37, conf_loss:   6.28, prob_loss:   9.65, total_loss:  24.30\n",
      "epoch: 6 step:   56/250, lr:0.000100, giou_loss:   4.75, conf_loss:   6.13, prob_loss:   6.46, total_loss:  17.33\n",
      "epoch: 6 step:   57/250, lr:0.000100, giou_loss:   6.80, conf_loss:   7.45, prob_loss:  12.94, total_loss:  27.19\n",
      "epoch: 6 step:   58/250, lr:0.000100, giou_loss:   7.63, conf_loss:   6.40, prob_loss:  11.75, total_loss:  25.77\n",
      "epoch: 6 step:   59/250, lr:0.000100, giou_loss:   7.51, conf_loss:   6.26, prob_loss:   7.54, total_loss:  21.32\n",
      "epoch: 6 step:   60/250, lr:0.000100, giou_loss:   6.04, conf_loss:   6.50, prob_loss:  15.64, total_loss:  28.18\n",
      "epoch: 6 step:   61/250, lr:0.000100, giou_loss:   7.71, conf_loss:   6.58, prob_loss:  14.22, total_loss:  28.51\n",
      "epoch: 6 step:   62/250, lr:0.000100, giou_loss:   5.65, conf_loss:   5.86, prob_loss:   5.28, total_loss:  16.79\n",
      "epoch: 6 step:   63/250, lr:0.000100, giou_loss:   5.83, conf_loss:   6.64, prob_loss:  10.78, total_loss:  23.24\n",
      "epoch: 6 step:   64/250, lr:0.000100, giou_loss:  10.25, conf_loss:   7.21, prob_loss:  13.42, total_loss:  30.88\n",
      "epoch: 6 step:   65/250, lr:0.000100, giou_loss:   5.05, conf_loss:   6.78, prob_loss:  10.56, total_loss:  22.40\n",
      "epoch: 6 step:   66/250, lr:0.000100, giou_loss:   5.89, conf_loss:   6.12, prob_loss:   7.45, total_loss:  19.46\n",
      "epoch: 6 step:   67/250, lr:0.000100, giou_loss:   4.27, conf_loss:   6.88, prob_loss:   7.79, total_loss:  18.95\n",
      "epoch: 6 step:   68/250, lr:0.000100, giou_loss:   4.79, conf_loss:   5.79, prob_loss:   8.82, total_loss:  19.39\n",
      "epoch: 6 step:   69/250, lr:0.000100, giou_loss:   5.43, conf_loss:   6.13, prob_loss:  14.20, total_loss:  25.76\n",
      "epoch: 6 step:   70/250, lr:0.000100, giou_loss:   6.81, conf_loss:   6.24, prob_loss:   7.82, total_loss:  20.87\n",
      "epoch: 6 step:   71/250, lr:0.000100, giou_loss:   7.36, conf_loss:   6.79, prob_loss:  13.00, total_loss:  27.14\n",
      "epoch: 6 step:   72/250, lr:0.000100, giou_loss:   6.42, conf_loss:   6.21, prob_loss:  12.14, total_loss:  24.76\n",
      "epoch: 6 step:   73/250, lr:0.000100, giou_loss:   5.90, conf_loss:   5.95, prob_loss:   7.44, total_loss:  19.30\n",
      "epoch: 6 step:   74/250, lr:0.000100, giou_loss:   6.21, conf_loss:   7.62, prob_loss:  10.68, total_loss:  24.50\n",
      "epoch: 6 step:   75/250, lr:0.000100, giou_loss:   5.81, conf_loss:   5.53, prob_loss:   5.20, total_loss:  16.54\n",
      "epoch: 6 step:   76/250, lr:0.000100, giou_loss:   8.64, conf_loss:   7.10, prob_loss:   9.96, total_loss:  25.70\n",
      "epoch: 6 step:   77/250, lr:0.000100, giou_loss:   7.97, conf_loss:   6.90, prob_loss:  13.97, total_loss:  28.84\n",
      "epoch: 6 step:   78/250, lr:0.000100, giou_loss:   8.51, conf_loss:   6.64, prob_loss:  10.73, total_loss:  25.87\n",
      "epoch: 6 step:   79/250, lr:0.000100, giou_loss:   6.46, conf_loss:   6.30, prob_loss:   7.60, total_loss:  20.36\n",
      "epoch: 6 step:   80/250, lr:0.000100, giou_loss:   7.41, conf_loss:   6.08, prob_loss:   9.46, total_loss:  22.95\n",
      "epoch: 6 step:   81/250, lr:0.000100, giou_loss:   6.90, conf_loss:   5.75, prob_loss:  15.72, total_loss:  28.38\n",
      "epoch: 6 step:   82/250, lr:0.000100, giou_loss:   7.73, conf_loss:   6.61, prob_loss:  13.28, total_loss:  27.62\n",
      "epoch: 6 step:   83/250, lr:0.000100, giou_loss:   7.86, conf_loss:   6.16, prob_loss:   9.09, total_loss:  23.10\n",
      "epoch: 6 step:   84/250, lr:0.000100, giou_loss:   6.22, conf_loss:   6.40, prob_loss:   9.09, total_loss:  21.71\n",
      "epoch: 6 step:   85/250, lr:0.000100, giou_loss:   6.20, conf_loss:   5.87, prob_loss:  10.43, total_loss:  22.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step:   86/250, lr:0.000100, giou_loss:   8.97, conf_loss:   6.48, prob_loss:  13.71, total_loss:  29.16\n",
      "epoch: 6 step:   87/250, lr:0.000100, giou_loss:   5.81, conf_loss:   5.80, prob_loss:   6.79, total_loss:  18.40\n",
      "epoch: 6 step:   88/250, lr:0.000100, giou_loss:   6.35, conf_loss:   6.14, prob_loss:   8.86, total_loss:  21.35\n",
      "epoch: 6 step:   89/250, lr:0.000100, giou_loss:   5.79, conf_loss:   5.80, prob_loss:  12.14, total_loss:  23.72\n",
      "epoch: 6 step:   90/250, lr:0.000100, giou_loss:   6.86, conf_loss:   6.19, prob_loss:  16.67, total_loss:  29.72\n",
      "epoch: 6 step:   91/250, lr:0.000100, giou_loss:   6.51, conf_loss:   5.99, prob_loss:   7.26, total_loss:  19.75\n",
      "epoch: 6 step:   92/250, lr:0.000100, giou_loss:   4.85, conf_loss:   6.19, prob_loss:   5.96, total_loss:  16.99\n",
      "epoch: 6 step:   93/250, lr:0.000100, giou_loss:   7.44, conf_loss:   5.91, prob_loss:  10.32, total_loss:  23.68\n",
      "epoch: 6 step:   94/250, lr:0.000100, giou_loss:   6.79, conf_loss:   5.88, prob_loss:   9.06, total_loss:  21.73\n",
      "epoch: 6 step:   95/250, lr:0.000100, giou_loss:   5.91, conf_loss:   5.73, prob_loss:   6.92, total_loss:  18.56\n",
      "epoch: 6 step:   96/250, lr:0.000100, giou_loss:   6.19, conf_loss:   6.23, prob_loss:   8.50, total_loss:  20.91\n",
      "epoch: 6 step:   97/250, lr:0.000100, giou_loss:   6.75, conf_loss:   8.25, prob_loss:  13.36, total_loss:  28.36\n",
      "epoch: 6 step:   98/250, lr:0.000100, giou_loss:   4.44, conf_loss:   5.59, prob_loss:   7.51, total_loss:  17.54\n",
      "epoch: 6 step:   99/250, lr:0.000100, giou_loss:   7.12, conf_loss:   6.37, prob_loss:  12.63, total_loss:  26.12\n",
      "epoch: 6 step:  100/250, lr:0.000100, giou_loss:   7.00, conf_loss:   7.07, prob_loss:  10.00, total_loss:  24.07\n",
      "epoch: 6 step:  101/250, lr:0.000100, giou_loss:   7.92, conf_loss:   6.17, prob_loss:   6.58, total_loss:  20.67\n",
      "epoch: 6 step:  102/250, lr:0.000100, giou_loss:   7.85, conf_loss:   6.29, prob_loss:  14.22, total_loss:  28.36\n",
      "epoch: 6 step:  103/250, lr:0.000100, giou_loss:   7.33, conf_loss:   8.19, prob_loss:  10.02, total_loss:  25.53\n",
      "epoch: 6 step:  104/250, lr:0.000100, giou_loss:   4.90, conf_loss:   6.25, prob_loss:   6.19, total_loss:  17.35\n",
      "epoch: 6 step:  105/250, lr:0.000100, giou_loss:   8.31, conf_loss:   7.75, prob_loss:  15.00, total_loss:  31.05\n",
      "epoch: 6 step:  106/250, lr:0.000100, giou_loss:   5.73, conf_loss:   5.64, prob_loss:   6.91, total_loss:  18.28\n",
      "epoch: 6 step:  107/250, lr:0.000100, giou_loss:   4.90, conf_loss:   5.70, prob_loss:  13.03, total_loss:  23.64\n",
      "epoch: 6 step:  108/250, lr:0.000100, giou_loss:   8.34, conf_loss:   6.24, prob_loss:  13.73, total_loss:  28.31\n",
      "epoch: 6 step:  109/250, lr:0.000100, giou_loss:   6.78, conf_loss:   6.01, prob_loss:   8.85, total_loss:  21.64\n",
      "epoch: 6 step:  110/250, lr:0.000099, giou_loss:   6.19, conf_loss:   5.95, prob_loss:  11.33, total_loss:  23.47\n",
      "epoch: 6 step:  111/250, lr:0.000099, giou_loss:   5.18, conf_loss:   5.59, prob_loss:   5.45, total_loss:  16.23\n",
      "epoch: 6 step:  112/250, lr:0.000099, giou_loss:   7.48, conf_loss:   6.59, prob_loss:   6.98, total_loss:  21.05\n",
      "epoch: 6 step:  113/250, lr:0.000099, giou_loss:   6.90, conf_loss:   6.16, prob_loss:   9.63, total_loss:  22.69\n",
      "epoch: 6 step:  114/250, lr:0.000099, giou_loss:   4.88, conf_loss:   5.53, prob_loss:   6.94, total_loss:  17.35\n",
      "epoch: 6 step:  115/250, lr:0.000099, giou_loss:   5.39, conf_loss:   5.72, prob_loss:   8.21, total_loss:  19.33\n",
      "epoch: 6 step:  116/250, lr:0.000099, giou_loss:   4.19, conf_loss:   5.53, prob_loss:   5.29, total_loss:  15.01\n",
      "epoch: 6 step:  117/250, lr:0.000099, giou_loss:   7.49, conf_loss:   6.84, prob_loss:  13.85, total_loss:  28.19\n",
      "epoch: 6 step:  118/250, lr:0.000099, giou_loss:   6.62, conf_loss:   6.10, prob_loss:   8.25, total_loss:  20.97\n",
      "epoch: 6 step:  119/250, lr:0.000099, giou_loss:   7.42, conf_loss:   6.04, prob_loss:  10.31, total_loss:  23.77\n",
      "epoch: 6 step:  120/250, lr:0.000099, giou_loss:   6.62, conf_loss:   5.52, prob_loss:   6.32, total_loss:  18.46\n",
      "epoch: 6 step:  121/250, lr:0.000099, giou_loss:   6.44, conf_loss:   5.61, prob_loss:   8.87, total_loss:  20.92\n",
      "epoch: 6 step:  122/250, lr:0.000099, giou_loss:   7.31, conf_loss:   5.50, prob_loss:  11.06, total_loss:  23.87\n",
      "epoch: 6 step:  123/250, lr:0.000099, giou_loss:   5.76, conf_loss:   5.59, prob_loss:   9.24, total_loss:  20.58\n",
      "epoch: 6 step:  124/250, lr:0.000099, giou_loss:   6.95, conf_loss:   6.25, prob_loss:   9.20, total_loss:  22.40\n",
      "epoch: 6 step:  125/250, lr:0.000099, giou_loss:   5.11, conf_loss:   5.95, prob_loss:   7.06, total_loss:  18.12\n",
      "epoch: 6 step:  126/250, lr:0.000099, giou_loss:   6.60, conf_loss:   6.83, prob_loss:  12.61, total_loss:  26.04\n",
      "epoch: 6 step:  127/250, lr:0.000099, giou_loss:   5.46, conf_loss:   6.27, prob_loss:  11.25, total_loss:  22.97\n",
      "epoch: 6 step:  128/250, lr:0.000099, giou_loss:   7.30, conf_loss:   5.93, prob_loss:   9.05, total_loss:  22.27\n",
      "epoch: 6 step:  129/250, lr:0.000099, giou_loss:   6.94, conf_loss:   6.33, prob_loss:  12.87, total_loss:  26.14\n",
      "epoch: 6 step:  130/250, lr:0.000099, giou_loss:   5.79, conf_loss:   5.62, prob_loss:   5.90, total_loss:  17.31\n",
      "epoch: 6 step:  131/250, lr:0.000099, giou_loss:   5.48, conf_loss:   5.53, prob_loss:  11.64, total_loss:  22.65\n",
      "epoch: 6 step:  132/250, lr:0.000099, giou_loss:   7.30, conf_loss:   5.84, prob_loss:  10.20, total_loss:  23.34\n",
      "epoch: 6 step:  133/250, lr:0.000099, giou_loss:   5.37, conf_loss:   5.73, prob_loss:   5.23, total_loss:  16.33\n",
      "epoch: 6 step:  134/250, lr:0.000099, giou_loss:   5.95, conf_loss:   7.45, prob_loss:   8.74, total_loss:  22.15\n",
      "epoch: 6 step:  135/250, lr:0.000099, giou_loss:   4.78, conf_loss:   5.56, prob_loss:   7.48, total_loss:  17.82\n",
      "epoch: 6 step:  136/250, lr:0.000099, giou_loss:   5.44, conf_loss:   6.57, prob_loss:  10.76, total_loss:  22.77\n",
      "epoch: 6 step:  137/250, lr:0.000099, giou_loss:   8.08, conf_loss:   6.37, prob_loss:  10.18, total_loss:  24.63\n",
      "epoch: 6 step:  138/250, lr:0.000099, giou_loss:   7.02, conf_loss:   6.19, prob_loss:   8.76, total_loss:  21.97\n",
      "epoch: 6 step:  139/250, lr:0.000099, giou_loss:   6.49, conf_loss:   5.97, prob_loss:   6.01, total_loss:  18.47\n",
      "epoch: 6 step:  140/250, lr:0.000099, giou_loss:   6.05, conf_loss:   6.27, prob_loss:   7.33, total_loss:  19.64\n",
      "epoch: 6 step:  141/250, lr:0.000099, giou_loss:   5.52, conf_loss:   5.28, prob_loss:   8.63, total_loss:  19.43\n",
      "epoch: 6 step:  142/250, lr:0.000099, giou_loss:   4.82, conf_loss:   5.33, prob_loss:   4.77, total_loss:  14.93\n",
      "epoch: 6 step:  143/250, lr:0.000099, giou_loss:   4.45, conf_loss:   5.16, prob_loss:   7.32, total_loss:  16.93\n",
      "epoch: 6 step:  144/250, lr:0.000099, giou_loss:   5.80, conf_loss:   5.79, prob_loss:   5.80, total_loss:  17.39\n",
      "epoch: 6 step:  145/250, lr:0.000099, giou_loss:   5.00, conf_loss:   5.27, prob_loss:   3.79, total_loss:  14.05\n",
      "epoch: 6 step:  146/250, lr:0.000099, giou_loss:   8.02, conf_loss:   6.04, prob_loss:   9.00, total_loss:  23.06\n",
      "epoch: 6 step:  147/250, lr:0.000099, giou_loss:   8.09, conf_loss:   7.70, prob_loss:  14.02, total_loss:  29.81\n",
      "epoch: 6 step:  148/250, lr:0.000099, giou_loss:   7.30, conf_loss:   6.04, prob_loss:  12.91, total_loss:  26.25\n",
      "epoch: 6 step:  149/250, lr:0.000099, giou_loss:   6.42, conf_loss:   5.89, prob_loss:   8.64, total_loss:  20.95\n",
      "epoch: 6 step:  150/250, lr:0.000099, giou_loss:   6.35, conf_loss:   6.36, prob_loss:  10.59, total_loss:  23.30\n",
      "epoch: 6 step:  151/250, lr:0.000099, giou_loss:   7.51, conf_loss:   5.80, prob_loss:  16.04, total_loss:  29.35\n",
      "epoch: 6 step:  152/250, lr:0.000099, giou_loss:   7.06, conf_loss:   6.01, prob_loss:   9.03, total_loss:  22.11\n",
      "epoch: 6 step:  153/250, lr:0.000099, giou_loss:   6.51, conf_loss:   5.42, prob_loss:  14.60, total_loss:  26.53\n",
      "epoch: 6 step:  154/250, lr:0.000099, giou_loss:   7.47, conf_loss:   5.46, prob_loss:  12.48, total_loss:  25.42\n",
      "epoch: 6 step:  155/250, lr:0.000099, giou_loss:   6.89, conf_loss:   5.65, prob_loss:   7.11, total_loss:  19.66\n",
      "epoch: 6 step:  156/250, lr:0.000099, giou_loss:   6.20, conf_loss:   5.32, prob_loss:   7.41, total_loss:  18.94\n",
      "epoch: 6 step:  157/250, lr:0.000099, giou_loss:   5.75, conf_loss:   6.23, prob_loss:  12.87, total_loss:  24.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step:  158/250, lr:0.000099, giou_loss:   6.02, conf_loss:   5.74, prob_loss:  11.25, total_loss:  23.02\n",
      "epoch: 6 step:  159/250, lr:0.000099, giou_loss:   4.94, conf_loss:   4.97, prob_loss:  12.48, total_loss:  22.39\n",
      "epoch: 6 step:  160/250, lr:0.000099, giou_loss:   7.20, conf_loss:   5.61, prob_loss:  11.58, total_loss:  24.39\n",
      "epoch: 6 step:  161/250, lr:0.000099, giou_loss:   7.09, conf_loss:   5.58, prob_loss:  12.58, total_loss:  25.25\n",
      "epoch: 6 step:  162/250, lr:0.000099, giou_loss:   5.84, conf_loss:   5.79, prob_loss:   8.93, total_loss:  20.56\n",
      "epoch: 6 step:  163/250, lr:0.000099, giou_loss:   7.48, conf_loss:   7.40, prob_loss:  16.20, total_loss:  31.07\n",
      "epoch: 6 step:  164/250, lr:0.000099, giou_loss:   4.40, conf_loss:   5.24, prob_loss:   7.73, total_loss:  17.36\n",
      "epoch: 6 step:  165/250, lr:0.000099, giou_loss:   6.43, conf_loss:   5.89, prob_loss:  17.12, total_loss:  29.44\n",
      "epoch: 6 step:  166/250, lr:0.000099, giou_loss:   7.79, conf_loss:   6.33, prob_loss:  15.60, total_loss:  29.72\n",
      "epoch: 6 step:  167/250, lr:0.000099, giou_loss:   7.56, conf_loss:   6.20, prob_loss:  13.44, total_loss:  27.20\n",
      "epoch: 6 step:  168/250, lr:0.000099, giou_loss:   7.24, conf_loss:   6.81, prob_loss:  12.66, total_loss:  26.71\n",
      "epoch: 6 step:  169/250, lr:0.000099, giou_loss:   5.79, conf_loss:   5.67, prob_loss:  18.33, total_loss:  29.80\n",
      "epoch: 6 step:  170/250, lr:0.000099, giou_loss:   5.84, conf_loss:   6.79, prob_loss:  12.84, total_loss:  25.47\n",
      "epoch: 6 step:  171/250, lr:0.000099, giou_loss:   6.75, conf_loss:   5.53, prob_loss:   9.73, total_loss:  22.00\n",
      "epoch: 6 step:  172/250, lr:0.000099, giou_loss:   8.51, conf_loss:   7.31, prob_loss:  21.66, total_loss:  37.48\n",
      "epoch: 6 step:  173/250, lr:0.000099, giou_loss:   5.97, conf_loss:   6.14, prob_loss:   6.77, total_loss:  18.88\n",
      "epoch: 6 step:  174/250, lr:0.000099, giou_loss:   7.73, conf_loss:   6.53, prob_loss:  10.48, total_loss:  24.74\n",
      "epoch: 6 step:  175/250, lr:0.000099, giou_loss:   5.64, conf_loss:   5.40, prob_loss:  14.70, total_loss:  25.73\n",
      "epoch: 6 step:  176/250, lr:0.000099, giou_loss:   7.04, conf_loss:   6.78, prob_loss:  12.36, total_loss:  26.19\n",
      "epoch: 6 step:  177/250, lr:0.000099, giou_loss:   8.74, conf_loss:   6.81, prob_loss:  16.24, total_loss:  31.79\n",
      "epoch: 6 step:  178/250, lr:0.000099, giou_loss:   7.68, conf_loss:   5.77, prob_loss:  10.04, total_loss:  23.49\n",
      "epoch: 6 step:  179/250, lr:0.000099, giou_loss:   5.76, conf_loss:   5.26, prob_loss:   9.74, total_loss:  20.76\n",
      "epoch: 6 step:  180/250, lr:0.000099, giou_loss:   4.58, conf_loss:   4.87, prob_loss:   8.39, total_loss:  17.84\n",
      "epoch: 6 step:  181/250, lr:0.000099, giou_loss:   5.58, conf_loss:   5.59, prob_loss:   7.64, total_loss:  18.81\n",
      "epoch: 6 step:  182/250, lr:0.000099, giou_loss:   4.39, conf_loss:   6.89, prob_loss:   6.91, total_loss:  18.20\n",
      "epoch: 6 step:  183/250, lr:0.000099, giou_loss:   6.22, conf_loss:   5.75, prob_loss:  13.69, total_loss:  25.66\n",
      "epoch: 6 step:  184/250, lr:0.000099, giou_loss:   5.71, conf_loss:   5.76, prob_loss:  11.67, total_loss:  23.14\n",
      "epoch: 6 step:  185/250, lr:0.000099, giou_loss:   7.71, conf_loss:   5.62, prob_loss:  11.82, total_loss:  25.15\n",
      "epoch: 6 step:  186/250, lr:0.000099, giou_loss:   7.72, conf_loss:   5.35, prob_loss:  11.99, total_loss:  25.06\n",
      "epoch: 6 step:  187/250, lr:0.000099, giou_loss:   5.56, conf_loss:   4.83, prob_loss:   7.61, total_loss:  17.99\n",
      "epoch: 6 step:  188/250, lr:0.000099, giou_loss:   6.40, conf_loss:   5.27, prob_loss:   7.60, total_loss:  19.27\n",
      "epoch: 6 step:  189/250, lr:0.000099, giou_loss:   7.26, conf_loss:   6.58, prob_loss:  12.20, total_loss:  26.05\n",
      "epoch: 6 step:  190/250, lr:0.000099, giou_loss:   8.12, conf_loss:   7.41, prob_loss:  12.77, total_loss:  28.30\n",
      "epoch: 6 step:  191/250, lr:0.000099, giou_loss:   9.37, conf_loss:   6.75, prob_loss:  16.47, total_loss:  32.58\n",
      "epoch: 6 step:  192/250, lr:0.000099, giou_loss:   7.37, conf_loss:   5.92, prob_loss:  11.73, total_loss:  25.01\n",
      "epoch: 6 step:  193/250, lr:0.000099, giou_loss:   5.19, conf_loss:   5.02, prob_loss:   7.79, total_loss:  18.00\n",
      "epoch: 6 step:  194/250, lr:0.000099, giou_loss:   5.44, conf_loss:   5.06, prob_loss:   6.08, total_loss:  16.59\n",
      "epoch: 6 step:  195/250, lr:0.000099, giou_loss:   7.35, conf_loss:   6.18, prob_loss:  14.19, total_loss:  27.72\n",
      "epoch: 6 step:  196/250, lr:0.000099, giou_loss:   7.24, conf_loss:   5.42, prob_loss:   7.88, total_loss:  20.55\n",
      "epoch: 6 step:  197/250, lr:0.000099, giou_loss:   5.94, conf_loss:   5.50, prob_loss:   6.45, total_loss:  17.89\n",
      "epoch: 6 step:  198/250, lr:0.000099, giou_loss:   6.81, conf_loss:   6.15, prob_loss:  15.00, total_loss:  27.96\n",
      "epoch: 6 step:  199/250, lr:0.000099, giou_loss:   6.80, conf_loss:   5.68, prob_loss:   9.15, total_loss:  21.64\n",
      "epoch: 6 step:  200/250, lr:0.000099, giou_loss:   6.10, conf_loss:   5.02, prob_loss:   8.26, total_loss:  19.37\n",
      "epoch: 6 step:  201/250, lr:0.000099, giou_loss:   4.72, conf_loss:   5.18, prob_loss:   3.77, total_loss:  13.67\n",
      "epoch: 6 step:  202/250, lr:0.000099, giou_loss:   7.11, conf_loss:   5.74, prob_loss:  11.33, total_loss:  24.18\n",
      "epoch: 6 step:  203/250, lr:0.000099, giou_loss:   6.89, conf_loss:   6.45, prob_loss:  11.59, total_loss:  24.92\n",
      "epoch: 6 step:  204/250, lr:0.000099, giou_loss:   7.29, conf_loss:   5.18, prob_loss:  12.63, total_loss:  25.09\n",
      "epoch: 6 step:  205/250, lr:0.000099, giou_loss:   4.74, conf_loss:   5.51, prob_loss:   6.63, total_loss:  16.88\n",
      "epoch: 6 step:  206/250, lr:0.000099, giou_loss:   4.82, conf_loss:   4.87, prob_loss:   7.66, total_loss:  17.36\n",
      "epoch: 6 step:  207/250, lr:0.000099, giou_loss:   6.90, conf_loss:   5.22, prob_loss:  12.19, total_loss:  24.30\n",
      "epoch: 6 step:  208/250, lr:0.000099, giou_loss:   4.84, conf_loss:   4.86, prob_loss:  12.38, total_loss:  22.08\n",
      "epoch: 6 step:  209/250, lr:0.000099, giou_loss:   5.17, conf_loss:   4.70, prob_loss:   6.68, total_loss:  16.55\n",
      "epoch: 6 step:  210/250, lr:0.000099, giou_loss:   8.30, conf_loss:   6.98, prob_loss:  12.41, total_loss:  27.69\n",
      "epoch: 6 step:  211/250, lr:0.000099, giou_loss:   5.98, conf_loss:   5.86, prob_loss:  10.59, total_loss:  22.43\n",
      "epoch: 6 step:  212/250, lr:0.000099, giou_loss:   7.08, conf_loss:   5.42, prob_loss:   9.39, total_loss:  21.89\n",
      "epoch: 6 step:  213/250, lr:0.000099, giou_loss:   7.08, conf_loss:   6.03, prob_loss:   8.17, total_loss:  21.28\n",
      "epoch: 6 step:  214/250, lr:0.000099, giou_loss:   5.87, conf_loss:   5.44, prob_loss:  11.36, total_loss:  22.68\n",
      "epoch: 6 step:  215/250, lr:0.000099, giou_loss:   8.91, conf_loss:   7.38, prob_loss:  25.33, total_loss:  41.61\n",
      "epoch: 6 step:  216/250, lr:0.000099, giou_loss:   4.41, conf_loss:   5.35, prob_loss:   6.22, total_loss:  15.98\n",
      "epoch: 6 step:  217/250, lr:0.000099, giou_loss:   5.48, conf_loss:   4.80, prob_loss:   4.48, total_loss:  14.76\n",
      "epoch: 6 step:  218/250, lr:0.000099, giou_loss:   4.86, conf_loss:   4.97, prob_loss:  10.08, total_loss:  19.92\n",
      "epoch: 6 step:  219/250, lr:0.000099, giou_loss:   6.75, conf_loss:   5.44, prob_loss:  11.89, total_loss:  24.08\n",
      "epoch: 6 step:  220/250, lr:0.000099, giou_loss:   7.30, conf_loss:   6.97, prob_loss:  11.46, total_loss:  25.74\n",
      "epoch: 6 step:  221/250, lr:0.000099, giou_loss:   8.92, conf_loss:   6.66, prob_loss:  10.82, total_loss:  26.40\n",
      "epoch: 6 step:  222/250, lr:0.000099, giou_loss:   7.31, conf_loss:   5.94, prob_loss:  13.91, total_loss:  27.16\n",
      "epoch: 6 step:  223/250, lr:0.000099, giou_loss:   7.82, conf_loss:   6.24, prob_loss:   9.36, total_loss:  23.43\n",
      "epoch: 6 step:  224/250, lr:0.000099, giou_loss:   3.65, conf_loss:   4.70, prob_loss:   6.26, total_loss:  14.61\n",
      "epoch: 6 step:  225/250, lr:0.000099, giou_loss:   5.56, conf_loss:   5.28, prob_loss:  11.36, total_loss:  22.21\n",
      "epoch: 6 step:  226/250, lr:0.000099, giou_loss:   8.95, conf_loss:   5.75, prob_loss:  11.70, total_loss:  26.40\n",
      "epoch: 6 step:  227/250, lr:0.000099, giou_loss:   5.67, conf_loss:   5.21, prob_loss:   8.00, total_loss:  18.88\n",
      "epoch: 6 step:  228/250, lr:0.000099, giou_loss:   8.01, conf_loss:   6.90, prob_loss:  10.18, total_loss:  25.08\n",
      "epoch: 6 step:  229/250, lr:0.000099, giou_loss:   7.62, conf_loss:   5.79, prob_loss:  11.97, total_loss:  25.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step:  230/250, lr:0.000099, giou_loss:   4.93, conf_loss:   4.99, prob_loss:  13.08, total_loss:  23.00\n",
      "epoch: 6 step:  231/250, lr:0.000099, giou_loss:   6.36, conf_loss:   5.17, prob_loss:   8.45, total_loss:  19.98\n",
      "epoch: 6 step:  232/250, lr:0.000099, giou_loss:   6.40, conf_loss:   5.50, prob_loss:   9.17, total_loss:  21.07\n",
      "epoch: 6 step:  233/250, lr:0.000099, giou_loss:   4.67, conf_loss:   4.84, prob_loss:   5.77, total_loss:  15.28\n",
      "epoch: 6 step:  234/250, lr:0.000099, giou_loss:   5.56, conf_loss:   4.97, prob_loss:   9.90, total_loss:  20.43\n",
      "epoch: 6 step:  235/250, lr:0.000099, giou_loss:   8.47, conf_loss:   5.57, prob_loss:  10.70, total_loss:  24.73\n",
      "epoch: 6 step:  236/250, lr:0.000099, giou_loss:   7.16, conf_loss:   5.37, prob_loss:   8.78, total_loss:  21.31\n",
      "epoch: 6 step:  237/250, lr:0.000099, giou_loss:   9.28, conf_loss:   5.93, prob_loss:  16.24, total_loss:  31.46\n",
      "epoch: 6 step:  238/250, lr:0.000099, giou_loss:   8.88, conf_loss:   6.07, prob_loss:  14.36, total_loss:  29.31\n",
      "epoch: 6 step:  239/250, lr:0.000099, giou_loss:   5.69, conf_loss:   5.43, prob_loss:   6.38, total_loss:  17.50\n",
      "epoch: 6 step:  240/250, lr:0.000099, giou_loss:   6.74, conf_loss:   5.35, prob_loss:  10.34, total_loss:  22.43\n",
      "epoch: 6 step:  241/250, lr:0.000099, giou_loss:   7.34, conf_loss:   7.41, prob_loss:  13.61, total_loss:  28.35\n",
      "epoch: 6 step:  242/250, lr:0.000099, giou_loss:   7.54, conf_loss:   6.30, prob_loss:   9.67, total_loss:  23.51\n",
      "epoch: 6 step:  243/250, lr:0.000099, giou_loss:   4.48, conf_loss:   4.92, prob_loss:   7.24, total_loss:  16.64\n",
      "epoch: 6 step:  244/250, lr:0.000099, giou_loss:   7.51, conf_loss:   5.86, prob_loss:   9.30, total_loss:  22.66\n",
      "epoch: 6 step:  245/250, lr:0.000099, giou_loss:   6.65, conf_loss:   4.99, prob_loss:  19.81, total_loss:  31.46\n",
      "epoch: 6 step:  246/250, lr:0.000099, giou_loss:   5.98, conf_loss:   5.18, prob_loss:  10.70, total_loss:  21.87\n",
      "epoch: 6 step:  247/250, lr:0.000099, giou_loss:   5.25, conf_loss:   4.61, prob_loss:   7.45, total_loss:  17.31\n",
      "epoch: 6 step:  248/250, lr:0.000099, giou_loss:   6.81, conf_loss:   5.05, prob_loss:  10.81, total_loss:  22.67\n",
      "epoch: 6 step:  249/250, lr:0.000099, giou_loss:   4.85, conf_loss:   4.53, prob_loss:   7.09, total_loss:  16.47\n",
      "epoch: 6 step:    0/250, lr:0.000099, giou_loss:   6.88, conf_loss:   5.09, prob_loss:  11.58, total_loss:  23.54\n",
      "epoch: 6 step:    1/250, lr:0.000099, giou_loss:   5.25, conf_loss:   4.91, prob_loss:  13.04, total_loss:  23.19\n",
      "\n",
      "\n",
      "giou_val_loss:   6.43, conf_val_loss:  22.98, prob_val_loss:  20.28, total_val_loss:  49.70\n",
      "\n",
      "\n",
      "epoch: 7 step:    2/250, lr:0.000099, giou_loss:   7.95, conf_loss:   5.32, prob_loss:  13.12, total_loss:  26.39\n",
      "epoch: 7 step:    3/250, lr:0.000099, giou_loss:   4.79, conf_loss:   4.91, prob_loss:  10.23, total_loss:  19.94\n",
      "epoch: 7 step:    4/250, lr:0.000099, giou_loss:   7.35, conf_loss:   5.52, prob_loss:   9.75, total_loss:  22.61\n",
      "epoch: 7 step:    5/250, lr:0.000099, giou_loss:   6.82, conf_loss:   5.34, prob_loss:  14.33, total_loss:  26.49\n",
      "epoch: 7 step:    6/250, lr:0.000099, giou_loss:   7.94, conf_loss:   5.03, prob_loss:  10.40, total_loss:  23.37\n",
      "epoch: 7 step:    7/250, lr:0.000099, giou_loss:   6.24, conf_loss:   5.10, prob_loss:   6.31, total_loss:  17.65\n",
      "epoch: 7 step:    8/250, lr:0.000099, giou_loss:   5.15, conf_loss:   5.19, prob_loss:   6.71, total_loss:  17.05\n",
      "epoch: 7 step:    9/250, lr:0.000099, giou_loss:   8.03, conf_loss:   5.37, prob_loss:  12.48, total_loss:  25.88\n",
      "epoch: 7 step:   10/250, lr:0.000099, giou_loss:   5.71, conf_loss:   4.95, prob_loss:   6.29, total_loss:  16.95\n",
      "epoch: 7 step:   11/250, lr:0.000099, giou_loss:   5.00, conf_loss:   4.70, prob_loss:   4.38, total_loss:  14.08\n",
      "epoch: 7 step:   12/250, lr:0.000099, giou_loss:   5.43, conf_loss:   4.94, prob_loss:   7.86, total_loss:  18.23\n",
      "epoch: 7 step:   13/250, lr:0.000099, giou_loss:   5.10, conf_loss:   4.69, prob_loss:   4.98, total_loss:  14.78\n",
      "epoch: 7 step:   14/250, lr:0.000099, giou_loss:   6.20, conf_loss:   4.95, prob_loss:   6.79, total_loss:  17.94\n",
      "epoch: 7 step:   15/250, lr:0.000099, giou_loss:   5.92, conf_loss:   4.64, prob_loss:   6.69, total_loss:  17.25\n",
      "epoch: 7 step:   16/250, lr:0.000099, giou_loss:   6.25, conf_loss:   6.10, prob_loss:   7.09, total_loss:  19.44\n",
      "epoch: 7 step:   17/250, lr:0.000099, giou_loss:   4.98, conf_loss:   4.77, prob_loss:   7.78, total_loss:  17.54\n",
      "epoch: 7 step:   18/250, lr:0.000099, giou_loss:   5.97, conf_loss:   4.95, prob_loss:   7.08, total_loss:  18.00\n",
      "epoch: 7 step:   19/250, lr:0.000099, giou_loss:   7.11, conf_loss:   5.33, prob_loss:   8.71, total_loss:  21.15\n",
      "epoch: 7 step:   20/250, lr:0.000099, giou_loss:   9.93, conf_loss:   5.83, prob_loss:  13.00, total_loss:  28.76\n",
      "epoch: 7 step:   21/250, lr:0.000099, giou_loss:   5.78, conf_loss:   4.71, prob_loss:   7.26, total_loss:  17.75\n",
      "epoch: 7 step:   22/250, lr:0.000099, giou_loss:   4.45, conf_loss:   4.36, prob_loss:   3.98, total_loss:  12.79\n",
      "epoch: 7 step:   23/250, lr:0.000099, giou_loss:   4.88, conf_loss:   4.59, prob_loss:   3.79, total_loss:  13.26\n",
      "epoch: 7 step:   24/250, lr:0.000099, giou_loss:   4.64, conf_loss:   4.98, prob_loss:   2.91, total_loss:  12.53\n",
      "epoch: 7 step:   25/250, lr:0.000099, giou_loss:   5.26, conf_loss:   4.83, prob_loss:   7.38, total_loss:  17.47\n",
      "epoch: 7 step:   26/250, lr:0.000099, giou_loss:   5.33, conf_loss:   4.59, prob_loss:   8.28, total_loss:  18.20\n",
      "epoch: 7 step:   27/250, lr:0.000099, giou_loss:   6.71, conf_loss:   4.97, prob_loss:   7.15, total_loss:  18.83\n",
      "epoch: 7 step:   28/250, lr:0.000099, giou_loss:   7.48, conf_loss:   5.18, prob_loss:   8.28, total_loss:  20.95\n",
      "epoch: 7 step:   29/250, lr:0.000099, giou_loss:   6.38, conf_loss:   5.56, prob_loss:   7.08, total_loss:  19.02\n",
      "epoch: 7 step:   30/250, lr:0.000099, giou_loss:   6.24, conf_loss:   4.92, prob_loss:   7.23, total_loss:  18.39\n",
      "epoch: 7 step:   31/250, lr:0.000099, giou_loss:   5.24, conf_loss:   5.03, prob_loss:   7.36, total_loss:  17.62\n",
      "epoch: 7 step:   32/250, lr:0.000099, giou_loss:   5.02, conf_loss:   4.70, prob_loss:   4.49, total_loss:  14.21\n",
      "epoch: 7 step:   33/250, lr:0.000099, giou_loss:   6.47, conf_loss:   5.36, prob_loss:   8.16, total_loss:  19.99\n",
      "epoch: 7 step:   34/250, lr:0.000099, giou_loss:   5.86, conf_loss:   5.14, prob_loss:   7.56, total_loss:  18.56\n",
      "epoch: 7 step:   35/250, lr:0.000099, giou_loss:   8.07, conf_loss:   5.23, prob_loss:   9.74, total_loss:  23.04\n",
      "epoch: 7 step:   36/250, lr:0.000099, giou_loss:   6.68, conf_loss:   4.81, prob_loss:   4.38, total_loss:  15.87\n",
      "epoch: 7 step:   37/250, lr:0.000099, giou_loss:   5.61, conf_loss:   4.89, prob_loss:  11.74, total_loss:  22.23\n",
      "epoch: 7 step:   38/250, lr:0.000099, giou_loss:   7.91, conf_loss:   5.01, prob_loss:  13.83, total_loss:  26.75\n",
      "epoch: 7 step:   39/250, lr:0.000099, giou_loss:   6.36, conf_loss:   4.63, prob_loss:   6.98, total_loss:  17.98\n",
      "epoch: 7 step:   40/250, lr:0.000099, giou_loss:   7.23, conf_loss:   5.97, prob_loss:   5.90, total_loss:  19.09\n",
      "epoch: 7 step:   41/250, lr:0.000099, giou_loss:   6.32, conf_loss:   5.17, prob_loss:   8.15, total_loss:  19.65\n",
      "epoch: 7 step:   42/250, lr:0.000099, giou_loss:   6.41, conf_loss:   4.75, prob_loss:   6.09, total_loss:  17.25\n",
      "epoch: 7 step:   43/250, lr:0.000099, giou_loss:   7.62, conf_loss:   4.78, prob_loss:   9.05, total_loss:  21.46\n",
      "epoch: 7 step:   44/250, lr:0.000099, giou_loss:   6.92, conf_loss:   6.09, prob_loss:   8.77, total_loss:  21.78\n",
      "epoch: 7 step:   45/250, lr:0.000099, giou_loss:   7.01, conf_loss:   5.02, prob_loss:   6.36, total_loss:  18.39\n",
      "epoch: 7 step:   46/250, lr:0.000099, giou_loss:   5.43, conf_loss:   4.47, prob_loss:   5.82, total_loss:  15.72\n",
      "epoch: 7 step:   47/250, lr:0.000099, giou_loss:   6.82, conf_loss:   4.94, prob_loss:   7.71, total_loss:  19.47\n",
      "epoch: 7 step:   48/250, lr:0.000099, giou_loss:   6.07, conf_loss:   5.81, prob_loss:  14.16, total_loss:  26.04\n",
      "epoch: 7 step:   49/250, lr:0.000099, giou_loss:   4.66, conf_loss:   4.46, prob_loss:   4.50, total_loss:  13.62\n",
      "epoch: 7 step:   50/250, lr:0.000099, giou_loss:   8.20, conf_loss:   6.90, prob_loss:  12.00, total_loss:  27.10\n",
      "epoch: 7 step:   51/250, lr:0.000099, giou_loss:   4.75, conf_loss:   4.44, prob_loss:   4.96, total_loss:  14.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step:   52/250, lr:0.000099, giou_loss:   4.85, conf_loss:   4.47, prob_loss:   7.71, total_loss:  17.03\n",
      "epoch: 7 step:   53/250, lr:0.000099, giou_loss:   6.34, conf_loss:   4.73, prob_loss:   6.24, total_loss:  17.32\n",
      "epoch: 7 step:   54/250, lr:0.000099, giou_loss:   7.83, conf_loss:   5.19, prob_loss:   9.13, total_loss:  22.15\n",
      "epoch: 7 step:   55/250, lr:0.000099, giou_loss:   7.83, conf_loss:   5.14, prob_loss:   8.88, total_loss:  21.85\n",
      "epoch: 7 step:   56/250, lr:0.000099, giou_loss:   7.98, conf_loss:   5.97, prob_loss:   8.72, total_loss:  22.67\n",
      "epoch: 7 step:   57/250, lr:0.000099, giou_loss:   5.97, conf_loss:   5.00, prob_loss:  10.81, total_loss:  21.78\n",
      "epoch: 7 step:   58/250, lr:0.000099, giou_loss:   6.31, conf_loss:   4.35, prob_loss:  10.11, total_loss:  20.77\n",
      "epoch: 7 step:   59/250, lr:0.000099, giou_loss:   7.38, conf_loss:   5.82, prob_loss:  12.16, total_loss:  25.37\n",
      "epoch: 7 step:   60/250, lr:0.000099, giou_loss:   6.71, conf_loss:   4.88, prob_loss:   5.76, total_loss:  17.36\n",
      "epoch: 7 step:   61/250, lr:0.000099, giou_loss:   5.71, conf_loss:   4.33, prob_loss:   5.52, total_loss:  15.56\n"
     ]
    }
   ],
   "source": [
    "SAVE_BEST_ONLY = True # True이면 loss가 가장 좋은 모델 저장 \n",
    "SAVE_CHECKPOINT = False # True이면 학습 시 모든 모델 저장 \n",
    "CHECKPOINTS_FOLDER = \"checkpoints\"  # 모델이 저장될 디렉토리 \n",
    "MODEL_NAME = \"mnist_custom\"  # 저장될 모델의 이름 \n",
    "SCORE_THRESHOLD = 0.3 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "from config import *\n",
    "from bbox_iou import bbox_iou, bbox_giou\n",
    "from yolov3 import Create_YOLOv3\n",
    "\n",
    "yolo = Create_YOLOv3(train_mode=True, num_class=NUM_CLASS)\n",
    "\n",
    "best_val_loss = 1000 \n",
    "save_directory = os.path.join(CHECKPOINTS_FOLDER, MODEL_NAME)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for image_data, target in trainset:\n",
    "        results = train_step(image_data, target,\n",
    "                             num_class=NUM_CLASS)\n",
    "        cur_step = results[0] % steps_per_epoch\n",
    "        print(\"epoch:{:2.0f} step:{:5.0f}/{}, lr:{:.6f}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}\".format(epoch, cur_step, steps_per_epoch, results[1], results[2], results[3], results[4], results[5]))\n",
    " \n",
    "    if len(testset) == 0: \n",
    "        print(\"configure TEST options to validate model\")\n",
    "        yolo.save_weights(save_directory)\n",
    "        continue \n",
    "\n",
    "    count = 0\n",
    "    giou_val, conf_val, prob_val, total_val = 0, 0, 0, 0 \n",
    "\n",
    "    for image_data, target in testset:\n",
    "        results = validate_step(image_data, target,\n",
    "                                num_class=NUM_CLASS)\n",
    "        count += 1\n",
    "        giou_val += results[0]\n",
    "        conf_val += results[1]\n",
    "        prob_val += results[2]\n",
    "        total_val += results[3]\n",
    "\n",
    "    # validation loss 저장 \n",
    "    with validate_writer.as_default():\n",
    "        tf.summary.scalar(\"validate_loss/total_val\", \n",
    "                          total_val / count, step=epoch)\n",
    "        tf.summary.scalar(\"validate_loss/giou_val\",\n",
    "                          giou_val / count, step=epoch)\n",
    "        tf.summary.scalar(\"validate_loss/conf_val\",\n",
    "                          conf_val / count, step=epoch)\n",
    "        tf.summary.scalar(\"validate_loss/prob_val\",\n",
    "                          prob_val / count, step=epoch)\n",
    "        validate_writer.flush()\n",
    "    print(\"\\n\\ngiou_val_loss:{:7.2f}, conf_val_loss:{:7.2f}, prob_val_loss:{:7.2f}, total_val_loss:{:7.2f}\\n\\n\".format( giou_val / count, conf_val / count, prob_val / count, total_val / count))\n",
    "\n",
    "    if SAVE_CHECKPOINT and not SAVE_BEST_ONLY:\n",
    "        save_directory = os.path.join(CHECKPOINTS_FOLDER,  MODEL_NAME + \"_val_loss_{:7.2f}\".format(total_val / count))\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if best_val_loss > total_val / count:\n",
    "            best_val_loss = total_val / count\n",
    "            yolo.save_weights(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424329d0",
   "metadata": {},
   "source": [
    "# 예측 후 후처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9eb4df",
   "metadata": {},
   "source": [
    "## 박스 후처리(postprocess_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a16091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_boxes(pred_bbox, original_image, input_size, \n",
    "                      score_threshold):\n",
    "\n",
    "    valid_scale = [0, np.inf]\n",
    "    pred_bbox = np.array(pred_bbox)\n",
    "\n",
    "    pred_xywh = pred_bbox[:, 0:4]\n",
    "    pred_conf = pred_bbox[:, 4]\n",
    "    pred_prob = pred_bbox[:, 5:]\n",
    "\n",
    "    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax) \n",
    "    pred_coor = np.concatenate( \n",
    "        [pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n",
    "         pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n",
    "\n",
    "    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org) \n",
    "    org_h, org_w = original_image.shape[:2]\n",
    "    resize_ratio = min(input_size/org_w, input_size/org_h)\n",
    "\n",
    "    dw = (input_size - resize_ratio * org_w) / 2 \n",
    "    dh = (input_size - resize_ratio * org_h) / 2 \n",
    "\n",
    "    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n",
    "    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n",
    "\n",
    "    # 3. 범위를 벗어나는 박스를 자름 \n",
    "    pred_coor = np.concatenate(\n",
    "        [np.maximum(pred_coor[:, :2], [0, 0]),\n",
    "         np.minimum(pred_coor[:, 2:], [org_w-1, org_h-1])],\n",
    "        axis=-1)\n",
    "    invalid_mask = np.logical_or(\n",
    "        (pred_coor[:, 0] > pred_coor[:, 2]),\n",
    "        (pred_coor[:, 1] > pred_coor[:, 3]))\n",
    "    pred_coor[invalid_mask] = 0 \n",
    "\n",
    "    # 4. 유효하지 않은 상자 무시 \n",
    "    bboxes_scale = np.sqrt(\n",
    "        np.multiply.reduce(\n",
    "            pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
    "    scale_mask = np.logical_and(\n",
    "        (valid_scale[0] < bboxes_scale),\n",
    "        (bboxes_scale < valid_scale[1]))\n",
    "\n",
    "    # 5. 낮은 스코어의 상자 무시 \n",
    "    classes = np.argmax(pred_prob, axis=-1)\n",
    "    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
    "    score_mask = scores > score_threshold\n",
    "    mask = np.logical_and(scale_mask, score_mask)\n",
    "    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n",
    "\n",
    "    return np.concatenate([coors, scores[:, np.newaxis], \n",
    "                           classes[:, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f5b03",
   "metadata": {},
   "source": [
    "## 상자들의 IoU 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225723b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bboxes_iou(boxes1, boxes2):\n",
    "    boxes1 = np.array(boxes1)\n",
    "    boxes2 = np.array(boxes2)\n",
    "\n",
    "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "\n",
    "    ious = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19cf7d",
   "metadata": {},
   "source": [
    "## NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897aac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n",
    "    classes_in_img = list(set(bboxes[:, 5]))\n",
    "    best_bboxes = []\n",
    "\n",
    "    for cls in classes_in_img:\n",
    "        cls_mask = (bboxes[:, 5] == cls)\n",
    "        cls_bboxes = bboxes[cls_mask]\n",
    "\n",
    "        # 1. 경계 상자의 개수가 0보다 큰지 확인  \n",
    "        while len(cls_bboxes) > 0:\n",
    "            # 2. 가장 높은 점수를 갖는 경계 상자를 선택 \n",
    "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
    "            best_bbox = cls_bboxes[max_ind]\n",
    "            best_bboxes.append(best_bbox)\n",
    "            cls_bboxes = np.concatenate(\n",
    "                [cls_bboxes[: max_ind], \n",
    "                 cls_bboxes[max_ind + 1:]])\n",
    "  \n",
    "            # 3. 경계 상자의 모든 iou를 계산하고 iou 값이 임계값보다 높은 경계 상자를 제거 \n",
    "            iou = bboxes_iou(best_bbox[np.newaxis, :4],\n",
    "                             cls_bboxes[:, :4])\n",
    "            weight = np.ones((len(iou),), dtype=np.float32)\n",
    "\n",
    "            assert method in ['nms', 'soft-nms']\n",
    "\n",
    "            if method == 'nms':\n",
    "                iou_mask = iou > iou_threshold\n",
    "                weight[iou_mask] = 0.0 \n",
    "\n",
    "            if method == 'soft-nms':\n",
    "                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n",
    "\n",
    "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
    "            score_mask = cls_bboxes[:, 4] > 0. \n",
    "            cls_bboxes = cls_bboxes[score_mask]\n",
    "\n",
    "    return best_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925645ca",
   "metadata": {},
   "source": [
    "## bboxes_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def draw_bbox(image, bboxes, class_names,\n",
    "              show_label=True, show_confidence=True,\n",
    "              Text_colors=(0,0,0), rectangle_colors='', \n",
    "              tracking=False):\n",
    "    image_h, image_w, _ = image.shape\n",
    "    num_class = len(class_names)\n",
    "\n",
    "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(colors)\n",
    "    random.seed(None)\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        score = bbox[4]\n",
    "        class_ind = int(bbox[5])\n",
    "        bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
    "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
    "        if bbox_thick < 1: bbox_thick = 1 \n",
    "        fontScale = 0.75 * bbox_thick\n",
    "        x1, y1 = coor[0], coor[1]\n",
    "        x2, y2 = coor[2], coor[3]\n",
    "\n",
    "        # 경계상자 그리기 \n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), \n",
    "                      bbox_color, bbox_thick * 2)\n",
    "\n",
    "        if show_label:\n",
    "            score_str = \"\" \n",
    "            if show_confidence:\n",
    "                score_str = \" {:.2f}\".format(score)\n",
    "            if tracking: \n",
    "                score_str = \" \" + str(score)\n",
    "\n",
    "            try:\n",
    "                label = f\"{_class_names[class_ind]}{score_str}\"\n",
    "            except KeyError:\n",
    "                print(\"클래스 라벨이 잘못되었습니다.\")\n",
    "\n",
    "            # 텍스트 크기 \n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                fontScale, thickness=bbox_thick)\n",
    "            # 텍스트를 출력할 채워진 사각형 \n",
    "            cv2.rectangle(image, (x1, y1), \n",
    "                          (x1 + text_width,\n",
    "                           y1 - text_height - baseline),\n",
    "                          bbox_color, thickness=cv2.FILLED)\n",
    "            # 사각형 위에 텍스트 출력 \n",
    "            cv2.putText(image, label, (x1, y1 - 4), \n",
    "                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                        fontScale, Text_colors, bbox_thick,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0f26a",
   "metadata": {},
   "source": [
    "## detect_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from image_process import resize_to_square\n",
    "from data import read_class_names\n",
    "from post_process import *\n",
    "\n",
    "def detect_image(model, image_path, output_path,\n",
    "                 class_label_path, \n",
    "                 input_size=416, show=False,\n",
    "                 score_threshold=0.3, iou_threshold=0.45,\n",
    "                 rectangle_colors=''):\n",
    "    original_image = cv2.imread(image_path)\n",
    "    class_names = read_class_names(class_label_path)\n",
    "\n",
    "    image_data = resize_to_square(np.copy(original_image), \n",
    "                                  target_size=input_size)\n",
    "    image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "\n",
    "    pred_bbox = model.predict(image_data)\n",
    "\n",
    "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
    "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
    "\n",
    "    bboxes = postprocess_boxes(pred_bbox, original_image,\n",
    "                               input_size, score_threshold)\n",
    "    bboxes = nms(bboxes, iou_threshold, method='nms')\n",
    "\n",
    "    image = draw_bbox(original_image, bboxes, class_names,\n",
    "                      rectangle_colors=rectangle_colors)\n",
    "\n",
    "    if output_path != '':\n",
    "        cv2.imwrite(output_path, image)\n",
    "    if show:\n",
    "        cv2.imshow(\"predicted image\", image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb23fa",
   "metadata": {},
   "source": [
    "## detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10\n",
    "from yolov3 import Create_YOLOv3\n",
    "\n",
    "yolo = Create_YOLOv3(num_class=NUM_CLASS)\n",
    "yolo.load_weights(\"checkpoints/mnist_custom\")\n",
    "weight = yolo.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde13fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.set_weights(weight)\n",
    "result_image = detect_image(model=yolo,  \n",
    "                            image_path=\"mnist_test_c.jpg\",\n",
    "                            output_path=\"mnist_test_out.jpg\", \n",
    "                            class_label_path=\"mnist.names\", \n",
    "                            input_size=416, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70606f10",
   "metadata": {},
   "source": [
    "## Realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654aa3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from image_process import resize_to_square\n",
    "from data import read_class_names\n",
    "from post_process import *\n",
    "from yolov3 import Create_YOLOv3\n",
    "\n",
    "yolo = Create_YOLOv3(num_class=10)\n",
    "yolo.load_weights(\"checkpoints/mnist_custom\")\n",
    "weights = yolo.get_weights()\n",
    "class_names = read_class_names(\"mnist.names\")\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "if cap.isOpened():\n",
    "    while True:\n",
    "        yolo.set_weights(weights)\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            print(\"프레임을 받지 못했습니다.\")\n",
    "            break \n",
    "\n",
    "        # 밝기를 100만큼 더함 \n",
    "        dummy = np.full(image.shape, fill_value=100, \n",
    "                        dtype=np.uint8)\n",
    "        cv2.add(image, dummy, image)\n",
    "                \n",
    "        # 콘트라스트 강조함 \n",
    "        image = cv2.normalize(image, None, 0, 255,\n",
    "                              cv2.NORM_MINMAX)\n",
    "\n",
    "        # 이미지를 정사각형 모양으로 만듬 \n",
    "        image_data = resize_to_square(np.copy(image), 416)\n",
    "        image_data = image_data[np.newaxis,\n",
    "                                ...].astype(np.float32)\n",
    "\n",
    "        # 상자 예측 \n",
    "        pred_box = yolo.predict(image_data)\n",
    "        pred_box = [tf.reshape(x, (-1, tf.shape(x)[-1])) \n",
    "                    for x in pred_box]\n",
    "        pred_box = tf.concat(pred_box, axis=0)\n",
    "\n",
    "        # 상자 후처리 \n",
    "        bboxes = postprocess_boxes(pred_box, image, 416, 0.3)\n",
    "\n",
    "        # NMS에 의해 해당 영역에서 상자 하나만 남김 \n",
    "        bboxes = nms(bboxes, 0.45, method=\"nms\")\n",
    "\n",
    "        # 상자를 그림 \n",
    "        image = draw_bbox(image, bboxes, class_names)\n",
    "\n",
    "        cv2.imshow(\"Image\", image)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "else:\n",
    "    print('연결된 카메라가 없습니다.')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf296f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
